{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores WordNet synsets, presenting a simple method for finding in a text all mentions of all hyponyms of a given node in the WordNet hierarchy (e.g., finding all *buildings* in a text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries\n",
    "\n",
    "The first step is to import the necessary Python libraries.\n",
    "* **nltk**: The Natural Language Toolkit, which provides access to WordNet.\n",
    "* **re**: The regular expressions library, used for text manipulation.\n",
    "* **spacy**: A powerful library for Natural Language Processing (NLP), used here for efficient text tokenization and lemmatization.\n",
    "* **wordnet (as wn)**: The specific corpus from NLTK that we will be working with. We import it with the alias `wn` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Natural Language Toolkit for WordNet access\n",
    "import nltk, re, spacy\n",
    "\n",
    "# Import the WordNet corpus from NLTK and assign it the alias 'wn'\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Checking NLTK Version\n",
    "\n",
    "This cell simply checks and displays the installed version of the NLTK library to ensure compatibility and for documentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the current version of the NLTK library\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initializing SpaCy\n",
    "\n",
    "Here, we load a pre-trained English model from SpaCy. To make processing faster, we disable the parts of the pipeline we don't need for this task: the part-of-speech **tagger**, the **named entity recognizer (ner)**, and the dependency **parser**. Our main goal is to get lemmatized tokens, which SpaCy's tokenizer can provide without these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English model from SpaCy, disabling components that are not needed to speed up processing.\n",
    "# We only need the tokenizer and lemmatizer for this task.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger','ner','parser'])\n",
    "\n",
    "# The following lines are another way to remove components from the pipeline after loading.\n",
    "# They are redundant here because we used the 'disable' argument above, but show an alternative method.\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('ner')\n",
    "# nlp.remove_pipe('parser');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exploring WordNet Synsets\n",
    "\n",
    "A **synset** is a group of synonyms that represent a single concept. This code retrieves all synsets associated with the word \"car\" and prints each synset along with its definition. They are generally ordered from most to least common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all synsets for the word 'car'\n",
    "synsets=wn.synsets('car')\n",
    "\n",
    "# Iterate through each synset in the list\n",
    "for synset in synsets:\n",
    "    # Print the synset's name and its definition\n",
    "    print (synset, synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Getting Words from a Synset\n",
    "\n",
    "Each synset contains one or more **lemmas**, which are the specific words or phrases that belong to it. This code accesses the first noun synset for \"car\" (`car.n.01`) and prints all of its lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the synset 'car.n.01' and iterate through its lemmas\n",
    "for lemma in wn.synset(\"car.n.01\").lemmas():\n",
    "    # Print the name of each lemma\n",
    "    print (lemma.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Defining Hierarchy Traversal Functions\n",
    "\n",
    "WordNet is structured as a hierarchy. To easily navigate it, we define two simple `lambda` functions:\n",
    "* **hypo**: Finds the immediate children of a synset (hyponyms, or more specific concepts).\n",
    "* **hyper**: Finds the immediate parents of a synset (hypernyms, or more general concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are shortcuts from http://www.nltk.org/howto/wordnet.html to get hyponyms/hypernyms\n",
    "\n",
    "# Define a lambda function 'hypo' that takes a synset 's' and returns its direct hyponyms.\n",
    "hypo = lambda s: s.hyponyms()\n",
    "\n",
    "# Define a lambda function 'hyper' that takes a synset 's' and returns its direct hypernyms.\n",
    "hyper = lambda s: s.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Finding All Hyponyms (Descendants)\n",
    "\n",
    "Using the `closure()` method and our `hypo` function, we can recursively find all synsets that are descendants of a target synset. This code finds every type of \"car\" in the WordNet hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the closure() method with the 'hypo' function to recursively find all hyponyms of 'car.n.01'.\n",
    "# This effectively gets all synsets that are \"under\" car.n.01 in the hierarchy.\n",
    "list(wn.synset(\"car.n.01\").closure(hypo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Finding All Hypernyms (Ancestors)\n",
    "\n",
    "Similarly, we can use `closure()` with our `hyper` function to find all ancestors of a target synset, tracing its path up to the root of the hierarchy (like \"entity\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the closure() method with the 'hyper' function to recursively find all hypernyms of 'car.n.01'.\n",
    "# This traces the path from 'car' up to its most general concepts.\n",
    "list(wn.synset(\"car.n.01\").closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Function to Collect All Hyponym Words\n",
    "\n",
    "This function, `get_words_in_hypo`, automates the process of finding all words associated with a given synset *and* all of its descendants. It gathers every lemma from this entire branch of the WordNet tree, cleans up the formatting (replacing underscores with spaces), and returns them as a single set to avoid duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_hypo(synset):\n",
    "    \"\"\"\n",
    "    Returns a set of words/phrases that comprise the lemmas of a synset and all its hyponyms.\n",
    "    \"\"\"\n",
    "    # Initialize an empty set to store the words to avoid duplicates.\n",
    "    words=set()\n",
    "    \n",
    "    # Get all hyponyms (descendants) of the input synset.\n",
    "    hyponym_synsets=list(synset.closure(hypo))\n",
    "    \n",
    "    # Add the original synset itself to the list to include its own lemmas.\n",
    "    hyponym_synsets.append(synset)\n",
    "    \n",
    "    # Iterate through the combined list of the original synset and its hyponyms.\n",
    "    for s in hyponym_synsets:\n",
    "        # For each synset, iterate through its lemmas.\n",
    "        for l in s.lemmas():\n",
    "            # Get the lemma's name (e.g., 'motor_car').\n",
    "            word=l.name()\n",
    "            # Replace underscores with spaces for readability (e.g., 'motor car').\n",
    "            word=re.sub(\"_\", \" \", word)\n",
    "            # Add the cleaned word to the set.\n",
    "            words.add(word)\n",
    "    \n",
    "    # Return the final set of unique words.\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Testing the Word Collection Function\n",
    "\n",
    "This cell calls the function we just created on the `car.n.01` synset to demonstrate its output. The result is a comprehensive set of terms related to cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to get all words for 'car.n.01' and its descendants.\n",
    "get_words_in_hypo(wn.synset(\"car.n.01\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Function to Find Word Occurrences in Text\n",
    "\n",
    "The `find_all_words_in_text` function takes a set of target words and a text pre-processed by SpaCy. It iterates through every token in the text and checks if the token's **lemma** is in our target set. Using the lemma allows us to match different forms of a word (e.g., \"cars\" matches \"car\"). It returns a list of the numerical indices of all matching tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_words_in_text(words, spacy_tokens):\n",
    "    \"\"\" \n",
    "    For a given set of words, find each instance among a list of tokens already\n",
    "    processed by spacy. Returns a list of token indexes that match.\n",
    "    (Note this only identifies single words, not multi-word phrases.)\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the indices of matched tokens.\n",
    "    all_matches=[]\n",
    "    \n",
    "    # Iterate through the spacy tokens, getting both the index (idx) and the token object.\n",
    "    for idx, token in enumerate(spacy_tokens):\n",
    "        # Check if the token's lemma (its base form) is in our target set of words.\n",
    "        if token.lemma_ in words:\n",
    "            # If it's a match, add the token's index to our list.\n",
    "            all_matches.append(idx)\n",
    "            \n",
    "    # Return the list of all found indices.\n",
    "    return all_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Function to Display Results (Concordance)\n",
    "\n",
    "A **concordance** shows every occurrence of a word in its context. This `print_concordance` function takes the list of match indices and the SpaCy tokens. For each match, it prints the word, highlighted in red, along with a few words before and after it (a \"window\") to show how it was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_concordance(matches, spacy_tokens, window=3):\n",
    "    \"\"\" \n",
    "    For a given set of token indexes, prints out a window of words around each match,\n",
    "    in the style of a concordance.\n",
    "    \"\"\"\n",
    "    # Define ANSI escape codes for terminal colors to highlight the matched word.\n",
    "    RED=\"\\x1b[31m\"\n",
    "    BLACK=\"\\x1b[0m\"\n",
    "    \n",
    "    # Calculate left-side padding for alignment based on the window size.\n",
    "    spacing=window*10\n",
    "    \n",
    "    # Iterate through each match index found previously.\n",
    "    for match in matches:\n",
    "        # Calculate the start and end indices for the context window.\n",
    "        start=match-window\n",
    "        end=match+window+1\n",
    "        \n",
    "        # Ensure the start index is not less than 0.\n",
    "        if start < 0:\n",
    "            start=0\n",
    "        # Ensure the end index does not exceed the total number of tokens.\n",
    "        if end > len(spacy_tokens):\n",
    "            end=len(spacy_tokens)\n",
    "            \n",
    "        # Create the string of text *before* the matched word.\n",
    "        pre=' '.join([token.text for token in spacy_tokens[start:match]])\n",
    "        # Create the string of text *after* the matched word.\n",
    "        post=' '.join([token.text for token in spacy_tokens[match+1:end]])\n",
    "        \n",
    "        # Print the formatted concordance line with the matched word in red.\n",
    "        print(\"%s %s%s%s %s\" % (pre.rjust(spacing), RED, spacy_tokens[match].text, BLACK, post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Function to Read Text from a File\n",
    "\n",
    "This is a helper function to read a text file. It opens the file, reads its entire content, and uses a regular expression (`re.sub`) to replace any sequence of one or more whitespace characters (spaces, tabs, newlines) with a single space. This cleans and standardizes the text for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "    \"\"\" \n",
    "    Read a text file, replacing all whitespace sequences with a single space.\n",
    "    \"\"\"\n",
    "    # Open the file with UTF-8 encoding to handle a wide range of characters.\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Read the file's content and replace all whitespace sequences with a single space.\n",
    "        return re.sub(\"\\s+\", \" \", file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Loading the Text\n",
    "\n",
    "Here, we use our `read_text` function to load the novel *Pride and Prejudice* from a local file into the `book` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the read_text function to load the specified text file.\n",
    "# Note: The path \"../data/pride_and_prejudice.txt\" assumes a specific directory structure.\n",
    "book=read_text(\"../data/pride_and_prejudice.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Processing the Text with SpaCy\n",
    "\n",
    "Now we pass the entire text of the book to our `nlp` object. SpaCy processes the text, performing tokenization and lemmatization, and stores the result as a sequence of tokens in the `spacy_tokens` variable. This is the most computationally intensive step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the book's text with the SpaCy nlp object.\n",
    "# This creates a Doc object containing all the tokens and their linguistic features (like lemmas).\n",
    "spacy_tokens=nlp(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. The Main Search Function\n",
    "\n",
    "This `wordnet_search` function ties everything together. It takes a target WordNet synset and the SpaCy-processed tokens as input.\n",
    "1.  It calls `get_words_in_hypo` to build a list of all relevant words.\n",
    "2.  It passes this list to `find_all_words_in_text` to find all matches in the book.\n",
    "3.  Finally, it calls `print_concordance` to display the results in a user-friendly way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_search(synset, spacy_tokens):\n",
    "    \"\"\" \n",
    "    This function searches through all tokens to find any mention of words\n",
    "    in the given synset or any of its hyponyms.\n",
    "    \"\"\"\n",
    "    # 1. Get all target words from the synset and its hyponyms.\n",
    "    targets=get_words_in_hypo(synset)\n",
    "    \n",
    "    # 2. Find the indices of all tokens that match the target words.\n",
    "    matches=find_all_words_in_text(targets, spacy_tokens)\n",
    "    \n",
    "    # 3. Print the matches in a concordance view.\n",
    "    print_concordance(matches, spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Find all color terms in *Pride and Prejudice*\n",
    "\n",
    "To solve this, we first find the appropriate synset for \"color\" (`color.n.01`) and then pass it to our `wordnet_search` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the synset for color. 'color.n.01' is the primary noun sense.\n",
    "color_synset = wn.synset('color.n.01')\n",
    "\n",
    "# Call the search function with the color synset and the processed text.\n",
    "wordnet_search(color_synset, spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Find all vehicles mentioned in *Pride and Prejudice*\n",
    "\n",
    "We follow the same process for \"vehicle\". The synset `vehicle.n.01` represents conveyances that transport people or objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the synset for vehicle. 'vehicle.n.01' is \"a conveyance that transports people or objects\".\n",
    "vehicle_synset = wn.synset('vehicle.n.01')\n",
    "\n",
    "# Call the search function with the vehicle synset.\n",
    "wordnet_search(vehicle_synset, spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Find all verbs of speaking in *Pride and Prejudice*\n",
    "\n",
    "Here, we look for verbs. The synset `talk.v.01` (\"exchange thoughts; talk with\") is a good starting point for finding verbs related to speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the synset for speaking verbs. 'talk.v.01' is a good general choice.\n",
    "speak_synset = wn.synset('talk.v.01')\n",
    "\n",
    "# Call the search function.\n",
    "wordnet_search(speak_synset, spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Find all of the people in *Pride and Prejudice*\n",
    "\n",
    "The synset `person.n.01` (\"a human being\") is the most appropriate choice for finding general references to people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the synset for a person. 'person.n.01' is \"a human being\".\n",
    "person_synset = wn.synset('person.n.01')\n",
    "\n",
    "# Call the search function.\n",
    "wordnet_search(person_synset, spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. How can we improve this method?\n",
    "\n",
    "The current method has a major limitation: **word sense ambiguity**. It identifies a word if its lemma is in the target set, regardless of its meaning in context. For example, if we searched for things found at a river `bank` (the land), our current method would also incorrectly flag mentions of a financial `bank`.\n",
    "\n",
    "We could improve the method in several ways:\n",
    "\n",
    "1.  **Part-of-Speech (POS) Filtering**: We could re-enable SpaCy's POS tagger. If we are searching for a noun synset (like `bank.n.01`), we could filter out instances where the word \"bank\" is used as a verb (e.g., \"to bank a shot\"). This would reduce errors but not solve ambiguity between two noun senses.\n",
    "\n",
    "2.  **Word Sense Disambiguation (WSD)**: The most direct improvement would be to use a WSD algorithm. After finding a potential match, a WSD system would analyze the surrounding words (the context) to determine which of the word's possible synsets is the most likely one. We would only count a match if the algorithm assigned it to our target synset (or one of its hyponyms). This is a complex task but provides much higher accuracy. For example, in the phrase \"money in the bank,\" a WSD model would likely choose `bank.n.02` (the financial institution), whereas in \"sat on the river bank,\" it would choose `bank.n.01` (sloping land)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
