{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores a simple hypothesis test checking whether the accuracy of a trained model for binary classificadtion is meaningfully different from a majority class baseline.  We test this making a parametric assumption: we assume that the binary correct/incorrect results follow a binomial distribution (and approximate the binomial with a normal distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Imports\n",
    "This cell imports all the necessary libraries for the notebook.\n",
    "* **sys, collections, math**: Standard Python libraries for system functions, data structures (like `Counter`), and mathematical operations.\n",
    "* **pandas, numpy, scipy**: Core libraries for data science in Python. `pandas` is for data manipulation, `numpy` is for numerical operations, and `scipy` is for scientific and statistical computations, including sparse matrices and the normal distribution functions.\n",
    "* **sklearn**: Scikit-learn is the primary machine learning library used here for preprocessing and implementing the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sys module for system-specific parameters and functions.\n",
    "import sys\n",
    "# Import the Counter class for counting hashable objects, like labels.\n",
    "from collections import Counter\n",
    "# Import preprocessing tools from scikit-learn.\n",
    "from sklearn import preprocessing\n",
    "# Import linear models from scikit-learn, specifically for Logistic Regression.\n",
    "from sklearn import linear_model\n",
    "# Import pandas for data manipulation, though not directly used in these functions.\n",
    "import pandas as pd\n",
    "# Import sparse matrix tools from scipy, essential for efficient feature storage.\n",
    "from scipy import sparse\n",
    "# Import numpy for numerical operations, especially array manipulation.\n",
    "import numpy as np\n",
    "# Import the square root function from the math module.\n",
    "from math import sqrt \n",
    "# Import the normal distribution object from scipy.stats for statistical tests.\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Data Reading Function\n",
    "This cell defines a function `read_data` to load text data from a tab-separated values (`.tsv`) file. It iterates through each line, splitting it into a label and a text body, and appends them to two separate lists, `X` (features, i.e., text) and `Y` (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read data from a file.\n",
    "def read_data(filename):\n",
    "    # Initialize an empty list to store the text data (features).\n",
    "    X=[]\n",
    "    # Initialize an empty list to store the labels.\n",
    "    Y=[]\n",
    "    # Open the specified file with UTF-8 encoding.\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in file:\n",
    "            # Remove trailing whitespace and split the line by tabs.\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label.\n",
    "            label=cols[0]\n",
    "            # The second column is the text.\n",
    "            text=cols[1]\n",
    "            # Add the text to the feature list.\n",
    "            X.append(text)\n",
    "            # Add the label to the label list.\n",
    "            Y.append(label)\n",
    "    # Return the lists of features and labels.\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Setting the Data Directory\n",
    "This cell sets the file path for the directory containing the datasets. **This is a variable that the user must change** to point to the correct location on their own system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "# Set the string variable 'directory' to the path of the data folder.\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Loading the Datasets\n",
    "This cell calls the `read_data` function to load the training (`train.tsv`) and development (`dev.tsv`) sets into memory. The text and labels for each set are stored in separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data by calling the read_data function.\n",
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "# Load the development (validation) data.\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Majority Class Baseline Function\n",
    "The `majority_class` function calculates the baseline accuracy. It works by finding the most common label in the training set (`trainY`) and then \"predicting\" that same label for every single item in the development set (`devY`). The accuracy of this simple strategy serves as our baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the majority class baseline accuracy.\n",
    "def majority_class(trainY, devY):\n",
    "    # Create a Counter object to store frequencies of each label.\n",
    "    labelCounts=Counter()\n",
    "    # Iterate through the labels in the training set.\n",
    "    for label in trainY:\n",
    "        # Increment the count for the current label.\n",
    "        labelCounts[label]+=1\n",
    "    # Find the most common label and its count, and get the label name.\n",
    "    majority=labelCounts.most_common(1)[0][0]\n",
    "    \n",
    "    # Initialize a counter for correct predictions to zero.\n",
    "    correct=0.\n",
    "    # Iterate through the labels in the development set.\n",
    "    for label in devY:\n",
    "        # Check if the development label matches the majority label from the training set.\n",
    "        if label == majority:\n",
    "            # If it matches, increment the correct counter.\n",
    "            correct+=1\n",
    "            \n",
    "    # Print the majority class label and its accuracy on the dev set, formatted to 3 decimal places.\n",
    "    print(\"%s\\t%.3f\" % (majority, correct/len(devY)))\n",
    "    # Return the calculated baseline accuracy.\n",
    "    return correct/len(devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Dictionary-Based Feature Function\n",
    "This cell defines a simple feature engineering function, `political_dictionary_feature`. It checks if words from a predefined \"dictionary\" (in this case, small sets of politically-associated words) appear in the input text. If a word is found, it creates a binary feature (e.g., `word_in_dem_dictionary: 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a sample dictionary we can create by inspecting the output of the Mann-Whitney test (in 2.compare/)\n",
    "# Define a set of words associated with Democrats.\n",
    "dem_dictionary=set([\"republican\",\"cut\", \"opposition\"])\n",
    "# Define a set of words associated with Republicans.\n",
    "repub_dictionary=set([\"growth\",\"economy\"])\n",
    "\n",
    "# Define a function to create features based on these dictionaries.\n",
    "def political_dictionary_feature(tokens):\n",
    "    # Initialize an empty dictionary to store features for the current text.\n",
    "    feats={}\n",
    "    # Iterate over each token (word) in the input text.\n",
    "    for word in tokens:\n",
    "        # If the word is in the Democrat dictionary...\n",
    "        if word in dem_dictionary:\n",
    "            # ...set the corresponding feature to 1.\n",
    "            feats[\"word_in_dem_dictionary\"]=1\n",
    "        # If the word is in the Republican dictionary...\n",
    "        if word in repub_dictionary:\n",
    "            # ...set its corresponding feature to 1.\n",
    "            feats[\"word_in_repub_dictionary\"]=1\n",
    "    # Return the dictionary of features.\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Unigram Feature Function\n",
    "This function, `unigram_feature`, implements a common \"bag-of-words\" approach. For every word in the input text, it creates a unique binary feature (e.g., `UNIGRAM_economy: 1`). This is a more comprehensive but less targeted feature engineering method compared to the dictionary approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create unigram (single word) features.\n",
    "def unigram_feature(tokens):\n",
    "    # Initialize an empty dictionary to store features.\n",
    "    feats={}\n",
    "    # Iterate over each word in the tokenized text.\n",
    "    for word in tokens:\n",
    "        # Create a feature named \"UNIGRAM_<word>\" and set its value to 1.\n",
    "        feats[\"UNIGRAM_%s\" % word]=1\n",
    "    # Return the dictionary of unigram features.\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Feature Building Function\n",
    "The `build_features` function orchestrates the feature engineering process. It takes a list of documents and a list of feature functions (like the ones defined above). It applies every feature function to each document and aggregates the results, returning a list where each item is a dictionary of all features for a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build features for a dataset.\n",
    "def build_features(trainX, feature_functions):\n",
    "    # Initialize an empty list to hold the feature dictionaries for all documents.\n",
    "    data=[]\n",
    "    # Iterate through each document in the input data.\n",
    "    for doc in trainX:\n",
    "        # Initialize an empty dictionary for the current document's features.\n",
    "        feats={}\n",
    "\n",
    "        # The sample text data is already tokenized; if yours is not, do so here.\n",
    "        # Split the document string into a list of tokens (words).\n",
    "        tokens=doc.split(\" \")\n",
    "        \n",
    "        # Iterate through the list of feature-generating functions provided.\n",
    "        for function in feature_functions:\n",
    "            # Call the function with the tokens and update the feats dictionary with the results.\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        # Add the completed feature dictionary for the document to the data list.\n",
    "        data.append(feats)\n",
    "    # Return the list of feature dictionaries.\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Vocabulary Creation Function\n",
    "This function, `create_vocab`, builds a vocabulary that maps every unique feature name found in the training data to a unique integer ID. This mapping is a crucial step to convert our text-based features into a numerical format that machine learning models can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to unique numerical ids.\n",
    "def create_vocab(data):\n",
    "    # Initialize an empty dictionary to store the feature-to-ID mapping.\n",
    "    feature_vocab={}\n",
    "    # Initialize the ID counter to 0.\n",
    "    idx=0\n",
    "    # Iterate through each document's feature dictionary in the data.\n",
    "    for doc in data:\n",
    "        # Iterate through each feature name in the document's dictionary.\n",
    "        for feat in doc:\n",
    "            # If the feature has not been seen before...\n",
    "            if feat not in feature_vocab:\n",
    "                # ...add it to the vocabulary with the current unique ID.\n",
    "                feature_vocab[feat]=idx\n",
    "                # Increment the ID for the next new feature.\n",
    "                idx+=1\n",
    "                \n",
    "    # Return the completed feature vocabulary.\n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 11: Feature Vectorization Function\n",
    "The `features_to_ids` function transforms the list of feature dictionaries into a `scipy.sparse.lil_matrix`. A sparse matrix is used because for any given document, most possible features will be absent (value of 0). This data structure saves memory by only storing the non-zero feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to a sparse representation\n",
    "# that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    "# values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    "# memory.\n",
    "\n",
    "def features_to_ids(data, feature_vocab):\n",
    "    # Create a new sparse matrix (LIL format) with dimensions (num_docs, num_features).\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    # Iterate through the data with both index and document's features.\n",
    "    for idx,doc in enumerate(data):\n",
    "        # Iterate through each feature in the current document's dictionary.\n",
    "        for f in doc:\n",
    "            # If the feature exists in our vocabulary (created from training data)...\n",
    "            if f in feature_vocab:\n",
    "                # ...set the value in the sparse matrix at the correct [row, column] coordinate.\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    # Return the populated sparse matrix.\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: Full ML Pipeline Function\n",
    "The `pipeline` function automates the entire process of training and evaluating a model. It builds features, creates a vocabulary (from the training data only, to prevent data leakage), vectorizes the data into sparse matrices, trains a `LogisticRegression` model, and prints its accuracy on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates a list of feature functions on the training/dev data arguments\n",
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "    # Build feature dictionaries for the training data.\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    # Build feature dictionaries for the development data.\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    # Create a vocabulary mapping feature names to IDs, using only the training data.\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    # Convert the training features to a sparse matrix of IDs.\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    # Convert the development features to a sparse matrix of IDs using the same vocabulary.\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Initialize a Logistic Regression model with specified parameters.\n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    # Train the model on the training data.\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    # Print the model's accuracy on the development data, formatted to 3 decimal places.\n",
    "    print(\"Accuracy: %.3f\" % logreg.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 13: Evaluation Function\n",
    "This `evaluate` function is very similar to `pipeline`, but instead of just printing the final accuracy score, it returns the model's raw predictions and the true labels from the development set. This output is necessary to perform the detailed statistical test in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains a model and returns the predicted and true labels for test data\n",
    "def evaluate(trainX, devX, trainY, devY, feature_functions):\n",
    "    # Build feature dictionaries for the training data.\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    # Build feature dictionaries for the development data.\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    # Create the vocabulary using only training set features.\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    # Convert training and development features to sparse matrices.\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Initialize a Logistic Regression model.\n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    # Train the model on the training data.\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    # Generate predictions for the development set.\n",
    "    predictions=logreg.predict(devX_ids)\n",
    "    # Return the model's predictions and the actual ground truth labels.\n",
    "    return (predictions, devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 14: Calculate Baseline\n",
    "This cell executes the `majority_class` function defined earlier to compute the baseline accuracy. This value is stored in the `baseline` variable and will be used as the null hypothesis ($H_0$) in our statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the majority class baseline accuracy and store it in the 'baseline' variable.\n",
    "baseline=majority_class(trainY,devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 15: Binomial Hypothesis Test Function\n",
    "This is the core statistical function. `binomial_test` performs a two-tailed Z-test to see if the model's accuracy is significantly different from the baseline.\n",
    "\n",
    "* **Null Hypothesis ($H_0$)**: The model's true accuracy is equal to the baseline accuracy.\n",
    "* **Alternative Hypothesis ($H_a$)**: The model's true accuracy is not equal to the baseline accuracy.\n",
    "\n",
    "It calculates the model's observed accuracy (`success_rate`), the standard error for a proportion, the Z-score, the p-value, and a confidence interval. Finally, it compares the Z-score to the critical value (`z_alpha`) to decide whether to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform a statistical test (Z-test for proportions).\n",
    "def binomial_test(predictions, truth, baseline, significance_level=0.95):\n",
    "    # Initialize a list to store results (1 for correct, 0 for incorrect).\n",
    "    correct=[]\n",
    "    # Iterate through the predictions and true labels simultaneously.\n",
    "    for pred, gold in zip(predictions, truth):\n",
    "        # Append 1 if prediction is correct, 0 otherwise.\n",
    "        correct.append(int(pred==gold))\n",
    "        \n",
    "    # Calculate the model's accuracy (mean of the 'correct' list).\n",
    "    success_rate=np.mean(correct)\n",
    "\n",
    "    # This is a two-tailed test, so we split the alpha level (1 - significance) between the two tails.\n",
    "    critical_value=(1-significance_level)/2\n",
    "    # Find the Z-score that corresponds to the critical value using the percent-point function (inverse CDF).\n",
    "    # We multiply by -1 because ppf finds the left-tail value.\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    # Print the calculated critical value and the corresponding Z-score (z_alpha).\n",
    "    print(\"Critical value: %.3f\\tz_alpha: %.3f\" % (critical_value, z_alpha))\n",
    "    \n",
    "    # The standard error is the square root of the variance / sample size.\n",
    "    # The variance for a binomial distribution is p*(1-p).\n",
    "    # We use the observed success_rate as our estimate for p.\n",
    "    standard_error=sqrt((success_rate*(1-success_rate))/len(correct))\n",
    "\n",
    "    # Calculate the Z-score: (observed_value - hypothesized_value) / standard_error.\n",
    "    Z=(success_rate-baseline)/standard_error\n",
    "    # Calculate the lower bound of the confidence interval.\n",
    "    lower=success_rate-z_alpha*standard_error\n",
    "    # Calculate the upper bound of the confidence interval.\n",
    "    upper=success_rate+z_alpha*standard_error\n",
    "    # Calculate the p-value for a two-tailed test using the cumulative distribution function (CDF) and multiply by 2.\n",
    "    pval=norm.cdf(-abs(Z)) * 2\n",
    "    # Print the model's accuracy and the sample size.\n",
    "    print (\"Accuracy: %.3f, n = %s\" % (success_rate, len(correct)))\n",
    "    # Print the calculated confidence interval.\n",
    "    print(\"%s%% Confidence interval: [%.3f,%.3f]\" % (significance_level*100, lower, upper))\n",
    "\n",
    "    # Print the calculated Z-score.\n",
    "    print(\"Z score: %.3f\" % Z)\n",
    "    # Print the calculated p-value.\n",
    "    print(\"p-value: %.5f\" % pval)\n",
    "\n",
    "    # Print the critical region in terms of accuracy scores.\n",
    "    print (\"Critical region corresponding to z_alpha=[%.3f,%.3f]: [%.3f, %.3f]\" % (-z_alpha, z_alpha, baseline-z_alpha*standard_error, baseline+z_alpha*standard_error))\n",
    "    # Print the final conclusion: whether we can reject the null hypothesis based on the Z-score.\n",
    "    print (\"Can we reject null that %.3f is different from %.3f at %s significance level? %s\" % (success_rate, baseline, significance_level*100, \"Yes\" if Z < -z_alpha or Z > z_alpha else \"No\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 16: First Experiment (Dictionary Features)\n",
    "This cell runs the first full experiment. It uses only the simple `political_dictionary_feature` set. It calls `evaluate` to train a model and get predictions, then passes those predictions to `binomial_test` to see if this very basic model is significantly better than just guessing the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of feature functions to use for this experiment.\n",
    "features=[political_dictionary_feature]\n",
    "# Train a model and get predictions and true labels using the specified features.\n",
    "predictions, truth=evaluate(trainX, devX, trainY, devY, features)\n",
    "# Perform the binomial test to compare the model's accuracy against the baseline.\n",
    "binomial_test(predictions, truth, baseline, significance_level=.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 17: Second Experiment (Unigram Features)\n",
    "This cell runs the second experiment, this time using the more powerful `unigram_feature` set. It follows the same process: evaluate the model and then run the statistical test. The results of this test will likely show a much larger Z-score and smaller p-value, indicating a highly significant improvement over the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of feature functions for the second experiment.\n",
    "features=[unigram_feature]\n",
    "# Train the model with unigram features and get the predictions.\n",
    "predictions, truth=evaluate(trainX, devX, trainY, devY, features)\n",
    "# Perform the statistical test on the results of the unigram model.\n",
    "binomial_test(predictions, truth, baseline, significance_level=.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 18: Empty Cell\n",
    "This is an empty cell, often left for future tests or code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}