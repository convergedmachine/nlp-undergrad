{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of the bootstrap to create confidence intervals for any statistic of interest that is estimated from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports all the necessary Python libraries.\n",
    "* `sys` for system-specific parameters and functions.\n",
    "* `Counter` for counting hashable objects.\n",
    "* `sklearn.preprocessing` for encoding categorical labels into numbers.\n",
    "* `sklearn.linear_model` for the Logistic Regression model.\n",
    "* `pandas` for data manipulation (though not heavily used here).\n",
    "* `scipy.sparse` to handle sparse matrices, which are memory-efficient for feature sets with many zero values.\n",
    "* `numpy` for numerical operations, especially with arrays.\n",
    "* `math.sqrt` for calculating the square root.\n",
    "* `scipy.stats.norm` to access properties of the normal distribution, like the Percent Point Function (`ppf`).\n",
    "* `random.choices` for random sampling with replacement, which is the core of the bootstrap method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from math import sqrt \n",
    "from scipy.stats import norm\n",
    "from random import choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a function `read_data` to load text data from a tab-separated file (`.tsv`). It reads the file line by line, splitting each line into a label and a text body. It assumes the text has already been tokenized (split into words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function to read a tab-separated file.\n",
    "def read_data(filename):\n",
    "    # Initialize empty lists to store text data (X) and labels (Y).\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    # Open the specified file with UTF-8 encoding.\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in file:\n",
    "            # Remove trailing whitespace and split the line by tabs.\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label.\n",
    "            label=cols[0]\n",
    "            # The second column is the text.\n",
    "            text=cols[1]\n",
    "            # Assumes text is already tokenized.\n",
    "            # Append the text to the X list.\n",
    "            X.append(text)\n",
    "            # Append the label to the Y list.\n",
    "            Y.append(label)\n",
    "    # Return the lists of texts and labels.\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell sets the path to the directory containing the dataset. You should change this string to match the location of your data files (`train.tsv`, `dev.tsv`, `test.tsv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data.\n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `read_data` function is called to load the training and development (validation) datasets from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data using the read_data function.\n",
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "# Load the development (validation) data.\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a simple feature engineering function. It uses two small, predefined dictionaries of words associated with Democrats and Republicans. The function `political_dictionary_feature` checks if any tokens from an input text are present in these dictionaries and creates binary features (`word_in_dem_dictionary`, `word_in_repub_dictionary`) if they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of words associated with the Democratic party.\n",
    "dem_dictionary=set([\"republican\",\"cut\", \"opposition\"])\n",
    "# Create a set of words associated with the Republican party.\n",
    "repub_dictionary=set([\"growth\",\"economy\"])\n",
    "\n",
    "# Defines a function to extract features based on the political dictionaries.\n",
    "def political_dictionary_feature(tokens):\n",
    "    # Initialize an empty dictionary to store features for a document.\n",
    "    feats={}\n",
    "    # Iterate through each word (token) in the document.\n",
    "    for word in tokens:\n",
    "        # If the word is in the Democratic dictionary...\n",
    "        if word in dem_dictionary:\n",
    "            # ...set the corresponding feature to 1.\n",
    "            feats[\"word_in_dem_dictionary\"]=1\n",
    "        # If the word is in the Republican dictionary...\n",
    "        if word in repub_dictionary:\n",
    "            # ...set the corresponding feature to 1.\n",
    "            feats[\"word_in_repub_dictionary\"]=1\n",
    "    # Return the dictionary of features.\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_features` function processes a list of documents (`trainX`). For each document, it splits the text into tokens and then applies a list of provided feature functions (like the `political_dictionary_feature` function defined above) to generate a set of features for that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function to apply feature extraction to an entire dataset.\n",
    "def build_features(trainX, feature_functions):\n",
    "    # Initialize an empty list to hold the feature dictionaries for all documents.\n",
    "    data=[]\n",
    "    # Iterate through each document in the input data.\n",
    "    for doc in trainX:\n",
    "        # Initialize an empty dictionary for the current document's features.\n",
    "        feats={}\n",
    "\n",
    "        # The sample text is already tokenized; if not, you would tokenize here.\n",
    "        tokens=doc.split(\" \")\n",
    "        \n",
    "        # Iterate through the list of feature functions to apply.\n",
    "        for function in feature_functions:\n",
    "            # Update the features dictionary with the results from the current function.\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        # Append the completed feature dictionary to the data list.\n",
    "        data.append(feats)\n",
    "    # Return the list of feature dictionaries.\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function, `create_vocab`, builds a vocabulary from the training data. It iterates through all the features generated for each document and assigns a unique integer ID to each unique feature name. This is a crucial step to convert text features into a numerical format that machine learning models can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to unique numerical ids.\n",
    "def create_vocab(data):\n",
    "    # Initialize an empty dictionary to store the feature-to-ID mapping.\n",
    "    feature_vocab={}\n",
    "    # Initialize a counter for the unique feature IDs.\n",
    "    idx=0\n",
    "    # Iterate through each document's feature dictionary.\n",
    "    for doc in data:\n",
    "        # Iterate through each feature name in the dictionary.\n",
    "        for feat in doc:\n",
    "            # If the feature has not been seen before...\n",
    "            if feat not in feature_vocab:\n",
    "                # ...add it to the vocabulary with a new unique ID.\n",
    "                feature_vocab[feat]=idx\n",
    "                # Increment the ID for the next new feature.\n",
    "                idx+=1\n",
    "                \n",
    "    # Return the completed feature vocabulary.\n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `features_to_ids` function converts the list of feature dictionaries into a sparse matrix. A sparse matrix is used because most documents will not contain most features, resulting in many zero values. Storing only the non-zero values is highly memory-efficient. This function uses the vocabulary created by `create_vocab` to map feature names to the correct column index in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to a sparse representation.\n",
    "def features_to_ids(data, feature_vocab):\n",
    "    # Create a sparse matrix in \"List of Lists\" format with dimensions (num_docs, num_features).\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    # Iterate through each document and its index.\n",
    "    for idx,doc in enumerate(data):\n",
    "        # Iterate through each feature in the document's feature dictionary.\n",
    "        for f in doc:\n",
    "            # If the feature exists in our training vocabulary...\n",
    "            if f in feature_vocab:\n",
    "                # ...set the value in the sparse matrix at (row=doc_idx, col=feature_id).\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    # Return the final sparse matrix.\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate` function orchestrates the entire model training and evaluation pipeline. It takes training and development data, along with feature functions, and performs the following steps:\n",
    "1.  Builds features for both training and development sets.\n",
    "2.  Creates a vocabulary based *only* on the training data to prevent data leakage.\n",
    "3.  Converts feature dictionaries into sparse matrices.\n",
    "4.  Encodes string labels (e.g., 'Democrat') into numerical labels (e.g., 0, 1).\n",
    "5.  Trains a logistic regression model.\n",
    "6.  Evaluates the model's accuracy on the development set.\n",
    "7.  Returns the model's predictions and the true labels for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains a model and returns the predicted and true labels for the dev data.\n",
    "def evaluate(trainX, devX, trainY, devY, feature_functions):\n",
    "    # Generate feature dictionaries for the training data.\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    # Generate feature dictionaries for the development data.\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # Create a vocabulary from features found *only* in the training data.\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    # Convert the training features into a sparse matrix of IDs.\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    # Convert the development features into a sparse matrix of IDs using the same vocabulary.\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Initialize a LabelEncoder to convert string labels to integers.\n",
    "    le=preprocessing.LabelEncoder()\n",
    "    # Fit the encoder on the training labels to learn the mapping.\n",
    "    le.fit(trainY)\n",
    "\n",
    "    # Transform both training and development labels to integers.\n",
    "    trainY=le.transform(trainY)\n",
    "    devY=le.transform(devY)\n",
    "    \n",
    "    # Print which class corresponds to the label '1'.\n",
    "    print (\"Class 1 is %s\" % le.inverse_transform([1]))\n",
    "    \n",
    "    # Initialize a Logistic Regression model with specified parameters.\n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    # Train the model on the training data.\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    # Print the accuracy of the model on the development data.\n",
    "    print (\"Accuracy: %.3f\"  % logreg.score(devX_ids, devY))\n",
    "    # Get the model's predictions for the development data.\n",
    "    predictions=logreg.predict(devX_ids)\n",
    "    \n",
    "    # Return the predictions and the true labels for the dev set.\n",
    "    return (predictions, devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `binomial_confidence_intervals` function calculates a parametric confidence interval for accuracy, assuming the errors follow a binomial distribution. It uses the normal approximation to the binomial distribution.\n",
    "1. It calculates the mean accuracy (`success_rate`).\n",
    "2. It finds the Z-score (`z_alpha`) corresponding to the desired confidence level.\n",
    "3. It calculates the standard error of the mean.\n",
    "4. It computes the lower and upper bounds of the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function to calculate a parametric confidence interval for a binomial outcome (e.g., accuracy).\n",
    "def binomial_confidence_intervals(predictions, truth, confidence_level=0.95):\n",
    "    # Initialize a list to store correctness (1 if correct, 0 if incorrect).\n",
    "    correct=[]\n",
    "    # Iterate through predictions and true labels simultaneously.\n",
    "    for pred, gold in zip(predictions, truth):\n",
    "        # Append 1 if the prediction matches the true label, otherwise 0.\n",
    "        correct.append(int(pred==gold))\n",
    "        \n",
    "    # Calculate the success rate (accuracy) as the mean of the 'correct' list.\n",
    "    success_rate=np.mean(correct)\n",
    "\n",
    "    # For a two-tailed test, find the area for each tail.\n",
    "    critical_value=(1-confidence_level)/2\n",
    "    # The ppf (Percent Point Function) finds the z-score for the given cumulative probability.\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    \n",
    "    # The standard error for a binomial distribution is sqrt(p*(1-p)/n).\n",
    "    standard_error=sqrt((success_rate*(1-success_rate))/len(correct))\n",
    "\n",
    "    # Calculate the lower bound of the confidence interval.\n",
    "    lower=success_rate-z_alpha*standard_error\n",
    "    # Calculate the upper bound of the confidence interval.\n",
    "    upper=success_rate+z_alpha*standard_error\n",
    "    # Print the formatted result.\n",
    "    print(\"%.3f, %s%% Confidence interval: [%.3f,%.3f]\" % (success_rate, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple helper function to calculate accuracy by comparing predicted labels to true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function to calculate accuracy.\n",
    "def accuracy(truth, predictions):\n",
    "    # Initialize a counter for correct predictions.\n",
    "    correct=0.\n",
    "    # Iterate through the indices of the labels.\n",
    "    for idx in range(len(truth)):\n",
    "        # Get the true label.\n",
    "        g=truth[idx]\n",
    "        # Get the predicted label.\n",
    "        p=predictions[idx]\n",
    "        # If they match...\n",
    "        if g == p:\n",
    "            # ...increment the counter.\n",
    "            correct+=1\n",
    "    # Return the total correct divided by the total number of items.\n",
    "    return correct/len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specify features for model and train logistic regression**\n",
    "\n",
    "This cell specifies which feature function(s) to use and then calls the `evaluate` function to train the model and get the predictions and true labels for the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list containing the feature function(s) to be used.\n",
    "features=[political_dictionary_feature]\n",
    "# Call the evaluate function to train the model and get results.\n",
    "predictions, truth=evaluate(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell calls the `binomial_confidence_intervals` function to calculate and print the 95% parametric confidence interval for the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the binomial confidence interval for the model's accuracy.\n",
    "binomial_confidence_intervals(predictions, truth, confidence_level=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1: Implement the bootstrap**\n",
    "Implement the bootstrap to create confidence intervals at a specified confidence level for any function `metric(truth, predictions)` where *truth* is an array of true labels for a set of data points, and *predictions* is an array of predicted labels for those same points. See `accuracy(truth, predictions)` above for an example of a metric that should be supported. `bootstrap` should return a tuple of (lower, median, upper), where *lower* is the lower confidence bound, *upper* is the upper confidence bound, and *median* is the median value of the metric among the bootstrap resamples. Hint: see `np.percentile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation of the bootstrap function. It works by:\n",
    "1.  Repeatedly resampling (with replacement) from the set of (true label, prediction) pairs.\n",
    "2.  Calculating the desired metric (e.g., accuracy) for each resampled set.\n",
    "3.  This process creates a distribution of the metric's values.\n",
    "4.  The confidence interval is then determined by taking the percentiles of this distribution. For a 95% confidence interval, we take the 2.5th and 97.5th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the bootstrap function to create non-parametric confidence intervals.\n",
    "def bootstrap(gold, predictions, metric, B=10000, confidence_level=0.95):\n",
    "    # Calculate the critical value for the lower tail.\n",
    "    critical_value=(1-confidence_level)/2\n",
    "    # Convert the critical value to a percentile for the lower bound.\n",
    "    lower_sig=100*critical_value\n",
    "    # Convert the critical value to a percentile for the upper bound.\n",
    "    upper_sig=100*(1-critical_value)\n",
    "    # Create a list of [true_label, predicted_label] pairs.\n",
    "    data=[]\n",
    "    for g, p in zip(gold, predictions):\n",
    "        data.append([g,p])\n",
    "\n",
    "    # Initialize a list to store the metric scores from each bootstrap sample.\n",
    "    metric_scores=[]\n",
    "    \n",
    "    # Loop B times (B is the number of bootstrap resamples).\n",
    "    for b in range(B):\n",
    "        # Create a new sample by choosing with replacement from the original data.\n",
    "        choice=choices(data, k=len(data))\n",
    "        # Convert the chosen sample to a NumPy array for easier slicing.\n",
    "        choice=np.array(choice)\n",
    "        # Calculate the desired metric on the resampled data.\n",
    "        score=metric(choice[:,0], choice[:,1])\n",
    "        \n",
    "        # Append the calculated score to our list of scores.\n",
    "        metric_scores.append(score)\n",
    "    \n",
    "    # Calculate the percentiles corresponding to the lower bound, median, and upper bound.\n",
    "    percentiles=np.percentile(metric_scores, [lower_sig, 50, upper_sig])\n",
    "    \n",
    "    # Extract the lower bound from the percentiles array.\n",
    "    lower=percentiles[0]\n",
    "    # Extract the median from the percentiles array.\n",
    "    median=percentiles[1]\n",
    "    # Extract the upper bound from the percentiles array.\n",
    "    upper=percentiles[2]\n",
    "    \n",
    "    # Return the lower bound, median, and upper bound.\n",
    "    return lower, median, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q2: Use your bootstrap implementation for accuracy**\n",
    "Use your bootstrap implementation to generate confidence intervals for accuracy. How do these compare to the parametric intervals above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell calls the newly implemented `bootstrap` function, passing the `accuracy` function as the metric. It then prints the resulting confidence interval. You can compare this interval to the one generated by the parametric `binomial_confidence_intervals` function; they should be very similar, which gives confidence in the bootstrap implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired confidence level.\n",
    "confidence_level=0.95\n",
    "# Call the bootstrap function with the accuracy metric.\n",
    "lower, median,upper=bootstrap(truth, predictions, accuracy, B=10000,confidence_level=confidence_level)\n",
    "# Print the results in a formatted string.\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q3: Implement the F1 score**\n",
    "Implement the F1 score for binary data. Calculate F1 as the harmonic mean of precision and recall for the positive class (i.e., y=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a function to calculate the F1 score for the positive class (label `1`).\n",
    "- **Precision** is the ratio of true positives to all predicted positives (`correct / trials`).\n",
    "- **Recall** is the ratio of true positives to all actual positives (`correct / trues`).\n",
    "- **F1 Score** is the harmonic mean of precision and recall: `2 * (precision * recall) / (precision + recall)`.\n",
    "The code includes checks to prevent division by zero if there are no predicted positives or no actual positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function to calculate the F1 score for the positive class (label=1).\n",
    "def F1(truth, predictions):\n",
    "    # Initialize counter for true positives (predicted=1, truth=1).\n",
    "    correct=0.\n",
    "    # Initialize counter for predicted positives (predicted=1).\n",
    "    trials=0.\n",
    "    # Initialize counter for actual positives (truth=1).\n",
    "    trues=0.\n",
    "    # Iterate through the indices of the labels.\n",
    "    for idx in range(len(truth)):\n",
    "        # Get the true label.\n",
    "        g=truth[idx]\n",
    "        # Get the predicted label.\n",
    "        p=predictions[idx]\n",
    "        # If it's a true positive...\n",
    "        if g == p and g == 1:\n",
    "            # ...increment the true positive counter.\n",
    "            correct+=1\n",
    "        # If it's an actual positive...\n",
    "        if g == 1:\n",
    "            # ...increment the actual positive counter.\n",
    "            trues+=1\n",
    "        # If it's a predicted positive...\n",
    "        if p == 1:\n",
    "            # ...increment the predicted positive counter.\n",
    "            trials+=1\n",
    "            \n",
    "    # Calculate precision, handling division by zero.\n",
    "    precision=correct/trials if trials > 0 else 0\n",
    "    # Calculate recall, handling division by zero.\n",
    "    recall=correct/trues if trues > 0 else 0\n",
    "    # Calculate F1 score, handling division by zero.\n",
    "    f=(2*precision*recall)/(precision+recall) if (precision+recall) > 0 else 0\n",
    "    # Return the F1 score.\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q4: Use your bootstrap implementation for the F1 score**\n",
    "Use your bootstrap implementation to generate confidence intervals for the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell demonstrates the power of the bootstrap method. The same `bootstrap` function can be used to find a confidence interval for the F1 score simply by passing the `F1` function as the metric. This is a major advantage over parametric methods, which often require complex, metric-specific formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired confidence level.\n",
    "confidence_level=0.95\n",
    "# Call the bootstrap function with the F1 metric.\n",
    "lower, median,upper=bootstrap(truth, predictions, F1, B=10000,confidence_level=confidence_level)\n",
    "# Print the results in a formatted string.\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an empty code cell, left for potential future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
