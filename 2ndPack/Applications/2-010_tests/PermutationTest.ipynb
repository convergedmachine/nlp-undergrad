{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of the permutation test to assess the significance of coefficents learned in logistic regression (testing against the null that each $\\beta$ = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: Imports\n",
    "\n",
    "This first code block imports all the necessary libraries for the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sys module to interact with the Python interpreter.\n",
    "import sys\n",
    "# Import preprocessing from scikit-learn, specifically for LabelEncoder to convert text labels to integers.\n",
    "from sklearn import preprocessing\n",
    "# Import linear_model from scikit-learn, which contains the LogisticRegression classifier.\n",
    "from sklearn import linear_model\n",
    "# Import choices from the random module, although it's not used in the final script.\n",
    "from random import choices\n",
    "# Import CountVectorizer to convert a collection of text documents to a matrix of token counts.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Import the shuffle function to randomly permute the labels for the permutation test.\n",
    "from random import shuffle\n",
    "# Import the NumPy library, which is essential for numerical operations, especially for handling arrays.\n",
    "import numpy as np\n",
    "# Import the copy module to create a deep copy of the labels list, ensuring the original list is not modified.\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 2: Data Reading Function\n",
    "\n",
    "This cell defines a function `read_data` to load the dataset from a tab-separated values (`.tsv`) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named 'read_data' that takes a filename as input.\n",
    "def read_data(filename):\n",
    "    # Initialize an empty list 'X' to store the text data (features).\n",
    "    X=[]\n",
    "    # Initialize an empty list 'Y' to store the corresponding labels.\n",
    "    Y=[]\n",
    "    # Open the specified file with UTF-8 encoding to handle a wide range of characters.\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in file:\n",
    "            # Remove trailing whitespace and split the line into columns by the tab character.\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label.\n",
    "            label=cols[0]\n",
    "            # The second column is the text.\n",
    "            text=cols[1]\n",
    "            # Assumes the text is already tokenized (words are separated by spaces).\n",
    "            # Append the text to the feature list X.\n",
    "            X.append(text)\n",
    "            # Append the label to the label list Y.\n",
    "            Y.append(label)\n",
    "    # Return the lists of features (X) and labels (Y).\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 3: Set Data Directory\n",
    "\n",
    "Here, we define the path to the folder containing our dataset. You should change this string to match the location on your own machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "# Set a variable 'directory' to the path of the data folder.\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 4: Load Training and Development Data\n",
    "\n",
    "This code calls the `read_data` function to load the training and development (validation) sets into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the read_data function to load the training data and store it in trainX and trainY.\n",
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "# Call the read_data function to load the development data and store it in devX and devY.\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 5: Featurization Function\n",
    "\n",
    "The `featurize` function converts the raw text data into a numerical format that the machine learning model can understand. It uses a \"bag-of-words\" approach with binary values (presence or absence of a word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function 'featurize' that takes the raw text of the training and development sets as input.\n",
    "def featurize(trainX, devX):\n",
    "    # Initialize CountVectorizer.\n",
    "    # max_features=10000: Keep only the 10,000 most frequent words.\n",
    "    # analyzer=str.split: Split text into words using whitespace.\n",
    "    # lowercase=False: Do not convert words to lowercase.\n",
    "    # strip_accents=None: Do not remove accents.\n",
    "    # binary=True: Use 1 if a word is present in a document, 0 otherwise (instead of word counts).\n",
    "    vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "\n",
    "    # Fit the vectorizer to the training data to learn the vocabulary and then transform trainX into a feature matrix.\n",
    "    X_train = vectorizer.fit_transform(trainX)\n",
    "    # Transform the development data using the same vocabulary learned from the training data.\n",
    "    X_dev = vectorizer.transform(devX)\n",
    "\n",
    "    # Return the transformed training and development data, and the fitted vectorizer object.\n",
    "    return X_train, X_dev, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 6: Model Training Function\n",
    "\n",
    "This function, `train`, takes the featurized training data and labels, and trains a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function 'train' that takes the training features, training labels, and a LabelEncoder object.\n",
    "def train(X_train, trainY, le):\n",
    "    # Use the LabelEncoder to transform the string labels (e.g., \"positive\", \"negative\") into integers (e.g., 1, 0).\n",
    "    Y_train=le.transform(trainY)\n",
    "    # Initialize the Logistic Regression model.\n",
    "    # C=100: A smaller C specifies stronger regularization. Here, C=100 means relatively weak regularization.\n",
    "    # solver='lbfgs': An efficient optimization algorithm.\n",
    "    # penalty='l2': Use L2 regularization to prevent overfitting.\n",
    "    # max_iter=10000: Allow up to 10,000 iterations for the solver to converge.\n",
    "    logreg = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    # Train the model using the feature matrix (X_train) and the integer labels (Y_train).\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    # Return the trained logistic regression model object.\n",
    "    return logreg\n",
    "    # This line is unreachable because the function already returned on the line above.\n",
    "    return logreg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 7: Model Testing Function\n",
    "\n",
    "The `test` function evaluates the trained model on the development set and prints its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function 'test' to evaluate the model's performance.\n",
    "def test(logreg, devX_feat, devY, le):\n",
    "    # Transform the string labels of the development set into integers.\n",
    "    Y_dev=le.transform(devY)\n",
    "    # Use the model's 'score' method to calculate accuracy on the development set and print it, formatted to 3 decimal places.\n",
    "    print(\"Accuracy: %.3f\" % logreg.score(devX_feat, Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 8: Weight Analysis Function\n",
    "\n",
    "This function, `analyze_weights`, is for interpreting the model. It prints the 25 most influential words for each class, along with their learned coefficients and calculated p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to analyze and display the model's coefficients (weights).\n",
    "def analyze_weights(coefs, label_encoder, vocab, p_values):\n",
    "    # Create a 'reverse_vocab' dictionary to map from feature index back to the word.\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Get the indices that would sort the coefficients array in ascending order.\n",
    "    sort_index = np.argsort(coefs)\n",
    "\n",
    "    # Print the name of the first class (corresponding to negative coefficients).\n",
    "    print(label_encoder.inverse_transform([0])[0])\n",
    "    # Loop through the 25 smallest (most negative) coefficients.\n",
    "    for k in sort_index[:25]:\n",
    "        # Print the coefficient, the corresponding word, and its p-value.\n",
    "        print (\"%.5f\\t%s\\t%.4f\" % (coefs[k], reverse_vocab[k], p_values[k] ))\n",
    "\n",
    "    # Print a newline for spacing.\n",
    "    print()\n",
    "    # Print the name of the second class (corresponding to positive coefficients).\n",
    "    print(label_encoder.inverse_transform([1])[0])\n",
    "\n",
    "    # Loop through the 25 largest (most positive) coefficients in descending order.\n",
    "    for k in reversed(sort_index[-25:]):\n",
    "        # Print the coefficient, the corresponding word, and its p-value.\n",
    "        print (\"%.5f\\t%s\\t%.4f\" % (coefs[k], reverse_vocab[k], p_values[k] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 9: Main Execution and Permutation Test\n",
    "\n",
    "This is the main block where the script executes the entire pipeline: featurization, training, and the permutation test itself.\n",
    "\n",
    "The **permutation test** works by shuffling the labels (`trainY`) and re-training the model many times. For each feature, we count how often the coefficient from a shuffled-label model is more extreme (larger in absolute value) than the coefficient from the original, unshuffled model. This count, divided by the number of permutations, gives us the p-value. A small p-value (e.g., < 0.05) suggests the original coefficient is statistically significant and not just due to random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize the training and development text data.\n",
    "X_train, X_dev, vectorizer=featurize(trainX, devX)\n",
    "# Initialize the LabelEncoder.\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Fit the encoder on the training labels to learn the mapping from strings to integers.\n",
    "le.fit(trainY)\n",
    "\n",
    "# Train the logistic regression model on the original (unshuffled) data.\n",
    "logreg=train(X_train, trainY, le)\n",
    "# Test the model on the development set and print its accuracy.\n",
    "test(logreg, X_dev, devY, le)\n",
    "\n",
    "# Extract the learned coefficients from the trained model. This is our \"true\" set of weights.\n",
    "true_coefficients=logreg.coef_[0]\n",
    "\n",
    "# Set the number of permutations (P). For a real analysis, this should be much higher (e.g., 1000 or 10000).\n",
    "P=100\n",
    "\n",
    "# Initialize a NumPy array of zeros to store the p-value for each coefficient.\n",
    "p_values=np.zeros(len(true_coefficients))\n",
    "# Create a deep copy of the training labels to be used for shuffling.\n",
    "permutedY=copy.deepcopy(trainY)\n",
    "\n",
    "# Start the permutation loop, which will run P times.\n",
    "for i in range(P):\n",
    "    # Print the progress every 10 iterations.\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    # The core of the permutation test: shuffle the labels randomly.\n",
    "    # This breaks the true relationship between the features (X) and the labels (Y).\n",
    "    shuffle(permutedY)\n",
    "    \n",
    "    # Train a new logistic regression model on the original features but with the newly shuffled labels.\n",
    "    permuted_logreg=train(X_train, permutedY, le)\n",
    "    # Extract the coefficients from this new model.\n",
    "    coefficients=permuted_logreg.coef_[0]\n",
    "    \n",
    "    # Compare the coefficients from the permuted model to the true coefficients.\n",
    "    # Iterate through each coefficient index.\n",
    "    for idx, coef in enumerate(coefficients):\n",
    "        # Check if the absolute value of the coefficient from the permuted model is greater than\n",
    "        # the absolute value of the original, true coefficient.\n",
    "        if abs(true_coefficients[idx]) < abs(coef):\n",
    "            # If it is, we count this as an instance where a random permutation produced a more \"extreme\" result.\n",
    "            # Increment the p-value counter for this feature by 1/P.\n",
    "            p_values[idx]+=1./P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 10: Save Results to File\n",
    "\n",
    "This block saves the final results—the true coefficient, the corresponding word, and its calculated p-value—to a text file named `weights.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reverse vocabulary to map feature indices back to words.\n",
    "inverse_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}            \n",
    "# Open a file named \"weights.txt\" in write mode.\n",
    "out=open(\"weights.txt\", \"w\")\n",
    "# Iterate through each of the true coefficients with its index.\n",
    "for idx, coef in enumerate(true_coefficients):\n",
    "    # Write the coefficient, the corresponding word, and its p-value to the file, separated by tabs.\n",
    "    out.write(\"%.3f\\t%s\\t%.5f\\n\" % (coef, inverse_vocab[idx], p_values[idx]))\n",
    "# Close the file to ensure all data is written to disk.\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Cell 11: Display Final Analysis\n",
    "\n",
    "Finally, this cell calls the `analyze_weights` function to print the most significant positive and negative features directly to the notebook's output for immediate review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the analysis function to display the top 25 words for each class, along with their weights and p-values.\n",
    "analyze_weights(true_coefficients, le, vectorizer.vocabulary_, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}