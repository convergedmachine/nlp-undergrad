{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contextualizes accuracy against a majority class baseline, and analyzes the most important features for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Necessary Libraries\n",
    "\n",
    "First, we import the essential libraries for our task.\n",
    "* `collections.Counter` is a handy tool for counting hashable objects, which we'll use to find the most common class label.\n",
    "* `sklearn` (Scikit-learn) provides powerful and easy-to-use tools for machine learning. We import modules for feature extraction (`CountVectorizer`), data preprocessing (`preprocessing`), and our classification model (`linear_model`).\n",
    "* `numpy` is a fundamental package for numerical computation in Python, which we'll use for array manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Counter class from the collections module for counting items in a list.\n",
    "from collections import Counter\n",
    "# Import CountVectorizer to convert text data into a matrix of token counts.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Import the preprocessing module to encode categorical labels into numerical format.\n",
    "from sklearn import preprocessing\n",
    "# Import the linear_model module which contains the Logistic Regression classifier.\n",
    "from sklearn import linear_model\n",
    "# Import the numpy library for efficient numerical operations, especially for handling arrays.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reading the Data\n",
    "\n",
    "This function, `read_data`, is designed to open a tab-separated values (`.tsv`) file and parse its contents. It iterates through each line, splitting it into a label and a text body. It then appends these to two separate lists, `X` (for the text features) and `Y` (for the labels), which are returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read data from a file.\n",
    "def read_data(filename):\n",
    "    # Initialize an empty list to store the text samples (features).\n",
    "    X=[]\n",
    "    # Initialize an empty list to store the corresponding labels.\n",
    "    Y=[]\n",
    "    # Open the specified file with UTF-8 encoding to handle a wide range of characters.\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in file:\n",
    "            # Remove any trailing whitespace (like newline characters) and split the line by the tab character.\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label.\n",
    "            label=cols[0]\n",
    "            # The second column is the text.\n",
    "            # The sample text is already tokenized; if yours is not, do so here.\n",
    "            text=cols[1]\n",
    "            # Add the text to our feature list.\n",
    "            X.append(text)\n",
    "            # Add the label to our label list.\n",
    "            Y.append(label)\n",
    "    # Return the lists of features (X) and labels (Y).\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setting the Data Directory\n",
    "\n",
    "This cell sets a variable that points to the location of your dataset. Make sure this path is correct and that the directory contains the `train.tsv` and `dev.tsv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "# This variable holds the path to the folder containing the data files.\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading Training and Development Sets\n",
    "\n",
    "Here, we use our `read_data` function to load the training and development (validation) datasets from their respective `.tsv` files. The training data (`trainX`, `trainY`) will be used to train our model, and the development data (`devX`, `devY`) will be used to evaluate its performance and tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the read_data function to load the training data.\n",
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "# Call the read_data function to load the development (validation) data.\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Implement the majority class baseline for your data that we went over in `Hyperparameters.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implementing the Majority Class Baseline\n",
    "\n",
    "A **majority class baseline** is a simple model that always predicts the most frequent label from the training set, regardless of the input. It's a fundamental benchmark: any useful machine learning model must perform better than this baseline. This function calculates the majority label and then computes the accuracy of this naive prediction on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the majority class baseline accuracy.\n",
    "def majority_class(trainY, devY):\n",
    "    # Use Counter to count the occurrences of each unique label in the training set.\n",
    "    label_counts = Counter(trainY)\n",
    "    # Find the most common label and its count, and select just the label.\n",
    "    majority_label = label_counts.most_common(1)[0][0]\n",
    "    \n",
    "    # Create a list of predictions where every prediction is the majority label.\n",
    "    # The length of this list is the same as the number of items in the development set.\n",
    "    predictions = [majority_label] * len(devY)\n",
    "    \n",
    "    # Calculate accuracy by comparing the predictions to the true development set labels.\n",
    "    # sum(p == y for p, y in zip(predictions, devY)) counts how many predictions were correct.\n",
    "    correct = sum(p == y for p, y in zip(predictions, devY))\n",
    "    # Accuracy is the number of correct predictions divided by the total number of predictions.\n",
    "    accuracy = correct / len(devY)\n",
    "    \n",
    "    # Return the list of predictions and the calculated accuracy.\n",
    "    return predictions, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluating the Baseline Model\n",
    "\n",
    "This cell executes the `majority_class` function to get the predictions and accuracy for our baseline model. The resulting accuracy `a` shows the score our more sophisticated model needs to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the majority_class function with the training and development labels.\n",
    "# 'p' will store the predictions, and 'a' will store the accuracy.\n",
    "p, a = majority_class(trainY,devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: After experimenting with hyperparameter choices in class, what is the best accuracy that you uncovered on your development data?  Which hyperparameter choices led to that accuracy?  Plug in the values here and execute the cell to yield the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training and Evaluating a Logistic Regression Model\n",
    "\n",
    "This cell contains the complete pipeline for training a Logistic Regression classifier.\n",
    "\n",
    "1.  **Label Encoding**: Machine learning models require numerical input. `LabelEncoder` converts our text-based labels (e.g., 'positive', 'negative') into integers (e.g., 1, 0).\n",
    "2.  **Vectorization**: `CountVectorizer` converts the text documents into a numerical matrix. Each row represents a document, and each column represents a unique word from the vocabulary. A `1` in the matrix indicates the presence of a word in a document (since `binary=True`). We limit the vocabulary to the 10,000 most frequent words to keep the model manageable.\n",
    "3.  **Model Training**: We initialize a `LogisticRegression` model with specific hyperparameters (`C=0.1`, `penalty='l2'`) and train it on the vectorized training data (`X_train`) and encoded labels (`Y_train`).\n",
    "4.  **Evaluation**: Finally, we evaluate the trained model's accuracy on the held-out development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LabelEncoder.\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Fit the encoder on the training labels to learn the mapping from text labels to numbers.\n",
    "le.fit(trainY)\n",
    "# Transform the training labels into their numerical representation.\n",
    "Y_train=le.transform(trainY)\n",
    "# Transform the development labels using the same learned mapping.\n",
    "Y_dev=le.transform(devY)\n",
    "\n",
    "# Initialize CountVectorizer with chosen hyperparameters.\n",
    "# max_features=10000: Use the 10,000 most frequent words as features.\n",
    "# analyzer=str.split: Split text on whitespace (since it's pre-tokenized).\n",
    "# lowercase=False: Do not convert text to lowercase.\n",
    "# strip_accents=None: Do not remove accents.\n",
    "# binary=True: Use 1 for presence and 0 for absence of a word, rather than its frequency.\n",
    "vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "\n",
    "# Fit the vectorizer on the training text to learn the vocabulary and then transform the text into a feature matrix.\n",
    "X_train = vectorizer.fit_transform(trainX)\n",
    "# Transform the development text into a feature matrix using the vocabulary learned from the training data.\n",
    "X_dev = vectorizer.transform(devX)\n",
    "\n",
    "# Initialize the Logistic Regression model.\n",
    "# C=0.1: Sets the inverse of regularization strength. Smaller C means stronger regularization.\n",
    "# solver='lbfgs': An algorithm for optimization.\n",
    "# penalty='l2': Use L2 regularization to prevent overfitting.\n",
    "logreg = linear_model.LogisticRegression(C=0.1, solver='lbfgs', penalty='l2')\n",
    "\n",
    "# Train the model using the training data features (X_train) and labels (Y_train).\n",
    "model=logreg.fit(X_train, Y_train)\n",
    "\n",
    "# Calculate and print the accuracy of the trained model on the development set.\n",
    "print(\"Accuracy: %.3f\" % logreg.score(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: For binary classification using logistic regression, the parameters of the learned model are given in `model.coef_[0]`.  Print out the 25 features that are most associated with each class (i.e., the 25 parameters that have the largest positive values and the 25 parameters with largest negative values).  For reference, consider the `inverse_transform` function in [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder.transform) to get the class labels that correspond to positive(=1) and negative(=0), and the `vocabulary_` function in [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to yield the index for each vocabulary term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Analyzing Model Feature Weights\n",
    "\n",
    "To understand *why* our model makes its predictions, we can inspect its learned weights (coefficients). For a binary logistic regression model, each feature (word) has a single weight.\n",
    "\n",
    "* A large **positive** weight means the presence of that word strongly suggests the positive class (label `1`).\n",
    "* A large **negative** weight means the presence of that word strongly suggests the negative class (label `0`).\n",
    "\n",
    "This function extracts these weights, identifies the top 25 words for each class, and prints them out. This provides valuable insight into the model's decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to analyze and display the most important feature weights.\n",
    "def analyze_weights(learned_model, label_encoder, count_vectorizer):\n",
    "    # Get the array of learned coefficients (weights) from the model for the positive class (class 1).\n",
    "    coefs = learned_model.coef_[0]\n",
    "    \n",
    "    # Get the vocabulary from the vectorizer. The .items() gives (term, index) pairs.\n",
    "    # We sort by index to create an array of terms where the index matches the column in the feature matrix.\n",
    "    vocab = np.array([term for term, idx in sorted(count_vectorizer.vocabulary_.items(), key=lambda x: x[1])])\n",
    "    \n",
    "    # Use the label encoder to find the original string labels for the numerical classes 0 and 1.\n",
    "    class_labels = label_encoder.inverse_transform([0, 1])\n",
    "    \n",
    "    # Get the indices of the coefficients sorted from smallest to largest.\n",
    "    # We take the last 25 and reverse them to get the indices of the 25 largest positive weights.\n",
    "    top_pos_idx = np.argsort(coefs)[-25:][::-1]\n",
    "    # Use these indices to get the corresponding terms from the vocabulary array.\n",
    "    top_pos_terms = vocab[top_pos_idx]\n",
    "    \n",
    "    # Get the indices of the 25 smallest (most negative) coefficients.\n",
    "    top_neg_idx = np.argsort(coefs)[:25]\n",
    "    # Use these indices to get the corresponding terms from the vocabulary array.\n",
    "    top_neg_terms = vocab[top_neg_idx]\n",
    "    \n",
    "    # Print the header for the positive class features.\n",
    "    print(f\"\\nTop 25 features for class '{class_labels[1]}' (positive weights):\")\n",
    "    # Loop through the top positive terms and their corresponding weights.\n",
    "    for term, weight in zip(top_pos_terms, coefs[top_pos_idx]):\n",
    "        # Print the term and its weight, formatted for alignment and readability.\n",
    "        print(f\"{term:20s} {weight:.4f}\")\n",
    "    \n",
    "    # Print the header for the negative class features.\n",
    "    print(f\"\\nTop 25 features for class '{class_labels[0]}' (negative weights):\")\n",
    "    # Loop through the top negative terms and their corresponding weights.\n",
    "    for term, weight in zip(top_neg_terms, coefs[top_neg_idx]):\n",
    "        # Print the term and its weight, formatted for alignment and readability.\n",
    "        print(f\"{term:20s} {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Running the Weight Analysis\n",
    "\n",
    "This final cell calls the `analyze_weights` function, passing it the trained model, the label encoder, and the count vectorizer. This will print the lists of the most influential words for each class, completing our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the analysis function with the trained model and the fitted encoder and vectorizer.\n",
    "analyze_weights(model, le, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
