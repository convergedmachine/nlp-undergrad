{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook explores the concept of **distributional similarity**. The underlying idea is that words that appear in similar contexts tend to have similar meanings. We will build high-dimensional, sparse vector representations for words based on the contexts they appear in. These vectors will then be used to find similar words and interpret the specific contexts that contribute to their similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores distribitional simliarity in a dataset of 10,000 Wikipedia articles (4.4M words), building high-dimensional, sparse representations for words from the distinct contexts they appear in.  These representations allow for analysis of the most similar words to a given query, and are interpretable with respect to the specific contexts that are most important for determining that two words are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "\n",
    "First, we import the necessary Python libraries. \n",
    "- `collections.defaultdict`: A dictionary that provides a default value for a non-existent key.\n",
    "- `collections.Counter`: A dictionary subclass for counting hashable objects.\n",
    "- `math`: Provides access to basic mathematical functions.\n",
    "- `operator`: Provides functions corresponding to Python's intrinsic operators (e.g., for sorting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary that assigns a default value (e.g., 0) to a new key\n",
    "from collections import defaultdict, Counter\n",
    "# For mathematical operations like square root\n",
    "import math\n",
    "# Used for sorting dictionaries by their values\n",
    "import operator\n",
    "# Not used in this version, but can be useful for reading compressed files\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration\n",
    "\n",
    "We set two global parameters:\n",
    "- `window`: Defines the size of the context window around a target word. A window of 2 means we look at the 2 words to the left and 2 words to the right.\n",
    "- `vocabSize`: The number of most frequent words for which we will create vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of words to the left and right to be considered as context\n",
    "window=2\n",
    "# We will only create representations for the 10,000 most common words\n",
    "vocabSize=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Loading\n",
    "\n",
    "We load the text data from a file. The text is converted to lowercase and split into a list of words (tokens) to make processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "filename=\"../data/wiki.10K.txt\"\n",
    "\n",
    "# Open the file, read its content, convert to lowercase, and split into a list of words\n",
    "wiki_data=open(filename, encoding=\"utf-8\").read().lower().split(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vocabulary Creation\n",
    "\n",
    "The `create_vocab` function identifies the `vocabSize` most frequent words in the dataset. It then initializes a dictionary called `word_representations`. Each key in this dictionary is one of the top words, and its value is an empty `defaultdict`. This inner dictionary will eventually store the context counts for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll only create word representation for the most frequent K words\n",
    "\n",
    "def create_vocab(data):\n",
    "    # Initialize the main dictionary to hold our word vectors\n",
    "    word_representations={}\n",
    "    # Use Counter to count the frequency of each word in the data\n",
    "    vocab=Counter()\n",
    "    for i, word in enumerate(data):\n",
    "        vocab[word]+=1\n",
    "\n",
    "    # Get a list of the most common words, limited by vocabSize\n",
    "    topK=[k for k,v in vocab.most_common(vocabSize)]\n",
    "    # For each of the top words, initialize its representation as an empty defaultdict\n",
    "    # This inner dict will map context -> count\n",
    "    for k in topK:\n",
    "        word_representations[k]=defaultdict(float)\n",
    "    return word_representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Context Counting Strategies\n",
    "\n",
    "The notebook defines two different ways to count the \"context\" of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Unigram Context (`count_unigram_context`)\n",
    "\n",
    "This function defines the context as a \"bag of words\". It iterates through the data, and for each word in our vocabulary, it looks at the words in the surrounding window. It then increments the count for each individual neighboring word in the target word's context vector. The position or order of the context words is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word representation for a word = its unigram distributional context (the unigrams that show\n",
    "# up in a window before and after its occurence)\n",
    "\n",
    "def count_unigram_context(data, word_representations):\n",
    "    # Iterate through each word in the dataset with its index\n",
    "    for i, word in enumerate(data):\n",
    "        # Only process words that are in our target vocabulary\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        # Define the start of the context window, ensuring it doesn't go below 0\n",
    "        start=i-window if i-window > 0 else 0\n",
    "        # Define the end of the context window, ensuring it doesn't exceed the data length\n",
    "        end=i+window+1 if i+window+1 < len(data) else len(data)\n",
    "        # Iterate through the context window\n",
    "        for j in range(start, end):\n",
    "            # Make sure we don't count the word itself as its own context\n",
    "            if i != j:\n",
    "                # Increment the count for the context word (data[j]) for the target word (word)\n",
    "                word_representations[word][data[j]]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Directional Context (`count_directional_context`)\n",
    "\n",
    "This function uses a more sophisticated definition of context. Instead of treating context words individually, it captures the entire sequence of words to the left and to the right of the target word. It also prepends \"L:\" or \"R:\" to distinguish between left and right contexts. This captures word order and direction, creating more specific and potentially more informative context features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_directional_context(data, word_representations):\n",
    "    # Iterate through each word in the dataset with its index\n",
    "    for i, word in enumerate(data):\n",
    "        # Only process words that are in our target vocabulary\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        # Define the start and end of the context window\n",
    "        start=i-window if i-window > 0 else 0\n",
    "        end=i+window+1 if i+window+1 < len(data) else len(data)\n",
    "        \n",
    "        # Create the left context string: all words from 'start' to the target word 'i'\n",
    "        left=\"L: %s\" % ' '.join(data[start:i])\n",
    "        # Create the right context string: all words from after 'i' to the 'end'\n",
    "        right=\"R: %s\" % ' '.join(data[i+1:end])\n",
    "        \n",
    "        # Increment the count for the left and right context strings\n",
    "        word_representations[word][left]+=1\n",
    "        word_representations[word][right]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Vector Normalization\n",
    "\n",
    "The `normalize` function converts each word's context count vector into a unit vector (a vector with a length or L2 norm of 1). This is a crucial step for using cosine similarity. When vectors are normalized, their dot product is mathematically equivalent to their cosine similarity, which simplifies the calculation significantly. \n",
    "\n",
    "$$ \\cos(\\theta) = {A \\cdot B \\over ||A|| ||B||} $$\n",
    "\n",
    "If `||A||` and `||B||` are both 1, then:\n",
    "\n",
    "$$ \\cos(\\theta) = A \\cdot B $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a word represenatation vector that its L2 norm is 1.\n",
    "# we do this so that the cosine similarity reduces to a simple dot product\n",
    "\n",
    "def normalize(word_representations):\n",
    "    # Iterate through each word in our vocabulary\n",
    "    for word in word_representations:\n",
    "        total=0\n",
    "        # Calculate the sum of the squares of the context counts (the squared magnitude)\n",
    "        for key in word_representations[word]:\n",
    "            total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        # The L2 norm is the square root of the sum of squares\n",
    "        total=math.sqrt(total)\n",
    "        \n",
    "        # If the total is zero (word has no context), skip to avoid division by zero\n",
    "        if total == 0:\n",
    "            continue\n",
    "            \n",
    "        # Divide each context count by the L2 norm to create a unit vector\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]/=total\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Similarity Calculation\n",
    "\n",
    "The following functions implement the cosine similarity logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Dot Product\n",
    "\n",
    "This function, `dictionary_dot_product`, calculates the dot product between two sparse vectors that are represented as dictionaries. It iterates through the keys (contexts) of the first dictionary and, if the same key exists in the second, multiplies their values and adds the result to the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot=0\n",
    "    # Iterate through the context keys in the first word's vector\n",
    "    for key in dict1:\n",
    "        # If the same context exists in the second word's vector\n",
    "        if key in dict2:\n",
    "            # Multiply their values and add to the dot product\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Finding All Similarities\n",
    "\n",
    "The `find_sim` function takes a query word and calculates its cosine similarity (via dot product, since vectors are normalized) with every other word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim(word_representations, query):\n",
    "    # Check if the query word is in our vocabulary\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    # Dictionary to store similarity scores for all other words\n",
    "    scores={}\n",
    "    # Iterate through every word in the vocabulary\n",
    "    for word in word_representations:\n",
    "        # Calculate the cosine similarity (dot product of normalized vectors)\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        # Store the score\n",
    "        scores[word]=cosine\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Finding Nearest Neighbors\n",
    "\n",
    "This function, `find_nearest_neighbors`, ties everything together. It uses `find_sim` to get all similarity scores for a query word, then sorts them in descending order to find the `K` most similar words (the \"nearest neighbors\") and prints them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    # Get similarity scores for the query against all words\n",
    "    scores=find_sim(word_representations, query)\n",
    "    # If scores were calculated successfully\n",
    "    if scores != None:\n",
    "        # Sort the scores dictionary by value in descending order\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        # Print the top K most similar words and their scores\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Initial Model Run (Without TF-IDF)\n",
    "\n",
    "Now we run the pipeline using the `count_directional_context` strategy. We create the vocabulary, count the contexts, and normalize the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the difference between `count_unigram_context` and `count_directional_context` for determining what counts as \"context\".  `count_unigram_context` counts an individual unigram in the bag of words around a target as a \"context\" variable, while `count_directional_context` counts the sequence of words before and after the word as a single \"context\"--and specifies the direction it occurs (to the left or right of the word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the vocabulary and word representation structure\n",
    "word_representations=create_vocab(wiki_data)\n",
    "# Step 2: Populate the structure with directional context counts\n",
    "count_directional_context(wiki_data, word_representations)\n",
    "# Step 3: Normalize the context vectors for cosine similarity calculation\n",
    "normalize(word_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Nearest Neighbors for \"actor\"\n",
    "\n",
    "Let's test our model by finding the nearest neighbors for the word \"actor\". The results show other professions, which is a good sign that the model is capturing semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tactor\t1.00000\n",
      "1\tpolitician\t0.54099\n",
      "2\tactress\t0.52242\n",
      "3\tcricketer\t0.42361\n",
      "4\tartist\t0.40005\n",
      "5\twriter\t0.38234\n",
      "6\tcyclist\t0.36833\n",
      "7\tmusician\t0.33385\n",
      "8\tdiplomat\t0.32010\n",
      "9\tpoet\t0.31124\n"
     ]
    }
   ],
   "source": [
    "find_nearest_neighbors(word_representations, \"actor\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Interpreting Similarity\n",
    "\n",
    "The `find_shared_contexts` function helps us understand *why* two words are considered similar. It finds the common contexts between two words and calculates how much each shared context contributes to their total cosine similarity score (which is the product of their values in the normalized vectors). It then prints the contexts with the highest contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the contexts shared between two words that have the most contribution\n",
    "# to the cosine similarity\n",
    "\n",
    "def find_shared_contexts(word_representations, query1, query2, K):\n",
    "    # Check if the first query word is in our vocabulary\n",
    "    if query1 not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query1)\n",
    "        return None\n",
    "    \n",
    "    # Check if the second query word is in our vocabulary\n",
    "    if query2 not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query2)\n",
    "        return None\n",
    "    \n",
    "    # Dictionary to store the contribution of each shared context\n",
    "    context_scores={}\n",
    "    dict1=word_representations[query1]\n",
    "    dict2=word_representations[query2]\n",
    "    \n",
    "    # Iterate through the contexts of the first word\n",
    "    for key in dict1:\n",
    "        # If the context is also present for the second word\n",
    "        if key in dict2:\n",
    "            # The contribution to the dot product is the product of their values\n",
    "            score=dict1[key]*dict2[key]\n",
    "            context_scores[key]=score\n",
    "\n",
    "    # Sort the contexts by their contribution score in descending order\n",
    "    sorted_x = sorted(context_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # Print the top K shared contexts\n",
    "    for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "        print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 Shared Contexts for \"actor\" and \"politician\"\n",
    "\n",
    "Here, we inspect the shared contexts between \"actor\" and \"politician\". The results show contexts like \"an american\", \"an indian\", and sentence-ending patterns. This indicates that both words are often used to describe a person's nationality or role in a similar grammatical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tR: . he\t0.21961\n",
      "1\tL: an american\t0.13391\n",
      "2\tR: ) .\t0.11417\n",
      "3\tR: in the\t0.01410\n",
      "4\tL: an indian\t0.00761\n",
      "5\tL: a canadian\t0.00677\n",
      "6\tL: an english\t0.00564\n",
      "7\tR: of the\t0.00564\n",
      "8\tL: a french\t0.00423\n",
      "9\tR: , who\t0.00338\n"
     ]
    }
   ],
   "source": [
    "find_shared_contexts(word_representations, \"actor\", \"politician\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Improving the Model with TF-IDF\n",
    "\n",
    "A problem with raw counts is that very common contexts (e.g., being preceded by \"the\") dominate the similarity score but carry little semantic meaning. **TF-IDF (Term Frequency-Inverse Document Frequency)** is a weighting scheme that addresses this. It increases the weight of contexts that are frequent for a specific word (high TF) but rare across all other words (high IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Fill out a function `scale_tfidf` below.  This function takes as input a dict of word_representations and scales the value for each context in `word_representations[word]` by its tf-idf score.  Use the term frequency for tf and ${N \\over |\\{d \\in D : t \\in d\\}|}$ for idf.  Here, tf measure the count of a *context* term for a particular *word*, and idf measures the number of distinct *words* a particular *context* is seen with.  This function should modify `word_representations` in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1 `scale_tfidf` Implementation\n",
    "\n",
    "This function implements the TF-IDF scaling.\n",
    "1.  **Calculate IDF**: It first iterates through all words and their contexts to build an IDF dictionary. This dictionary (`idfs`) maps each context to the number of *distinct words* it appears with.\n",
    "2.  **Apply TF-IDF Weighting**: It then iterates through the word representations again. For each context of each word, it multiplies the existing value (the TF) by the IDF score. The IDF score is calculated as `log(N / idfs[context])`, where `N` is the total number of words in our vocabulary. The logarithm is used to dampen the effect of the IDF score, preventing it from becoming too overwhelmingly large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_tfidf(word_representations):\n",
    "    # --- IDF CALCULATION ---\n",
    "    # This will store the document frequency for each context (how many words it appears with)\n",
    "    idfs=defaultdict(int)\n",
    "    # For each word in our vocabulary\n",
    "    for term in word_representations:\n",
    "        # For each context of that word\n",
    "        for context in word_representations[term]:\n",
    "            # Increment the count for that context, signifying it appeared with one more word\n",
    "            idfs[context]+=1\n",
    "            \n",
    "    # --- TF-IDF SCALING ---\n",
    "    # Total number of documents (in our case, words in the vocabulary)\n",
    "    N=len(word_representations)\n",
    "    # For each word in our vocabulary\n",
    "    for word in word_representations:\n",
    "        # For each context associated with that word\n",
    "        for key in word_representations[word]:\n",
    "            # The current value is the Term Frequency (TF)\n",
    "            # The IDF is log(N / number of words this context appears with)\n",
    "            # We multiply the TF by the IDF score to get the final TF-IDF weight\n",
    "            word_representations[word][key]*=math.log(N/idfs[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Model Run with TF-IDF\n",
    "\n",
    "Now we repeat the pipeline, but this time we add the `scale_tfidf` step before normalizing the vectors. This will re-weight the context counts to emphasize more meaningful contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the vocabulary and word representation structure\n",
    "tf_idf_word_representations=create_vocab(wiki_data)\n",
    "# Step 2: Populate the structure with directional context counts\n",
    "count_directional_context(wiki_data, tf_idf_word_representations)\n",
    "# Step 3: Re-weight the context counts using TF-IDF\n",
    "scale_tfidf(tf_idf_word_representations)\n",
    "# Step 4: Normalize the new TF-IDF vectors for cosine similarity calculation\n",
    "normalize(tf_idf_word_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Compare the results the results of tf-idf scaling with the non-scaled results above.  How does scaling change the quality of the nearest neighbors, or the sensibility of the significant contexts?  Provide examples to support your claims using `find_nearest_neighbors` and `find_shared_contexts` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Comparing Results\n",
    "\n",
    "Now we answer Q2 by directly comparing the outputs of the two models for the query \"two\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.1 Nearest Neighbors Comparison for \"two\"\n",
    "\n",
    "**Observation:**\n",
    "-   **Without TF-IDF (Top Output):** The nearest neighbors for \"two\" are a mix of semantically unrelated words like \"term\", \"song\", \"church\", and \"company\". While it finds other numbers (\"three\", \"four\"), the list is not very coherent. This is likely because all these words share common but generic grammatical contexts.\n",
    "-   **With TF-IDF (Bottom Output):** The results are dramatically better. The nearest neighbors are almost exclusively other numbers (\"three\", \"four\", \"five\", \"six\", etc.).\n",
    "\n",
    "**Conclusion:** TF-IDF scaling significantly improves the quality of nearest neighbors. By down-weighting common, non-descriptive contexts, it allows the model to focus on the more unique contexts that truly define the meaning of a word, leading to more semantically coherent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\ttwo\t1.00000\n",
      "1\tthree\t0.68766\n",
      "2\tfour\t0.61315\n",
      "3\tterm\t0.54894\n",
      "4\tsong\t0.54867\n",
      "5\tchurch\t0.54464\n",
      "6\tmain\t0.54448\n",
      "7\tcompany\t0.54311\n",
      "8\tband\t0.54142\n",
      "9\tbuilding\t0.54053\n",
      "\n",
      "0\ttwo\t1.00000\n",
      "1\tthree\t0.42725\n",
      "2\tfour\t0.33787\n",
      "3\tfive\t0.31393\n",
      "4\tseveral\t0.23219\n",
      "5\tsix\t0.22343\n",
      "6\tfew\t0.22313\n",
      "7\teight\t0.22192\n",
      "8\tseven\t0.20937\n",
      "9\ttwenty\t0.18513\n"
     ]
    }
   ],
   "source": [
    "query=\"two\"\n",
    "# Find neighbors using the original, non-scaled representations\n",
    "find_nearest_neighbors(word_representations, query, 10)\n",
    "print()\n",
    "# Find neighbors using the TF-IDF scaled representations\n",
    "find_nearest_neighbors(tf_idf_word_representations, query, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.2 Shared Contexts Comparison for \"two\" and \"three\"\n",
    "\n",
    "**Observation:**\n",
    "-   **Without TF-IDF (Top Output):** The most important shared contexts are extremely common, generic phrases like \"L: of the\", \"L: . the\", and \"R: of the\". These don't tell us much about what makes \"two\" and \"three\" similar, other than that they appear in grammatically similar places.\n",
    "-   **With TF-IDF (Bottom Output):** The top shared contexts are much more descriptive and sensible. Contexts like \"R: years later\", \"R: days later\", and \"L: divided into\" are highly relevant to numbers. This shows that the model has learned that numbers often appear in temporal or quantitative phrases.\n",
    "\n",
    "**Conclusion:** TF-IDF makes the significant contexts far more interpretable and meaningful. It successfully filters out grammatical noise and highlights the specific, semantically rich contexts that define the relationship between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tL: of the\t0.15490\n",
      "1\tL: . the\t0.09570\n",
      "2\tR: of the\t0.04114\n",
      "3\tL: the first\t0.03932\n",
      "4\tL: , and\t0.03102\n",
      "5\tL: there are\t0.03037\n",
      "6\tR: years later\t0.02998\n",
      "7\tL: , the\t0.02032\n",
      "8\tL: one of\t0.01649\n",
      "9\tR: years ,\t0.01081\n",
      "\n",
      "0\tR: years later\t0.05778\n",
      "1\tL: there are\t0.02744\n",
      "2\tR: days later\t0.01653\n",
      "3\tL: one of\t0.01490\n",
      "4\tR: years ,\t0.01324\n",
      "5\tR: years .\t0.01194\n",
      "6\tL: the first\t0.01144\n",
      "7\tR: or more\t0.01066\n",
      "8\tL: the next\t0.01057\n",
      "9\tL: divided into\t0.01024\n"
     ]
    }
   ],
   "source": [
    "query1=\"two\"\n",
    "query2=\"three\"\n",
    "# Find shared contexts using the original representations\n",
    "find_shared_contexts(word_representations, query1, query2, 10)\n",
    "print()\n",
    "# Find shared contexts using the TF-IDF scaled representations\n",
    "find_shared_contexts(tf_idf_word_representations, query1, query2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
