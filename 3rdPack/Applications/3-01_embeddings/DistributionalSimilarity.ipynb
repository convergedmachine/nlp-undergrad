{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "This notebook explores distributional similarity. The core idea is that words that appear in similar contexts tend to have similar meanings. We will represent words as vectors based on the words that appear around them and then use these vectors to find similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 1: Imports**\n",
    "This cell imports the necessary Python libraries for our tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import defaultdict for creating dictionaries that can handle missing keys\n",
    "from collections import defaultdict, Counter\n",
    "# Import math for mathematical operations, like square root\n",
    "import math\n",
    "# Import operator for easily sorting dictionaries by value\n",
    "import operator\n",
    "# Import gzip for handling compressed files (though not used in this version)\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 2: Global Parameters**\n",
    "Here, we define two key parameters that will control our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'window' defines the size of the context window around a target word.\n",
    "# A value of 2 means we'll look at 2 words to the left and 2 words to the right.\n",
    "window=2\n",
    "# 'vocabSize' limits our analysis to the 10,000 most frequent words in the text.\n",
    "# This makes computations more manageable and focuses on more meaningful words.\n",
    "vocabSize=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 3: Loading the Data**\n",
    "This cell loads the text data from a file. The data is a collection of Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the data file.\n",
    "filename=\"../data/wiki.10K.txt\"\n",
    "\n",
    "# Open the file, read its entire content, convert all text to lowercase,\n",
    "# and then split the text into a list of words based on spaces.\n",
    "wiki_data=open(filename, encoding=\"utf-8\").read().lower().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 4: Creating the Vocabulary**\n",
    "This function identifies the most frequent words in our dataset and prepares a data structure to store their contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll only create word representations for the most frequent K words (where K=vocabSize).\n",
    "\n",
    "def create_vocab(data):\n",
    "    # This will store our final word representations.\n",
    "    word_representations={}\n",
    "    # Use Counter to efficiently count the frequency of each word in the data.\n",
    "    vocab=Counter()\n",
    "    # Iterate through the data to populate the Counter.\n",
    "    for i, word in enumerate(data):\n",
    "        vocab[word]+=1\n",
    "\n",
    "    # Get a list of the most common words, limited by vocabSize.\n",
    "    topK=[k for k,v in vocab.most_common(vocabSize)]\n",
    "    # For each of the top K words, initialize its representation as a defaultdict.\n",
    "    # A defaultdict(float) will return 0.0 for any context word not yet seen.\n",
    "    for k in topK:\n",
    "        word_representations[k]=defaultdict(float)\n",
    "    # Return the dictionary that will hold the context vectors for our vocabulary words.\n",
    "    return word_representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 5: Counting Unigram Context**\n",
    "This function populates the `word_representations` dictionary. For each word in our vocabulary, it counts the individual words (unigrams) that appear within its context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A word's representation is its unigram distributional context (the individual words\n",
    "# that appear in a window before and after its occurrences).\n",
    "\n",
    "def count_unigram_context(data, word_representations):\n",
    "    # Iterate through each word and its index in the dataset.\n",
    "    for i, word in enumerate(data):\n",
    "        # If the word is not in our top K vocabulary, skip it.\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        # Define the start of the context window, ensuring it doesn't go below 0.\n",
    "        start=i-window if i-window > 0 else 0\n",
    "        # Define the end of the context window, ensuring it doesn't exceed the data length.\n",
    "        end=i+window+1 if i+window+1 < len(data) else len(data)\n",
    "        # Iterate through the words within the context window.\n",
    "        for j in range(start, end):\n",
    "            # Make sure we don't count the target word itself as part of its own context.\n",
    "            if i != j:\n",
    "                # Increment the count for the context word (data[j]) in the representation of the target word (word).\n",
    "                word_representations[word][data[j]]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 6: Counting Directional Context (Alternative Method)**\n",
    "This is an alternative way to define \"context.\" Instead of counting individual words, it treats the entire sequence of words to the left and right of the target word as unique context features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines context differently, by preserving the order and direction of surrounding words.\n",
    "\n",
    "def count_directional_context(data, word_representations):\n",
    "    # Iterate through each word and its index in the dataset.\n",
    "    for i, word in enumerate(data):\n",
    "        # If the word is not in our top K vocabulary, skip it.\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        # Define the start and end of the context window.\n",
    "        start=i-window if i-window > 0 else 0\n",
    "        end=i+window+1 if i+window+1 < len(data) else len(data)\n",
    "        # Create a string for the left context, prefixed with \"L:\".\n",
    "        left=\"L: %s\" % ' '.join(data[start:i])\n",
    "        # Create a string for the right context, prefixed with \"R:\".\n",
    "        right=\"R: %s\" % ' '.join(data[i+1:end])\n",
    "        \n",
    "        # Increment the count for the left context string.\n",
    "        word_representations[word][left]+=1\n",
    "        # Increment the count for the right context string.\n",
    "        word_representations[word][right]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 7: Normalizing Vectors**\n",
    "To calculate cosine similarity efficiently, we first need to normalize each word's context vector so that its length (L2 norm) is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a word representation vector so that its L2 norm is 1.\n",
    "# We do this so that the cosine similarity calculation reduces to a simple dot product.\n",
    "\n",
    "def normalize(word_representations):\n",
    "    # Iterate through each word in our vocabulary.\n",
    "    for word in word_representations:\n",
    "        # Initialize a variable to hold the sum of squares.\n",
    "        total=0\n",
    "        # Iterate through the context words and their counts for the current word.\n",
    "        for key in word_representations[word]:\n",
    "            # Add the square of the count to the total.\n",
    "            total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        # The L2 norm is the square root of the sum of squares.\n",
    "        total=math.sqrt(total)\n",
    "        # If the total is zero (word never appeared with context), skip division.\n",
    "        if total == 0: continue\n",
    "        # Iterate through the context words again to perform the normalization.\n",
    "        for key in word_representations[word]:\n",
    "            # Divide each context count by the L2 norm.\n",
    "            word_representations[word][key]/=total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 8: Calculating the Dot Product**\n",
    "This function calculates the dot product between two word vectors. Since the vectors are stored as dictionaries (sparse representation), we only need to consider the context words they have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the dot product between two dictionaries.\n",
    "def dictionary_dot_product(dict1, dict2):\n",
    "    # Initialize the dot product score to 0.\n",
    "    dot=0\n",
    "    # Iterate through the context words (keys) in the first dictionary.\n",
    "    for key in dict1:\n",
    "        # If the same context word exists in the second dictionary...\n",
    "        if key in dict2:\n",
    "            # ...multiply their corresponding values and add to the dot product.\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    # Return the final dot product score.\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 9: Finding Similarity Scores**\n",
    "This function calculates the cosine similarity between a given `query` word and all other words in our vocabulary. Since we normalized our vectors, this is simply their dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the similarity between a query word and all other words.\n",
    "def find_sim(word_representations, query):\n",
    "    # Check if the query word is in our vocabulary.\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    # Initialize a dictionary to store similarity scores.\n",
    "    scores={}\n",
    "    # Iterate through every word in our vocabulary.\n",
    "    for word in word_representations:\n",
    "        # Calculate the cosine similarity (dot product of normalized vectors).\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        # Store the similarity score.\n",
    "        scores[word]=cosine\n",
    "    # Return the dictionary of scores.\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 10: Finding Nearest Neighbors**\n",
    "This function uses the similarity scores to find and display the `K` words that are most similar to the `query` word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the K words with the highest cosine similarity to a query.\n",
    "\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    # Get the similarity scores for the query word against all other words.\n",
    "    scores=find_sim(word_representations, query)\n",
    "    # If scores were successfully calculated...\n",
    "    if scores is not None:\n",
    "        # Sort the words by their similarity score in descending order.\n",
    "        # operator.itemgetter(1) sorts by the dictionary's values.\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        # Iterate through the top K items in the sorted list.\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            # Print the rank, word, and similarity score, formatted neatly.\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 11: Comparing Context Methods**\n",
    "The following cells will demonstrate the `count_unigram_context` method. `count_unigram_context` treats individual words in the context window as features (a \"bag-of-words\" approach). In contrast, `count_directional_context` treats the entire left or right sequence as a single, unique feature, preserving word order. We will proceed with the unigram method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 12: Building the Word Representations**\n",
    "Now, we execute the functions to build our word representations. This involves three steps:\n",
    "1.  `create_vocab`: Identify the top 10,000 words.\n",
    "2.  `count_unigram_context`: Count their surrounding context words.\n",
    "3.  `normalize`: Normalize the resulting context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the vocabulary and initialize the representation dictionary.\n",
    "word_representations=create_vocab(wiki_data)\n",
    "# 2. Populate the dictionary with unigram context counts.\n",
    "count_unigram_context(wiki_data, word_representations)\n",
    "# 3. Normalize the context vectors to have a length of 1.\n",
    "normalize(word_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 13: Finding Similar Words for \"actor\"**\n",
    "Let's test our model by finding the 10 most similar words to \"actor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print the 10 nearest neighbors to the word \"actor\".\n",
    "find_nearest_neighbors(word_representations, \"actor\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 14: Finding Shared Contexts**\n",
    "This function helps us understand *why* two words are considered similar. It identifies the specific context words that contribute the most to their high cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the contexts shared between two words that contribute most to their similarity score.\n",
    "\n",
    "def find_shared_contexts(word_representations, query1, query2, K):\n",
    "    # Check if the first query word is in our vocabulary.\n",
    "    if query1 not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query1)\n",
    "        return None\n",
    "    \n",
    "    # Check if the second query word is in our vocabulary.\n",
    "    if query2 not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query2)\n",
    "        return None\n",
    "    \n",
    "    # Initialize a dictionary to store the contribution score of each shared context.\n",
    "    context_scores={}\n",
    "    # Get the vector for the first word.\n",
    "    dict1=word_representations[query1]\n",
    "    # Get the vector for the second word.\n",
    "    dict2=word_representations[query2]\n",
    "    \n",
    "    # Iterate through the context words of the first query.\n",
    "    for key in dict1:\n",
    "        # If the context word also appears in the second query's context...\n",
    "        if key in dict2:\n",
    "            # The contribution is the product of their normalized weights.\n",
    "            score=dict1[key]*dict2[key]\n",
    "            # Store this contribution score.\n",
    "            context_scores[key]=score\n",
    "\n",
    "    # Sort the shared contexts by their contribution score in descending order.\n",
    "    sorted_x = sorted(context_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # Iterate through the top K shared contexts.\n",
    "    for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "        # Print the rank, context word, and its contribution score.\n",
    "        print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 15: Analyzing Similarity between \"actor\" and \"politician\"**\n",
    "Now, let's find the top 10 shared contexts that make \"actor\" and \"politician\" similar in this dataset. This makes the model's reasoning interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print the top 10 shared contexts between \"actor\" and \"politician\".\n",
    "find_shared_contexts(word_representations, \"actor\", \"politician\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 16: Empty Cell**\n",
    "This is an empty cell, which can be used for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
