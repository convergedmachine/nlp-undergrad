{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initial Setup and Library Installation**\n",
    "\n",
    "This notebook explores word embeddings using the Gensim library. We will train our own Word2Vec model from a small text corpus and then compare its performance with pre-trained GloVe embeddings.\n",
    "\n",
    "Before getting started, you need to install the `gensim` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "conda install gensim=3.4.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Necessary Libraries**\n",
    "\n",
    "This cell imports all the required modules and functions.\n",
    "* `re`: The regular expression library, which we'll use for text cleaning.\n",
    "* `Word2Vec`: The class from Gensim used to train a Word2Vec model.\n",
    "* `KeyedVectors`: A class used to store and query word vectors efficiently, often used for loading pre-trained models.\n",
    "* `glove2word2vec`: A utility script to convert word vectors from the GloVe text format to the Word2Vec format that Gensim can load.\n",
    "* `datapath`: A helper function to locate test datasets that come bundled with Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expression module for text processing\n",
    "import re\n",
    "# Import the main Word2Vec model class and the KeyedVectors class from gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "# Import a utility to convert GloVe embeddings to the Word2Vec format\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# Import a utility to access test datasets included with gensim\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Training a New Word2Vec Model**\n",
    "\n",
    "First, let's train a new Word2Vec model on our own data. This involves two steps:\n",
    "1.  **Preprocessing the Data**: We'll read a text file, convert it to lowercase, clean it up, and split it into a list of sentences (where each sentence is a list of words). This format is required by Gensim.\n",
    "2.  **Training the Model**: We'll feed this processed data into the `Word2Vec` model to learn vector representations for the words in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loading and Preprocessing the Corpus**\n",
    "\n",
    "Here, we open a text file (`wiki.10K.txt`), read it line by line, and process each line to create a list of tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold our sentences\n",
    "sentences=[]\n",
    "# Define the path to our text data file\n",
    "filename=\"../data/wiki.10K.txt\"\n",
    "# Open the file for reading\n",
    "with open(filename) as file:\n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Remove any trailing whitespace (like newlines) and convert the line to lowercase\n",
    "        words=line.rstrip().lower()\n",
    "        # Use a regular expression to replace any sequence of one or more whitespace characters with a single space\n",
    "        words=re.sub(\"\\\\s+\", \" \", words)\n",
    "        # Split the cleaned line into a list of words (tokens) and append it to our sentences list\n",
    "        sentences.append(words.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instantiating and Training the Model**\n",
    "\n",
    "Now we create an instance of the `Word2Vec` model and train it on our `sentences` data.\n",
    "* `sentences`: Our corpus, formatted as a list of lists of words.\n",
    "* `size=100`: The dimensionality of the resulting word vectors will be 100.\n",
    "* `window=5`: The model will consider a context window of 5 words to the left and 5 to the right of the target word.\n",
    "* `min_count=2`: Words that appear fewer than 2 times in the corpus will be ignored.\n",
    "* `workers=10`: Use 10 parallel threads to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Word2Vec model with our specified parameters\n",
    "model = Word2Vec(\n",
    "        sentences,      # The corpus to train on\n",
    "        size=100,       # The desired vector dimension\n",
    "        window=5,       # The context window size\n",
    "        min_count=2,    # The minimum word frequency to consider\n",
    "        workers=10)     # The number of threads to use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Accessing and Saving the Trained Vectors**\n",
    "After training, the learned vectors are stored in the `wv` (word vectors) attribute of the model. We can save these vectors to a file for later use without having to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the KeyedVectors instance containing the trained word vectors\n",
    "my_trained_vectors = model.wv\n",
    "# Save the vectors to a text file named 'embeddings.txt'\n",
    "# binary=False saves them in a human-readable text format\n",
    "my_trained_vectors.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing Our Trained Model**\n",
    "Let's test our new model by finding the 10 words that are most semantically similar to the word \"actor\" based on the vector representations we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the most_similar method to find the top 10 words closest to \"actor\" in the vector space\n",
    "my_trained_vectors.most_similar(\"actor\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Using Pre-trained GloVe Embeddings**\n",
    "\n",
    "Training word embeddings from scratch requires a very large dataset to be effective. A common practice is to use vectors that have already been trained on massive text corpora. Here, we'll load pre-trained **GloVe** (Global Vectors for Word Representation) vectors. These vectors were trained on the \"Common Crawl\" dataset, which contains 42 billion tokens.\n",
    "\n",
    "First, download the vectors from [this link](https://drive.google.com/file/d/1n1jt0UIdI3CD26cY1EIeks39XH5S8O8M/view?usp=sharing) and place the file in your `data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting GloVe Vectors to Word2Vec Format**\n",
    "\n",
    "The pre-trained GloVe vectors are in a different file format than the one `gensim`'s `KeyedVectors` class expects. We first need to use the `glove2word2vec` utility to convert the file. This will create a new file in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the original downloaded GloVe file\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "# Define the path for the output file in Word2Vec format\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "# Run the conversion utility. The '_' is used to ignore the function's return value.\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loading the Pre-trained Vectors**\n",
    "Now we can load the newly converted file into a `KeyedVectors` object. This object provides an efficient way to query the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the converted GloVe vectors from the text file\n",
    "# binary=False indicates that the file is in a text format\n",
    "glove = KeyedVectors.load_word2vec_format(glove_in_w2v_format, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the Pre-trained GloVe Model**\n",
    "Let's run the same test as before: find the 10 most similar words to \"actor\". Because these GloVe vectors were trained on a much larger and more diverse dataset, we expect the results to be more robust and relevant than those from our small custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 words most similar to \"actor\" using the pre-trained GloVe vectors\n",
    "glove.most_similar(\"actor\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring Analogies with Vector Arithmetic**\n",
    "\n",
    "A fascinating property of word embeddings is that they capture semantic relationships, which can be explored using vector arithmetic. The classic example is \"king - man + woman â‰ˆ queen\". The `most_similar` function can perform this calculation by specifying which vectors to add (`positive`) and which to subtract (`negative`). Let's test this with a couple of analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Performing Analogy Tasks**\n",
    "\n",
    "Here, we solve the analogy \"Paris is to France as Berlin is to ______?\". This translates to the vector equation: `France - Paris + Berlin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 (commented out): King - Man + Woman\n",
    "# one=\"man\"\n",
    "# two=\"king\"\n",
    "# three=\"woman\"\n",
    "\n",
    "# Example 2: France - Paris + Berlin\n",
    "one=\"paris\"     # The vector to subtract\n",
    "two=\"france\"    # A vector to add\n",
    "three=\"berlin\"  # The other vector to add\n",
    "\n",
    "# Find the top 5 words closest to the result of the vector operation: france + berlin - paris\n",
    "glove.most_similar(positive=[two, three], negative=[one], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3: Evaluating Embedding Quality**\n",
    "\n",
    "We can also perform an *intrinsic evaluation* to quantitatively measure the quality of our word vectors. We do this by comparing the similarity scores our model assigns to word pairs against similarity scores assigned by humans. The **WordSim353** dataset is a standard benchmark for this task. It contains 353 pairs of words, each with a human-assigned similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluating the Pre-trained GloVe Vectors**\n",
    "\n",
    "The `evaluate_word_pairs` function calculates the cosine similarity for each word pair from the WordSim353 dataset that is present in our model's vocabulary. It then computes the Spearman and Pearson correlation coefficients between our model's similarities and the human scores. A higher correlation indicates that our model's understanding of word similarity aligns better with human judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the GloVe vectors against the WordSim353 dataset\n",
    "# datapath('wordsim353.tsv') provides the path to the test dataset\n",
    "glove.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluating Our Custom-Trained Vectors**\n",
    "\n",
    "Finally, let's run the same evaluation on the model we trained ourselves. We expect the correlation score to be significantly lower than the GloVe model's score because our model was trained on a much smaller dataset, limiting its ability to learn nuanced semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our custom-trained vectors against the WordSim353 dataset\n",
    "my_trained_vectors.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
