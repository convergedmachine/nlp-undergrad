{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Explanation and Comments\n",
    "\n",
    "This notebook demonstrates how to build and evaluate a Multilayer Perceptron (MLP) for binary text classification using the Keras library. It covers data preprocessing, model building, training, evaluation, and statistical analysis of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 1: Introduction and Setup**\n",
    "This markdown cell introduces the notebook's purpose and provides the necessary `pip` commands to install specific versions of TensorFlow and Keras that are compatible with the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thie notebook explores the multilayer perceptron for binary text classification for your text classification problem, using the keras library.  Before starting, be sure to install the following versions of tensorflow and keras (for python 3.7):\n",
    "\n",
    "```sh\n",
    "pip install tensorflow==1.13.0-rc2\n",
    "pip install keras==2.2.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 2: Importing Libraries**\n",
    "This cell imports all the required libraries and modules.\n",
    "* `keras`: The main library for building the neural network.\n",
    "* `numpy`: For numerical operations, especially with arrays.\n",
    "* `sklearn.preprocessing`: Used for `LabelEncoder` to convert text labels into numbers.\n",
    "* `keras.layers`: Contains `Dense` (a fully connected layer) and `Dropout` (for regularization).\n",
    "* `keras.models`: `Sequential` is used to build the model layer by layer.\n",
    "* `sklearn.feature_extraction.text`: `CountVectorizer` is used to convert text data into a matrix of token counts.\n",
    "* `keras.callbacks`: Provides tools like `ModelCheckpoint` and `EarlyStopping` to enhance the training process.\n",
    "* `random.choices`: Used in the bootstrap function for random sampling.\n",
    "* `pandas`: Used for data manipulation and for creating the density plot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import the main Keras library\n",
    "import keras\n",
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import preprocessing tools from scikit-learn, specifically for label encoding\n",
    "from sklearn import preprocessing\n",
    "# Import layers for building the MLP: Dense for fully-connected layers and Dropout for regularization\n",
    "from keras.layers import Dense, Dropout\n",
    "# Import the Sequential model type, which allows for building models layer-by-layer\n",
    "from keras.models import Sequential\n",
    "# Import CountVectorizer to convert text into numerical feature vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Import callbacks to monitor and control the training process\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "# Import the 'choices' function for random sampling with replacement (used for bootstrapping)\n",
    "from random import choices\n",
    "# Import pandas for data manipulation and visualization\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 3: Data Reading Function**\n",
    "This cell defines a function `read_data` to load the dataset from a tab-separated values (TSV) file. It iterates through each line, splits it into a label and text, and appends them to separate lists."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to read data from a specified file\n",
    "def read_data(filename):\n",
    "    # Initialize empty lists to store the text (X) and labels (Y)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    # Open the file with UTF-8 encoding to handle various characters\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate over each line in the file, keeping track of the index (though idx is not used)\n",
    "        for idx,line in enumerate(file):\n",
    "            # Split the line by the tab character (\"\\t\") and remove any trailing whitespace\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label\n",
    "            label=cols[0]\n",
    "            # The second column is the text\n",
    "            text=cols[1]\n",
    "            # Append the text to the X list\n",
    "            X.append(text)\n",
    "            # Append the label to the Y list\n",
    "            Y.append(label)\n",
    "\n",
    "    # Return the lists of texts and labels\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 4: Setting the Data Directory**\n",
    "This cell defines a variable `directory` that holds the path to the data files. This path needs to be updated by the user to point to the correct location of `train.tsv`, `dev.tsv`, and `test.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "# Set the path to the folder containing the dataset files\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 5: Loading and Preprocessing Data**\n",
    "This cell performs the main data loading and preprocessing steps.\n",
    "1.  **Read Data**: It calls the `read_data` function to load the training, development (validation), and test sets.\n",
    "2.  **Vectorize Text**: It initializes `CountVectorizer` to convert text into a \"bag-of-words\" numerical format. `binary=True` means it only records the presence (1) or absence (0) of a word, not its frequency. `max_features=10000` limits the vocabulary to the 10,000 most frequent words.\n",
    "3.  **Fit and Transform**: `fit_transform` is used on the training data (`trainX`) to learn the vocabulary and transform the text into vectors. `transform` is used on the dev and test sets to ensure they are vectorized using the *same* vocabulary learned from the training data.\n",
    "4.  **Encode Labels**: `LabelEncoder` is used to convert the string labels (e.g., \"positive\", \"negative\") into integers (e.g., 1, 0)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Read the training, development (dev), and test data using the function defined above\n",
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)\n",
    "testX, testY=read_data(\"%s/test.tsv\" % directory)\n",
    "\n",
    "# Initialize CountVectorizer to convert text into a matrix of word occurrences\n",
    "# max_features=10000: Use the top 10,000 most frequent words as the vocabulary\n",
    "# analyzer=str.split: Split text into words based on whitespace\n",
    "# lowercase=True: Convert all text to lowercase before tokenizing\n",
    "# strip_accents=None: Do not remove accents\n",
    "# binary=True: Use binary values (1 if a word is present, 0 otherwise) instead of word counts\n",
    "vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=True, strip_accents=None, binary=True)\n",
    "\n",
    "# Learn the vocabulary from the training data and transform it into a sparse matrix\n",
    "X_train = vectorizer.fit_transform(trainX)\n",
    "# Transform the dev and test data using the vocabulary learned from the training data\n",
    "X_dev = vectorizer.transform(devX)\n",
    "X_test = vectorizer.transform(testX)\n",
    "\n",
    "# Get the size of the vocabulary from the shape of the training matrix\n",
    "_,vocabSize=X_train.shape\n",
    "\n",
    "# Initialize a LabelEncoder to convert string labels to integers\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Fit the encoder on the training labels to learn the mapping (e.g., 'positive' -> 1, 'negative' -> 0)\n",
    "le.fit(trainY)\n",
    "\n",
    "# Transform the labels for all datasets into their integer representations\n",
    "Y_train=le.transform(trainY)\n",
    "Y_dev=le.transform(devY)\n",
    "Y_test=le.transform(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 6: MLP Model Definition**\n",
    "This cell defines a function `mlp` that creates a simple Multilayer Perceptron model.\n",
    "* **`Sequential()`**: Creates a linear stack of layers.\n",
    "* **`Dense(10, ...)`**: The first (hidden) layer with 10 neurons, a 'relu' activation function, and an input shape matching the vocabulary size.\n",
    "* **`Dropout(0.5)`**: A regularization layer that randomly sets 50% of neuron activations to zero during training to prevent overfitting.\n",
    "* **`Dense(1, ...)`**: The final (output) layer with a single neuron and a 'sigmoid' activation function, which outputs a probability between 0 and 1, suitable for binary classification.\n",
    "* **`model.compile(...)`**: Configures the model for training, specifying the loss function (`binary_crossentropy`), the optimizer (`adam`), and the metric to track (`acc` for accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to create the Multilayer Perceptron (MLP) model\n",
    "def mlp():\n",
    "    # Initialize a Sequential model\n",
    "    model = Sequential()\n",
    "    # Add the first Dense (fully connected) layer. This is the hidden layer.\n",
    "    # 10: Number of neurons in this layer.\n",
    "    # activation='relu': Use the Rectified Linear Unit activation function.\n",
    "    # input_shape=(vocabSize,): Specify the shape of the input data (the size of our vocabulary).\n",
    "    model.add(Dense(10, activation='relu', input_shape=(vocabSize,)))\n",
    "    # Add a Dropout layer for regularization.\n",
    "    # 0.5: The fraction of input units to drop, helps prevent overfitting.\n",
    "    model.add(Dropout(0.5))\n",
    "    # Add the output layer.\n",
    "    # 1: A single neuron for binary classification.\n",
    "    # activation='sigmoid': Sigmoid activation squashes the output to a probability between 0 and 1.\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model to configure the learning process.\n",
    "    # loss='binary_crossentropy': Loss function suitable for binary (0/1) classification.\n",
    "    # optimizer='adam': A popular and effective optimization algorithm.\n",
    "    # metrics=['acc']: The metric to be evaluated by the model during training and testing.\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 7: Training and Evaluation Function**\n",
    "This cell defines `train_and_evaluate`, a helper function to streamline the model training and evaluation process.\n",
    "* **Callbacks**: It sets up `EarlyStopping` to halt training if the validation loss (`val_loss`) doesn't improve for 5 epochs (`patience=5`), and `ModelCheckpoint` to save the best version of the model based on `val_loss`.\n",
    "* **Training**: It trains the model using `model.fit`, passing the training and validation data.\n",
    "* **Evaluation**: After training, it loads the best saved model weights and evaluates its accuracy on both the dev and test sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to handle the training and evaluation of a given model\n",
    "def train_and_evaluate(model, verbose=1, epochs=10):\n",
    "    # If verbose is greater than 0, print the model's architecture summary\n",
    "    if verbose > 0:\n",
    "        print (model.summary())\n",
    "\n",
    "    # Set up EarlyStopping to prevent overfitting.\n",
    "    # monitor='val_loss': Stop training when the loss on the validation set stops improving.\n",
    "    # min_delta=0: Any improvement, no matter how small, is considered.\n",
    "    # patience=5: Number of epochs with no improvement after which training will be stopped.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=5,\n",
    "        verbose=0, \n",
    "        mode='auto')\n",
    "\n",
    "    # Set up ModelCheckpoint to save the best model during training.\n",
    "    modelName=\"mymodel.hdf5\"\n",
    "    # monitor='val_loss': The metric to monitor for saving the best model.\n",
    "    # save_best_only=True: Only save the model if 'val_loss' has improved.\n",
    "    # mode='min': The monitored quantity should be minimized (loss).\n",
    "    checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train, \n",
    "                # Provide the development (validation) data to monitor performance\n",
    "                validation_data=(X_dev, Y_dev),\n",
    "                # Number of times to iterate over the entire training dataset\n",
    "                epochs=epochs,\n",
    "                # Control the verbosity of the training output\n",
    "                verbose=verbose,\n",
    "                # List of callbacks to apply during training\n",
    "                callbacks=[checkpoint, early_stopping])\n",
    "    \n",
    "    # Load the weights of the best model saved by the checkpoint\n",
    "    model.load_weights(modelName)\n",
    "\n",
    "    # Evaluate the best model's performance on the development (validation) set\n",
    "    dev_loss, dev_accuracy = model.evaluate(X_dev, Y_dev, batch_size=128, verbose=verbose)\n",
    "    if verbose > 0:\n",
    "        print(\"Dev Accuracy: %.3f\" % dev_accuracy)\n",
    "\n",
    "    # Evaluate the best model's performance on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, Y_test, batch_size=128, verbose=verbose)\n",
    "    if verbose > 0:\n",
    "        print(\"Test Accuracy: %.3f\" % test_accuracy)\n",
    "\n",
    "    # Return the final accuracy scores for the dev and test sets\n",
    "    return dev_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 8: Question 1**\n",
    "This markdown cell poses the first task: to experiment with five different MLP architectures to find the best-performing one on the development data. One of the models must be logistic regression (an MLP with no hidden layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Experiment with the network structure that works best for your binary classification dataset.  Explore the following choices: a.) number of layers in the MLP;  b.) the size of each layer; c.) the activation functions; d.) the use of dropout.  Which architecture performs best on the development data?  Create 5 different models and execute them below.  One of the models should be logistic regression (i.e., an MLP with *no* hidden layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 9: Model 1 (Example MLP)**\n",
    "This cell defines and trains the first model for Q1. This is the same MLP architecture defined in the `mlp` function earlier: one hidden layer with 10 neurons, 'relu' activation, and 50% dropout."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to create the first model architecture\n",
    "def get_model1():\n",
    "    # Initialize a Sequential model\n",
    "    model = Sequential()\n",
    "    # Add a hidden layer with 10 neurons, 'relu' activation, and the specified input shape\n",
    "    model.add(Dense(10, activation='relu', input_shape=(vocabSize,)))\n",
    "    # Add a Dropout layer with a rate of 0.5 for regularization\n",
    "    model.add(Dropout(0.5))\n",
    "    # Add the output layer with 1 neuron and 'sigmoid' activation for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with binary cross-entropy loss and the adam optimizer\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    # Return the compiled model\n",
    "    return model\n",
    "\n",
    "# Create an instance of the first model\n",
    "model1=get_model1()\n",
    "# Train and evaluate the model using the helper function\n",
    "dev_accuracy, test_accuracy=train_and_evaluate(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cells 10-12: Placeholder for Models 2-4**\n",
    "These cells are placeholders for the user to define and test their own model architectures as required by Q1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- MODEL 2 ---\n",
    "# Define a function for the second model architecture\n",
    "def get_model2():\n",
    "    model = Sequential()\n",
    "    # TODO: your model here. For example, a deeper network:\n",
    "    # model.add(Dense(16, activation='relu', input_shape=(vocabSize,))) \n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(8, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Create, train, and evaluate the second model\n",
    "# model2=get_model2()\n",
    "# dev_accuracy, test_accuracy=train_and_evaluate(model2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- MODEL 3 ---\n",
    "# Define a function for the third model architecture\n",
    "def get_model3():\n",
    "    model = Sequential()\n",
    "    # TODO: your model here. For example, a wider network:\n",
    "    # model.add(Dense(32, activation='relu', input_shape=(vocabSize,))) \n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Create, train, and evaluate the third model\n",
    "# model3=get_model3()\n",
    "# dev_accuracy, test_accuracy=train_and_evaluate(model3)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- MODEL 4 ---\n",
    "# Define a function for the fourth model architecture\n",
    "def get_model4():\n",
    "    model = Sequential()\n",
    "    # TODO: your model here. For example, using a different activation like 'tanh':\n",
    "    # model.add(Dense(10, activation='tanh', input_shape=(vocabSize,))) \n",
    "    # model.add(Dropout(0.25))\n",
    "    # model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Create, train, and evaluate the fourth model\n",
    "# model4=get_model4()\n",
    "# dev_accuracy, test_accuracy=train_and_evaluate(model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 13: Model 5 (Logistic Regression)**\n",
    "This cell defines and trains the logistic regression model. This is achieved by creating a `Sequential` model with only a single `Dense` output layer and no hidden layers. This architecture is mathematically equivalent to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to create the logistic regression model\n",
    "def get_logreg():\n",
    "    # This is an MLP with no hidden layers, which is equivalent to logistic regression.\n",
    "\n",
    "    # Initialize a Sequential model\n",
    "    model = Sequential()\n",
    "    # Add only the output layer directly.\n",
    "    # It takes the input and directly computes the final output via the sigmoid function.\n",
    "    model.add(Dense(1, activation='sigmoid', input_shape=(vocabSize,)))\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    # Return the compiled model\n",
    "    return model\n",
    "    \n",
    "# Create an instance of the logistic regression model\n",
    "logreg=get_logreg()\n",
    "# Train and evaluate the model\n",
    "dev_accuracy, test_accuracy=train_and_evaluate(logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 14: Prediction Explanation**\n",
    "This markdown cell briefly explains how to use the trained model to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate predictions for a given test set with the `predict_classes` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 15: Making Predictions**\n",
    "This cell demonstrates how to generate class predictions (0 or 1) for the test set using the trained logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select the model to use for predictions (here, the logistic regression model)\n",
    "model=logreg\n",
    "# Use the .predict_classes() method to get the predicted class labels for the test set\n",
    "predictions=model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 16: Question 2**\n",
    "This markdown cell asks the user to calculate 95% confidence intervals for the test accuracy of their single best model from Q1, using the bootstrap resampling method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: For the single best model you identified in Q1 above, calculate 95% confidence intervals it makes on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 17: Accuracy Metric Function**\n",
    "This cell defines a simple helper function to calculate accuracy by comparing the ground truth labels with the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to calculate accuracy\n",
    "def accuracy(truth, predictions):\n",
    "    # Initialize a counter for correct predictions\n",
    "    correct=0.\n",
    "    # Loop through each true label and its corresponding prediction\n",
    "    for idx in range(len(truth)):\n",
    "        g=truth[idx]  # Ground truth label\n",
    "        p=predictions[idx] # Predicted label\n",
    "        # If the prediction matches the truth, increment the counter\n",
    "        if g == p:\n",
    "            correct+=1\n",
    "    # Return the ratio of correct predictions to the total number of predictions\n",
    "    return correct/len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 18: Bootstrap Function**\n",
    "This cell defines the `bootstrap` function to calculate confidence intervals.\n",
    "1.  **Resampling**: It repeatedly (B=10,000 times) creates a new dataset by sampling with replacement from the original predictions.\n",
    "2.  **Metric Calculation**: For each resampled dataset, it calculates the accuracy.\n",
    "3.  **Percentiles**: After all iterations, it collects all the calculated accuracies and finds the percentiles that correspond to the 95% confidence interval (the 2.5th and 97.5th percentiles) and the median (50th percentile)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to perform bootstrap resampling to estimate confidence intervals\n",
    "# gold: The ground truth labels\n",
    "# predictions: The model's predictions\n",
    "# metric: The evaluation function to use (e.g., accuracy)\n",
    "# B: The number of bootstrap samples to create\n",
    "# confidence_level: The desired confidence level (e.g., 0.95 for 95%)\n",
    "def bootstrap(gold, predictions, metric, B=10000, confidence_level=0.95):\n",
    "    # Calculate the percentile boundaries for the confidence interval\n",
    "    critical_value=(1-confidence_level)/2\n",
    "    lower_sig=100*critical_value      # e.g., 2.5 for 95% CI\n",
    "    upper_sig=100*(1-critical_value)  # e.g., 97.5 for 95% CI\n",
    "    \n",
    "    # Combine the ground truth and predictions into pairs for easy resampling\n",
    "    data=[]\n",
    "    for g, p in zip(gold, predictions):\n",
    "        data.append([g,p])\n",
    "\n",
    "    # List to store the metric score for each bootstrap sample\n",
    "    accuracies=[]\n",
    "    \n",
    "    # Main bootstrap loop\n",
    "    for b in range(B):\n",
    "        # Create a new sample by choosing with replacement from the original data\n",
    "        choice=choices(data, k=len(data))\n",
    "        # Convert the sample to a NumPy array for easy slicing\n",
    "        choice=np.array(choice)\n",
    "        # Calculate the accuracy on this bootstrap sample\n",
    "        accuracy_score=metric(choice[:,0], choice[:,1]) # choice[:,0] is truth, choice[:,1] is prediction\n",
    "        \n",
    "        # Store the calculated accuracy\n",
    "        accuracies.append(accuracy_score)\n",
    "    \n",
    "    # Calculate the percentiles from the distribution of accuracies\n",
    "    percentiles=np.percentile(accuracies, [lower_sig, 50, upper_sig])\n",
    "    \n",
    "    # Extract the lower bound, median, and upper bound of the confidence interval\n",
    "    lower=percentiles[0]\n",
    "    median=percentiles[1]\n",
    "    upper=percentiles[2]\n",
    "    \n",
    "    # Return the calculated values\n",
    "    return lower, median, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 19: Calculating and Printing Confidence Intervals**\n",
    "This cell calls the `bootstrap` function with the test labels and predictions to compute the confidence interval for accuracy and then prints the result in a formatted string."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate the 95% confidence interval for accuracy using the bootstrap function\n",
    "lower, median, upper=bootstrap(Y_test, predictions, accuracy)\n",
    "# Print the results, showing the median accuracy and the [lower, upper] confidence interval\n",
    "print (\"Accuracy: %.3f [%.3f, %.3f]\" % (median, lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 20: Question 3**\n",
    "This markdown cell introduces the third task: to investigate the effect of random weight initialization on model performance. The user needs to train their best non-logistic-regression model 10 times, record the development accuracy each time, and plot the distribution of these accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Unlike logistic/linear regression, neural networks converge to different solutions as a function of their *initialization* (the random choice of the initial values for parameters).  For the best model that's not logistic regression you identified in Q1 above, train the model 10 times and save the accuracies attained on the development data.  Plot the distribution of dev accuracies using [pandas.DataFrame.plot.density](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.density.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 21: Re-training Loop**\n",
    "This cell implements the experiment for Q3. It loops 10 times, and in each iteration, it re-trains `model1` from scratch and appends the resulting development accuracy to a list. The `verbose=0` argument is used to suppress the detailed training output for each run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize an empty list to store the development accuracies from each run\n",
    "dev_accuracies=[]\n",
    "\n",
    "# Select the model to be re-trained (here, model1 is chosen)\n",
    "# Note: The model weights are re-initialized each time `train_and_evaluate` is called,\n",
    "# as the `fit` method starts from the current state of the weights. For a truly fresh start,\n",
    "# you would re-create the model inside the loop: `model = get_model1()`. However, Keras's\n",
    "# training process itself introduces sufficient randomness for this experiment.\n",
    "model=model1\n",
    "\n",
    "# Loop 10 times to train and evaluate the model repeatedly\n",
    "for i in range(10):\n",
    "    # Train and evaluate the model, with verbose=0 to keep the output clean\n",
    "    dev_accuracy, test_accuracy=train_and_evaluate(model, verbose=0)\n",
    "    # Append the resulting development accuracy to our list\n",
    "    dev_accuracies.append(dev_accuracy)\n",
    "    # Print the results of the current iteration\n",
    "    print(\"iteration: %s\\t%.3f\\t%.3f\" % (i, dev_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 22: Plotting the Accuracy Distribution**\n",
    "This cell uses pandas to create a Kernel Density Estimate (KDE) plot. A KDE plot is a way to visualize the distribution of a continuous variable. Here, it shows the distribution of the 10 development accuracy scores obtained in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert the list of accuracies into a pandas DataFrame\n",
    "df=pd.DataFrame(dev_accuracies)\n",
    "# Use the DataFrame's built-in plotting function to create a Kernel Density Estimate (KDE) plot.\n",
    "# This visualizes the distribution of the development accuracies.\n",
    "ax = df.plot.kde()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}