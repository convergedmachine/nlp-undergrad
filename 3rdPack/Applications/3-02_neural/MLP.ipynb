{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook demonstrates how to build and train a Multilayer Perceptron (MLP) for binary text classification. It uses the Keras library, a high-level neural networks API, running on top of TensorFlow. The process involves loading text data, converting it into a numerical format using a bag-of-words model, defining the neural network architecture, and then training and evaluating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Environment Setup\n",
    "\n",
    "The first markdown cell provides instructions for installing the specific versions of TensorFlow and Keras required to run the notebook. This ensures that the code runs without compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thie notebook explores the multilayer perceptron for binary text classification, using the keras library.  Before starting, be sure to install the following versions of tensorflow and keras (for python 3.7):\n",
    "\n",
    "```sh\n",
    "pip install tensorflow==1.13.0-rc2\n",
    "pip install keras==2.2.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Importing Necessary Libraries\n",
    "\n",
    "This cell imports all the required modules and functions. We import Keras for building the neural network, NumPy for numerical operations, and scikit-learn for data preprocessing and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Keras library, the main tool for building our neural network.\n",
    "import keras\n",
    "# Import NumPy for efficient numerical operations, especially with arrays.\n",
    "import numpy as np\n",
    "# Import the preprocessing module from scikit-learn to encode our labels.\n",
    "from sklearn import preprocessing\n",
    "# Import Dense for fully-connected layers and Dropout for regularization.\n",
    "from keras.layers import Dense, Dropout\n",
    "# Import Sequential to build our model layer-by-layer.\n",
    "from keras.models import Sequential\n",
    "# Import CountVectorizer to convert text data into a matrix of token counts.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Import callbacks to save the best model and stop training early if performance stagnates.\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Data Reading Function\n",
    "\n",
    "This cell defines a helper function `read_data` to load the dataset from a tab-separated values (TSV) file. The function reads each line, separates the label from the text, and stores them in two separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read data from a specified file.\n",
    "def read_data(filename):\n",
    "    # Initialize an empty list to store the text data (features).\n",
    "    X=[]\n",
    "    # Initialize an empty list to store the labels.\n",
    "    Y=[]\n",
    "    # Open the file with UTF-8 encoding to handle a wide range of characters.\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Loop through each line in the file, keeping track of the index (though idx is not used).\n",
    "        for idx,line in enumerate(file):\n",
    "            # Remove trailing whitespace and split the line by the tab character.\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label.\n",
    "            label=cols[0]\n",
    "            # The second column is the pre-tokenized text.\n",
    "            # It's assumed the text is already split into words.\n",
    "            text=cols[1]\n",
    "            # Add the text to the features list.\n",
    "            X.append(text)\n",
    "            # Add the label to the labels list.\n",
    "            Y.append(label)\n",
    "\n",
    "    # Return the lists of features and labels.\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Setting the Data Directory\n",
    "\n",
    "This cell defines a variable `directory` that holds the path to the data folder. This makes it easy to change the data source location without modifying the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the data files.\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Loading and Preprocessing Data\n",
    "\n",
    "Here, the data is loaded using the `read_data` function. The text data is then converted into a numerical format using `CountVectorizer`, which creates a binary bag-of-words representation. Finally, the string labels (e.g., \"positive\", \"negative\") are converted into integers (e.g., 1, 0) using `LabelEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training, development (validation), and test datasets using the function defined above.\n",
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)\n",
    "testX, testY=read_data(\"%s/test.tsv\" % directory)\n",
    "\n",
    "# Initialize the CountVectorizer.\n",
    "# max_features=10000: Use only the 10,000 most frequent words.\n",
    "# analyzer=str.split: Split text into words using whitespace.\n",
    "# lowercase=True: Convert all text to lowercase.\n",
    "# strip_accents=None: Do not remove accents.\n",
    "# binary=True: Use 1 if a word is present and 0 otherwise, instead of word counts.\n",
    "vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=True, strip_accents=None, binary=True)\n",
    "\n",
    "# Learn the vocabulary from the training data and transform it into a feature matrix.\n",
    "X_train = vectorizer.fit_transform(trainX)\n",
    "# Transform the development data using the vocabulary learned from the training data.\n",
    "X_dev = vectorizer.transform(devX)\n",
    "# Transform the test data using the same vocabulary.\n",
    "X_test = vectorizer.transform(testX)\n",
    "\n",
    "# Get the size of the vocabulary (number of features) from the shape of the training matrix.\n",
    "_,vocabSize=X_train.shape\n",
    "\n",
    "# Initialize the LabelEncoder to convert string labels to integers.\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Learn the label mapping from the training labels (e.g., 'pos' -> 1, 'neg' -> 0).\n",
    "le.fit(trainY)\n",
    "\n",
    "# Transform the labels for all datasets into their integer representations.\n",
    "Y_train=le.transform(trainY)\n",
    "Y_dev=le.transform(devY)\n",
    "Y_test=le.transform(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Defining the MLP Model Architecture\n",
    "\n",
    "This function `mlp` defines the structure of our neural network. It's a simple sequential model with one hidden layer.\n",
    "\n",
    "1.  **Input Layer**: Implicitly defined by `input_shape` in the first `Dense` layer. It has a size equal to our vocabulary size (10,000).\n",
    "2.  **Hidden Layer**: A `Dense` layer with 10 neurons and a 'relu' activation function.\n",
    "3.  **Dropout Layer**: Randomly sets 20% of the input units to 0 during training to prevent overfitting.\n",
    "4.  **Output Layer**: A `Dense` layer with a single neuron and a 'sigmoid' activation function, which outputs a probability between 0 and 1, perfect for binary classification.\n",
    "\n",
    "The model is then compiled with a loss function, an optimizer, and a metric to monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that creates and returns the MLP model.\n",
    "def mlp():\n",
    "    # Initialize a Sequential model, which allows building a model layer by layer.\n",
    "    model = Sequential()\n",
    "    # Add the first (hidden) layer: a fully-connected (Dense) layer with 10 neurons.\n",
    "    # 'relu' (Rectified Linear Unit) is the activation function.\n",
    "    # 'input_shape' specifies the number of features for the first layer (our vocabulary size).\n",
    "    model.add(Dense(10, activation='relu', input_shape=(vocabSize,)))\n",
    "    # Add a Dropout layer to prevent overfitting. It will randomly drop 20% of neuron connections during training.\n",
    "    model.add(Dropout(0.2))\n",
    "    # Add the output layer: a Dense layer with 1 neuron.\n",
    "    # 'sigmoid' activation squashes the output to a value between 0 and 1, representing a probability.\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Configure the model for training.\n",
    "    # 'loss='binary_crossentropy' is used for binary (two-class) classification problems.\n",
    "    # 'optimizer='adam'' is an efficient optimization algorithm.\n",
    "    # 'metrics=['acc']' specifies that we want to monitor accuracy during training.\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "    # Return the compiled model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Training and Evaluation Function\n",
    "\n",
    "The `train_and_evaluate` function handles the entire workflow of training the model, monitoring its performance, and evaluating it on the dev and test sets. It uses two important callbacks:\n",
    "* **`EarlyStopping`**: Halts the training process if the validation loss (`val_loss`) doesn't improve for a set number of epochs (`patience=10`), preventing wasted computation and overfitting.\n",
    "* **`ModelCheckpoint`**: Saves the model's weights to a file (`mymodel.hdf5`) only when the validation loss improves. This ensures we can later use the best version of the model, not necessarily the one from the final epoch.\n",
    "\n",
    "After training, it loads the best saved weights and evaluates the final model's accuracy on both the development and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a model, trains it, and evaluates its performance.\n",
    "def train_and_evaluate(model):\n",
    "    # Print a summary of the model's architecture (layers, parameters, etc.).\n",
    "    print (model.summary())\n",
    "\n",
    "    # Configure EarlyStopping to monitor the validation loss.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "        min_delta=0,       # Minimum change to qualify as an improvement.\n",
    "        patience=10,       # Number of epochs with no improvement after which training will be stopped.\n",
    "        verbose=0,         # Suppress verbose output.\n",
    "        mode='auto')       # Automatically infer the direction of improvement (min for loss).\n",
    "\n",
    "    # Define the filename for saving the best model.\n",
    "    modelName=\"mymodel.hdf5\"\n",
    "    # Configure ModelCheckpoint to save the model with the best validation loss.\n",
    "    checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "    # Start the training process.\n",
    "    model.fit(X_train, Y_train, \n",
    "                validation_data=(X_dev, Y_dev), # Data to use for validation after each epoch.\n",
    "                epochs=5,                       # The maximum number of times to iterate over the entire dataset.\n",
    "                callbacks=[checkpoint, early_stopping]) # List of callbacks to use during training.\n",
    "    \n",
    "    # Load the weights of the best model saved by the ModelCheckpoint callback.\n",
    "    model.load_weights(modelName)\n",
    "\n",
    "    # Evaluate the performance of the best model on the development set.\n",
    "    dev_loss, dev_accuracy = model.evaluate(X_dev, Y_dev, batch_size=128)\n",
    "    # Print the development accuracy, formatted to three decimal places.\n",
    "    print(\"Dev Accuracy: %.3f\" % dev_accuracy)\n",
    "\n",
    "    # Evaluate the performance of the best model on the test set.\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "    # Print the final test accuracy.\n",
    "    print(\"Test Accuracy: %.3f\" % test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Running the Experiment\n",
    "\n",
    "This final cell brings everything together. It first calls `mlp()` to create a new instance of the compiled model and then passes this model to the `train_and_evaluate()` function to start the training and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP model by calling the mlp() function and then pass it to the\n",
    "# train_and_evaluate function to run the complete training and evaluation pipeline.\n",
    "train_and_evaluate(mlp())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}