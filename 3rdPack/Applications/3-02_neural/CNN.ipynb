{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 1: Introduction**\n",
    "This is the initial markdown cell that introduces the notebook's purpose and gives a necessary setup instruction.\n",
    "\n",
    "---\n",
    "This notebook explores convolutional neural networks for text, using the keras `Sequential` and `Functional` interfaces.\n",
    "<br>\n",
    "\n",
    "Before getting started, install the pydot library\n",
    "\n",
    "```sh\n",
    "conda install pydot=1.3.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 2: Importing Libraries**\n",
    "This cell imports all the necessary libraries and modules. These include Keras for building the neural network, NumPy for numerical operations, and scikit-learn for data preprocessing. Utilities for visualizing the model are also imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main Keras library for building neural networks\n",
    "import keras\n",
    "# Import NumPy for efficient numerical operations, especially with arrays\n",
    "import numpy as np\n",
    "# Import the preprocessing module from scikit-learn for tasks like label encoding\n",
    "from sklearn import preprocessing\n",
    "# Import specific layers needed for the CNN model from Keras\n",
    "from keras.layers import Dense, Input, Embedding, GlobalMaxPooling1D, Conv1D, Concatenate, Dropout\n",
    "# Import the two main Keras model-building APIs: Sequential and Model (Functional)\n",
    "from keras.models import Model, Sequential\n",
    "# Import CountVectorizer for text feature extraction (though not used in the final model)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Import SVG to display model visualizations directly in the notebook\n",
    "from IPython.display import SVG\n",
    "# Import a utility to convert a Keras model to a dot format for visualization\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 3: Function to Load Word Embeddings**\n",
    "This cell defines the `load_embeddings` function. Its purpose is to read a file containing pre-trained word embeddings (like GloVe or Word2Vec) and load them into memory. It creates a vocabulary dictionary mapping words to integer IDs and an embedding matrix where the row index corresponds to a word's ID. Special tokens for padding (`_0_`) and unknown words (`_UNK_`) are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load pre-trained word embeddings from a file\n",
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    # Initialize an empty dictionary to map words to their integer IDs\n",
    "    vocab={}\n",
    "    # Initialize an empty list to store the embedding vectors\n",
    "    embeddings=[]\n",
    "    # Open and read the specified embeddings file\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        # Read the first line, which contains the vocabulary size and embedding dimension\n",
    "        cols=file.readline().split(\" \")\n",
    "        # Extract the total number of words in the file\n",
    "        num_words=int(cols[0])\n",
    "        # Extract the size (dimension) of each embedding vector\n",
    "        size=int(cols[1])\n",
    "        # Append a zero vector for the padding token (ID 0)\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Append another zero vector for the \"Unknown\" (UNK) token (ID 1)\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add the padding token to our vocabulary with ID 0\n",
    "        vocab[\"_0_\"]=0\n",
    "        # Add the UNK token to our vocabulary with ID 1\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        # Iterate through each line of the embeddings file with an index\n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            # Stop reading if we have reached the desired maximum vocabulary size\n",
    "            # We use idx+2 to account for the padding and UNK tokens\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            # Strip whitespace and split the line into the word and its vector parts\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            # Convert the vector parts (from the second element onwards) into a NumPy array\n",
    "            val=np.array(cols[1:])\n",
    "            # The first element is the word itself\n",
    "            word=cols[0]\n",
    "            \n",
    "            # Add the word's vector to our list of embeddings\n",
    "            embeddings.append(val)\n",
    "            # Add the word to our vocabulary, mapping it to its new ID (index + 2)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    # Convert the list of embeddings to a NumPy array and return it along with the vocabulary and embedding size\n",
    "    return np.array(embeddings), vocab, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 4: Function to Read Data**\n",
    "This cell defines the `read_data` function, which reads a tab-separated values (TSV) file containing text data and corresponding labels. It parses each line, separating the label from the pre-tokenized text, and returns them as two separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read text data from a labeled file\n",
    "def read_data(filename, vocab):\n",
    "    # Initialize an empty list to store the text sequences (features)\n",
    "    X=[]\n",
    "    # Initialize an empty list to store the labels\n",
    "    Y=[]\n",
    "    # Open the file, specifying UTF-8 encoding to handle a wide range of characters\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate through each line in the file\n",
    "        for line in file:\n",
    "            # Strip trailing whitespace and split the line by the tab character\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label\n",
    "            label=cols[0]\n",
    "            # The second column is the text, which is assumed to be already tokenized (words separated by spaces)\n",
    "            text=cols[1].split(\" \")\n",
    "            # Add the list of tokens to the features list\n",
    "            X.append(text)\n",
    "            # Add the label to the labels list\n",
    "            Y.append(label)\n",
    "    # Return the lists of text sequences and labels\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 5: Function to Convert Text to Word IDs**\n",
    "The `get_word_ids` function takes documents (as lists of tokens) and converts them into sequences of numerical IDs based on the provided vocabulary. It also ensures that all sequences have the same length by truncating longer ones and padding shorter ones with a '0' ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert documents (lists of tokens) into padded sequences of integer IDs\n",
    "def get_word_ids(docs, vocab, max_length=1000):\n",
    "    \n",
    "    # Initialize a list to hold all the processed documents (as ID sequences)\n",
    "    doc_ids=[]\n",
    "    \n",
    "    # Iterate through each document in the input list\n",
    "    for doc in docs:\n",
    "        # Initialize a list to store word IDs for the current document\n",
    "        wids=[]\n",
    "\n",
    "        # Iterate through each token in the document, but only up to the specified max_length\n",
    "        for token in doc[:max_length]:\n",
    "            # Look up the lowercase token in the vocabulary; if not found, use the UNK ID (1)\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            # Append the corresponding ID to the list for the current document\n",
    "            wids.append(val)\n",
    "        \n",
    "        # After processing tokens, pad the sequence to ensure it reaches max_length\n",
    "        # This loop runs from the current length of wids up to max_length\n",
    "        for i in range(len(wids),max_length):\n",
    "            # Append the padding ID (0) to the end of the list\n",
    "            wids.append(0)\n",
    "\n",
    "        # Add the final padded sequence of word IDs to the main list\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    # Convert the list of lists into a 2D NumPy array and return it\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 6: Loading the Pre-trained Embeddings**\n",
    "This cell executes the `load_embeddings` function. It specifies the path to the GloVe word embeddings file and sets a vocabulary limit of 100,000 words. The returned embedding matrix, vocabulary dictionary, and embedding dimension are stored in their respective variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the load_embeddings function to load GloVe embeddings\n",
    "# Use a pre-trained file containing 300-dimensional vectors for 50K words\n",
    "# Limit the vocabulary size to a maximum of 100,000 words\n",
    "embeddings, vocab, embedding_size=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 7: Setting the Data Directory**\n",
    "A simple variable assignment to hold the path to the dataset directory. This makes it easy to change the data source location in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "# Set a variable to store the path to the data directory for easy access\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 8: Reading the Training and Development Data**\n",
    "Here, the `read_data` function is called to load the training and development (validation) sets from their respective TSV files. The text and labels for each set are stored in separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data from the 'train.tsv' file within the specified directory\n",
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "# Read the development (validation) data from the 'dev.tsv' file\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 9: Preparing the Datasets for the Model**\n",
    "This cell uses the `get_word_ids` function to convert the raw text of the training and development sets into padded numerical sequences. The `max_length` is set to 200, meaning each review will be represented by a vector of 200 integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training text documents into padded sequences of word IDs, with a max length of 200\n",
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "# Convert the development text documents into padded sequences of word IDs, also with a max length of 200\n",
    "devX = get_word_ids(devText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 10: Encoding the Labels**\n",
    "The text labels (e.g., 'positive', 'negative') need to be converted into numbers (e.g., 1, 0) for the model to process. This cell uses scikit-learn's `LabelEncoder` to perform this transformation for both the training and development labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a LabelEncoder object from scikit-learn to convert string labels to integers\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Fit the encoder on the training labels to learn the mapping (e.g., 'pos' -> 1, 'neg' -> 0)\n",
    "le.fit(trainY)\n",
    "# Transform the training labels into their integer representations and convert to a NumPy array\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "# Transform the development labels using the same learned mapping and convert to a NumPy array\n",
    "Y_dev=np.array(le.transform(devY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 11: Defining a CNN Model with the Sequential API**\n",
    "This cell defines a function `cnn_sequential` that builds a simple CNN model using Keras's `Sequential` API. The model consists of an embedding layer (initialized with pre-trained GloVe vectors), a 1D convolutional layer to detect features (bigrams), a pooling layer to summarize the features, a dropout layer for regularization, and a final dense layer for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a CNN model using the Keras Sequential API\n",
    "def cnn_sequential(embeddings, vocab_size, word_embedding_dim):\n",
    "    # Initialize a Sequential model, which is a linear stack of layers\n",
    "    model = Sequential()\n",
    "    # Add the Embedding layer. It maps word IDs to dense vectors.\n",
    "    # 'weights' initializes it with our pre-trained GloVe embeddings.\n",
    "    # 'trainable=False' freezes the embeddings so they are not updated during training.\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=word_embedding_dim, weights=[embeddings], trainable=False))\n",
    "    # Add a 1D convolutional layer. It acts as a feature detector sliding over the sequence.\n",
    "    # 'filters=50': learns 50 different features.\n",
    "    # 'kernel_size=2': looks at 2 words (bigrams) at a time.\n",
    "    # 'activation=\"tanh\"': applies the tanh activation function.\n",
    "    model.add(Conv1D(filters=50, kernel_size=2, strides=1, padding=\"same\", activation=\"tanh\", name=\"CNN_bigram\"))\n",
    "    # Add a global max pooling layer. It takes the maximum value from each of the 50 feature maps.\n",
    "    # This distills the most important feature detected in the entire sequence.\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    # Add a Dropout layer. It randomly sets 20% of its input units to 0 during training to prevent overfitting.\n",
    "    model.add(Dropout(0.2))\n",
    "    # Add the final output layer. A single neuron with a sigmoid activation is used for binary classification.\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile the model, configuring it for training.\n",
    "    # 'loss='binary_crossentropy'': The loss function for a two-class problem.\n",
    "    # 'optimizer='adam'': An efficient optimization algorithm.\n",
    "    # 'metrics=['acc']': The metric to monitor during training is accuracy.\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 12: Building and Visualizing the Sequential Model**\n",
    "Here, the `cnn_sequential` function is called to create an instance of the model. The `.summary()` method is then used to print a textual representation of the model's architecture, and `model_to_dot` is used to create a graphical visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the sequential CNN model by calling the function defined above\n",
    "cnn_sequential_model=cnn_sequential(embeddings, len(vocab), embedding_size)\n",
    "# Print a summary of the model's architecture, including layers, output shapes, and number of parameters\n",
    "print (cnn_sequential_model.summary())\n",
    "# Generate a visual graph of the model architecture and display it as an SVG image in the notebook\n",
    "SVG(model_to_dot(cnn_sequential_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 13: Defining a CNN Model with the Functional API**\n",
    "This cell defines a function `cnn` that builds a more complex CNN using Keras's `Functional` API. This model has multiple parallel convolutional layers with different kernel sizes (2, 3, and 4) to capture features of different n-gram sizes (bigrams, trigrams, etc.). The outputs of these parallel branches are then concatenated and passed to the final classification layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a more complex multi-filter CNN using the Keras Functional API\n",
    "def cnn(embeddings, vocab_size, word_embedding_dim):\n",
    "\n",
    "    # Define the model's input layer. It expects sequences of integers of any length.\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Define the Embedding layer, initializing it with pre-trained weights and making it non-trainable.\n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings],\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    # Connect the embedding layer to the input. This creates the embedded sequences.\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    # --- Parallel Convolutional Layers ---\n",
    "    # Create a Conv1D layer to detect bigram features (kernel_size=2).\n",
    "    cnn2=Conv1D(filters=50, kernel_size=2, strides=1, padding=\"same\", activation=\"tanh\", name=\"CNN_bigram\")(embedded_sequences)\n",
    "    # Create a Conv1D layer to detect trigram features (kernel_size=3).\n",
    "    cnn3=Conv1D(filters=50, kernel_size=3, strides=1, padding=\"same\", activation=\"tanh\", name=\"CNN_trigram\")(embedded_sequences)\n",
    "    # Create a Conv1D layer to detect 4-gram features (kernel_size=4).\n",
    "    cnn4=Conv1D(filters=50, kernel_size=4, strides=1, padding=\"same\", activation=\"tanh\", name=\"CNN_4gram\")(embedded_sequences)\n",
    "\n",
    "    # --- Max Pooling for each convolutional path ---\n",
    "    # Apply global max pooling to the output of the bigram CNN.\n",
    "    maxpool2=GlobalMaxPooling1D()(cnn2)\n",
    "    # Apply global max pooling to the output of the trigram CNN.\n",
    "    maxpool3=GlobalMaxPooling1D()(cnn3)\n",
    "    # Apply global max pooling to the output of the 4-gram CNN.\n",
    "    maxpool4=GlobalMaxPooling1D()(cnn4)\n",
    "\n",
    "    # Concatenate the results from all three max-pooling layers into a single flat vector.\n",
    "    x=Concatenate()([maxpool2, maxpool3, maxpool4])\n",
    "\n",
    "    # Apply a Dropout layer for regularization to the concatenated vector.\n",
    "    x=Dropout(0.2)(x)\n",
    "    # Add a fully connected (Dense) layer with 50 neurons.\n",
    "    x=Dense(50)(x)\n",
    "    # Add the final output Dense layer with a sigmoid activation for binary classification.\n",
    "    x=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Create the final Model by specifying its input layer and output layer.\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    # Compile the model with the appropriate loss function, optimizer, and metrics.\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    # Return the compiled functional model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 14: Building and Visualizing the Functional Model**\n",
    "Similar to cell 12, this cell instantiates the functional CNN model, prints its summary, and generates a visual diagram of its more complex, parallel architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the functional CNN model by calling the 'cnn' function\n",
    "cnn_functional_model=cnn(embeddings, len(vocab), embedding_size)\n",
    "# Print a summary of the functional model's architecture\n",
    "print (cnn_functional_model.summary())\n",
    "# Generate and display a visual graph of the functional model\n",
    "SVG(model_to_dot(cnn_functional_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 15: Training the Model**\n",
    "This cell kicks off the training process. The `model.fit()` method is called on the functional model, providing it with the training data (`trainX`, `Y_train`) and the validation data (`devX`, `Y_dev`). The model will train for 10 epochs, updating its weights in batches of 128 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the more complex functional model to the 'model' variable for training\n",
    "model=cnn_functional_model\n",
    "# Call the fit method to train the model\n",
    "model.fit(trainX, Y_train, \n",
    "            # Provide the development set for validation after each epoch\n",
    "            validation_data=(devX, Y_dev),\n",
    "            # Train for 10 complete passes over the entire training dataset\n",
    "            epochs=10, \n",
    "            # Process the data in batches of 128 samples at a time\n",
    "            batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 16: Empty Cell**\n",
    "This is an empty code cell, often left for future experiments or code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
