{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of attention for text classification, comparing a model that represents a document by averaging its word embeddings to one that uses an attention mechanism to compute a weighted average over those embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Imports\n",
    "This cell imports all the necessary libraries. We'll use Keras for building the neural network, NumPy for numerical operations, and Scikit-learn for label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main Keras library for building neural networks\n",
    "import keras\n",
    "# Import NumPy for numerical operations, especially with arrays\n",
    "import numpy as np\n",
    "# Import preprocessing tools from scikit-learn, specifically for encoding labels\n",
    "from sklearn import preprocessing\n",
    "# Import specific layers and components from Keras to build the model\n",
    "from keras.layers import Dense, Input, Embedding, Lambda, Layer, Multiply, Dropout, Dot\n",
    "# Import the Model class to create a trainable model object\n",
    "from keras.models import Model\n",
    "# Import the Keras backend (we use it for custom layer operations)\n",
    "from keras import backend as K\n",
    "# Import TensorFlow, which Keras uses as its backend engine\n",
    "import tensorflow as tf\n",
    "# Import callbacks for saving the best model and stopping training early\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "# Import pandas for data manipulation, used here for visualizing attention\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Load Embeddings Function\n",
    "This function, `load_embeddings`, reads a file containing pre-trained word embeddings (like GloVe or Word2Vec). It builds a vocabulary mapping words to integer IDs and an embedding matrix where the row index corresponds to a word's ID. It also adds special tokens for padding (`_0_`) and unknown words (`_UNK_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load pre-trained word embeddings from a file\n",
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    # Create an empty dictionary to store our vocabulary (word -> integer ID)\n",
    "    vocab={}\n",
    "    # Create an empty list to store the embedding vectors\n",
    "    embeddings=[]\n",
    "    # Open the specified file to read the embeddings\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        # Read the first line, which often contains the number of words and the embedding dimension\n",
    "        cols=file.readline().split(\" \")\n",
    "        # Extract the total number of words in the file\n",
    "        num_words=int(cols[0])\n",
    "        # Extract the size (dimension) of each embedding vector\n",
    "        size=int(cols[1])\n",
    "        \n",
    "        # Add a zero vector for the padding token (ID 0)\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add a zero vector for the \"Unknown\" (UNK) token (ID 1)\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add the padding token to our vocabulary\n",
    "        vocab[\"_0_\"]=0\n",
    "        # Add the UNK token to our vocabulary\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        # Loop through each line in the embedding file\n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            # Stop if we have reached the desired maximum vocabulary size\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            # Split the line into the word and its vector components\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            # Convert the vector components to a NumPy array of floats\n",
    "            val=np.array(cols[1:])\n",
    "            # The first column is the word itself\n",
    "            word=cols[0]\n",
    "            \n",
    "            # Add the word's vector to our embeddings list\n",
    "            embeddings.append(val)\n",
    "            # Add the word to our vocabulary, mapping it to its new ID (index + 2)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    # Convert the list of embeddings to a NumPy array and return it along with the vocabulary and embedding size\n",
    "    return np.array(embeddings), vocab, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Read Data Function\n",
    "The `read_data` function reads a tab-separated value (TSV) file where each line contains a label and a text document. It separates them and returns two lists: one for the texts (`X`) and one for the labels (`Y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read text data from a file\n",
    "def read_data(filename, vocab):\n",
    "    # Initialize an empty list to store the text documents (features)\n",
    "    X=[]\n",
    "    # Initialize an empty list to store the labels\n",
    "    Y=[]\n",
    "    # Open the file, specifying utf-8 encoding for broad character support\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Remove trailing whitespace and split the line by the tab character\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label\n",
    "            label=cols[0]\n",
    "            # The second column is the text, which is assumed to be already tokenized (words separated by spaces)\n",
    "            text=cols[1].split(\" \")\n",
    "            # Append the list of tokens to the features list X\n",
    "            X.append(text)\n",
    "            # Append the label to the labels list Y\n",
    "            Y.append(label)\n",
    "    # Return the lists of documents and labels\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Convert Words to IDs Function\n",
    "This function, `get_word_ids`, takes a list of documents (each a list of tokens) and converts every token into its corresponding integer ID from the vocabulary. It also ensures all documents have the same length by truncating long ones and padding shorter ones with zeros. This fixed length is required for creating batches for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert documents (lists of tokens) into sequences of integer IDs\n",
    "def get_word_ids(docs, vocab, max_length=200):\n",
    "    \n",
    "    # Initialize an empty list to hold the ID sequences for all documents\n",
    "    doc_ids=[]\n",
    "    \n",
    "    # Iterate through each document in the input list\n",
    "    for doc in docs:\n",
    "        # Initialize an empty list for the current document's word IDs\n",
    "        wids=[]\n",
    "        # Iterate through the first `max_length` tokens of the document\n",
    "        for token in doc[:max_length]:\n",
    "            # Look up the token in the vocabulary (converted to lowercase). If not found, use the ID for UNK (1).\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            # Append the integer ID to the current document's list\n",
    "            wids.append(val)\n",
    "        \n",
    "        # Pad the sequence with zeros until it reaches `max_length`\n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "\n",
    "        # Add the final padded sequence of IDs to our list of all documents\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    # Convert the list of lists into a NumPy array and return it\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't downloaded the glove vectors, do so first -- the top 50K words in the \"Common Crawl (42B)\"  vectors (300-dimensional) can be found here: [glove.42B.300d.50K.txt](https://drive.google.com/file/d/1n1jt0UIdI3CD26cY1EIeks39XH5S8O8M/view?usp=sharing); download it and place  in your `data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Data Preparation\n",
    "This section prepares the data for the model. First, it converts the pre-trained GloVe embeddings from their original text format to the word2vec format, which the `load_embeddings` function is designed to read. Then, it loads these embeddings into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the glove2word2vec conversion script from the gensim library\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Specify the path to the original GloVe embeddings file\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "# Specify the path for the output file in word2vec format\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "# Run the conversion utility. The underscore `_` is used to discard the return value.\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Loading Embeddings\n",
    "Here, we call the `load_embeddings` function to get our embedding matrix, vocabulary dictionary, and the dimension of the embeddings. We limit the vocabulary to the top 50,000 words from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to load embeddings, limiting the vocabulary to 50,000 words\n",
    "embeddings, vocab, embedding_size=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Setting Data Directory\n",
    "This cell defines the directory where the training, development (validation), and test datasets are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to the directory containing your train.tsv, dev.tsv, and test.tsv files\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Reading Datasets\n",
    "We use the `read_data` function to load the raw text and labels for the training and development sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data from the specified directory\n",
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "# Read the development (validation) data\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Numerical Conversion\n",
    "The text documents are converted into padded sequences of integer IDs using the `get_word_ids` function. This prepares the data to be fed into the model's embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training text documents into padded sequences of word IDs\n",
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "# Convert the development text documents into padded sequences of word IDs\n",
    "devX = get_word_ids(devText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Label Encoding\n",
    "The string labels (e.g., \"positive\", \"negative\") are converted into integers (0 and 1). A neural network requires numerical inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Fit the encoder on the training labels to learn the mapping (e.g., \"pos\" -> 1, \"neg\" -> 0)\n",
    "le.fit(trainY)\n",
    "# Transform the training labels into integers and convert to a NumPy array\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "# Transform the development labels into integers and convert to a NumPy array\n",
    "Y_dev=np.array(le.transform(devY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try a simple model that represents a document by averaging the embeddings for the words it contains.  We'll again use appropriate masking to accommodate zero-padded sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: Custom Pooling Layer\n",
    "This custom layer, `MaskedAveragePooling1D`, performs average pooling but is aware of the mask generated by the `Embedding` layer (when `mask_zero=True`). It ensures that the padded time steps (words) are not included in the average calculation, preventing the padding from skewing the document representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Keras Layer for average pooling that respects masking\n",
    "class MaskedAveragePooling1D(Layer):\n",
    "    # The __init__ method is the constructor for the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        # Set a flag to indicate that this layer supports masking\n",
    "        self.supports_masking = True\n",
    "        # Call the parent class's constructor\n",
    "        super(MaskedAveragePooling1D, self).__init__(**kwargs)\n",
    "\n",
    "    # This method computes the mask for the output of this layer. We return None as this layer condenses the sequence, so the mask is no longer needed.\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    # The `call` method contains the layer's logic. It takes the input tensor `x` and its mask.\n",
    "    def call(self, x, mask=None):\n",
    "        # Check if a mask was provided by the previous layer\n",
    "        if mask is not None:\n",
    "            # Cast the boolean mask to a float tensor (e.g., True -> 1.0, False -> 0.0)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            # The mask has shape (batch_size, timesteps). We need to expand it to match the input tensor's shape (batch_size, timesteps, embedding_dim).\n",
    "            # `K.repeat` adds a new dimension and copies the mask along it.\n",
    "            mask = K.repeat(mask, x.shape[-1])\n",
    "            # Transpose the mask to align its dimensions with the input tensor `x` for element-wise multiplication\n",
    "            mask = tf.transpose(mask, [0,2,1])\n",
    "            # Multiply the input `x` by the mask. This sets the embedding vectors of padded words to zero.\n",
    "            x = x * mask\n",
    "            \n",
    "        # Sum the embeddings along the time steps axis (axis=1) and divide by the number of non-masked time steps to get the true average.\n",
    "        # `K.sum(mask, axis=1)` correctly counts the number of actual words in each sequence.\n",
    "        return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "\n",
    "    # This method defines the shape of the layer's output\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The output is a single vector per document, so the shape is (batch_size, embedding_dim)\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 13: Building the Averaging Model\n",
    "This function constructs the Keras model. It consists of:\n",
    "1.  An **Input** layer for the integer sequences.\n",
    "2.  An **Embedding** layer that converts integers to dense vectors using the pre-trained GloVe embeddings. `mask_zero=True` tells Keras to ignore the padding value (0). `trainable=False` freezes the embeddings so they don't change during training.\n",
    "3.  Our custom `MaskedAveragePooling1D` layer to get the document vector.\n",
    "4.  A final **Dense** layer with a sigmoid activation for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the embedding-averaging model\n",
    "def get_embedding_average(embeddings):\n",
    "\n",
    "    # Get the vocabulary size and embedding dimension from the shape of the embeddings matrix\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    # Define the input layer, which expects sequences of integers of variable length (None)\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Define the embedding layer\n",
    "    word_embedding_layer = Embedding(vocab_size,         # The number of words in our vocabulary\n",
    "                                    word_embedding_dim, # The dimension of each word vector\n",
    "                                    weights=[embeddings], # Initialize with our pre-trained GloVe embeddings\n",
    "                                    mask_zero=True,       # Enable masking to ignore padding (zeros)\n",
    "                                    trainable=False)      # Freeze the embeddings; we won't update them during training\n",
    "\n",
    "    \n",
    "    # Pass the input sequence through the embedding layer\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    # Apply our custom masked average pooling layer to get a single document vector\n",
    "    x=MaskedAveragePooling1D()(embedded_sequences)\n",
    "    \n",
    "    # Add a final dense layer with one neuron and a sigmoid activation function for binary classification\n",
    "    predictions=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Create the Keras Model, specifying the inputs and outputs\n",
    "    model = Model(inputs=word_sequence_input, outputs=predictions)\n",
    "\n",
    "    # Compile the model, defining the loss function, optimizer, and metrics\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 14: Model Summary\n",
    "We instantiate the model and print its summary to see the architecture and number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding averaging model\n",
    "embedding_model=get_embedding_average(embeddings)\n",
    "# Print a summary of the model's architecture\n",
    "print (embedding_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 15: Training the Averaging Model\n",
    "Now we train the baseline model. We use a `ModelCheckpoint` callback to save the best version of the model (based on validation loss) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current model to be the embedding averaging model\n",
    "model=embedding_model\n",
    "\n",
    "# Define the filename for saving the best model\n",
    "modelName=\"embedding_model.hdf5\"\n",
    "# Create a ModelCheckpoint callback to monitor validation loss and save only the best model\n",
    "checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, Y_train,                          # Training data and labels\n",
    "            validation_data=(devX, Y_dev),          # Validation data and labels\n",
    "            epochs=30, batch_size=128,              # Number of epochs and batch size\n",
    "            callbacks=[checkpoint])                 # List of callbacks to use during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add attention to that simple model to learn a *weighted* average over words---giving more weight to words in the document that are more important for representing the document for the purpose of this classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 17: Custom Attention Layer\n",
    "This custom `AttentionLayerMasking` layer calculates the attention weights. For each word vector in a sequence, it computes a score, turns these scores into a probability distribution (using softmax), and outputs these weights. It also correctly handles masking to ensure padded words get an attention weight of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Keras Layer to compute attention weights, with support for masking\n",
    "class AttentionLayerMasking(Layer):\n",
    "\n",
    "    # The constructor takes the output dimension (though not explicitly used here, it's good practice)\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(AttentionLayerMasking, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    # The `build` method is where weights are created. It's called automatically by Keras.\n",
    "    def build(self, input_shape):\n",
    "        # Get the dimension of the input embeddings\n",
    "        input_embedding_dim=input_shape[-1]\n",
    "        \n",
    "        # Create a trainable weight matrix (often called the context vector or kernel).\n",
    "        # Its job is to learn to project the input embeddings into a single score.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                            shape=(input_embedding_dim,1), # Shape allows dot product with each word vector\n",
    "                            initializer='uniform',         # Initialize weights uniformly\n",
    "                            trainable=True)                # This weight will be learned during training\n",
    "        super(AttentionLayerMasking, self).build(input_shape)\n",
    "\n",
    "    # This layer consumes the mask, so we don't propagate it further\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    # This `call` method contains the core logic for calculating attention\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        # 1. Compute scores: Perform a dot product between each word's representation `x` and the learned kernel.\n",
    "        # This results in a single score for each word in the sequence.\n",
    "        x=K.dot(x, self.kernel)\n",
    "        # 2. (Optional) Apply a non-linearity. Here we just exponentiate to make scores positive for softmax.\n",
    "        x=K.exp(x)\n",
    "        \n",
    "        # 3. Apply the mask: Zero out the scores for any padded words.\n",
    "        if mask is not None:\n",
    "            # Cast the boolean mask to floats (True -> 1.0, False -> 0.0)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            # Add a dimension to the mask so it can be broadcasted and multiplied with the scores `x`\n",
    "            mask = K.expand_dims(mask, axis=-1)\n",
    "            # Element-wise multiplication to zero out scores for padded positions\n",
    "            x = x * mask\n",
    "        \n",
    "        # 4. Normalize scores to get weights (Softmax): Divide each score by the sum of all scores in the sequence.\n",
    "        # This ensures the weights for each document sum to 1.\n",
    "        x /= K.sum(x, axis=1, keepdims=True)\n",
    "        # Squeeze the last dimension to get a final shape of (batch_size, timesteps)\n",
    "        x=K.squeeze(x, axis=2)\n",
    "\n",
    "        # Return the final attention weights\n",
    "        return x\n",
    "\n",
    "    # This method defines the shape of the layer's output\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The output is a vector of weights, one for each time step.\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 18: Building the Attention Model\n",
    "This function builds the new model architecture:\n",
    "1.  **Input** and **Embedding** layers are the same as before.\n",
    "2.  A **Dense** layer with a `tanh` activation is applied to the word embeddings. This projects the embeddings into a new space, allowing the model to learn a representation specifically for calculating attention.\n",
    "3.  Our custom `AttentionLayerMasking` takes these transformed embeddings and computes the attention weights.\n",
    "4.  A **Lambda** layer performs a batch-wise dot product between the attention weights and the *original* word embeddings. This computes the weighted average, creating the final document representation (context vector).\n",
    "5.  A final **Dense** layer with a sigmoid activation performs the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the model with the attention mechanism\n",
    "def get_embedding_with_attention_masking(embeddings):\n",
    "\n",
    "    # Get the vocabulary size and embedding dimension\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    # Define the input layer for sequences of integers\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Define the embedding layer, initialized with pre-trained weights and masking enabled\n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    # Get the embedded sequences from the input\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    # 1. Transform word embeddings into a new representation for attention calculation\n",
    "    # A Dense layer with a 'tanh' activation is a common choice for this.\n",
    "    attention_key_dim=300\n",
    "    attention_input=Dense(attention_key_dim, activation='tanh')(embedded_sequences)\n",
    "\n",
    "    # 2. Compute attention weights using our custom layer\n",
    "    # The output `attention_output` has shape (batch_size, timesteps) and contains the weight for each word.\n",
    "    attention_output = AttentionLayerMasking(word_embedding_dim, name=\"attention\")(attention_input)\n",
    "    \n",
    "    # 3. Compute the document representation as a weighted average of the original embeddings\n",
    "    # We use a Lambda layer to perform a batch dot product between the attention weights and the embeddings.\n",
    "    # This effectively calculates sum(attention_weight_i * embedding_i) for each document.\n",
    "    document_representation = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=1), name='dot')([attention_output,embedded_sequences])\n",
    "\n",
    "    # 4. Classify the resulting document representation\n",
    "    x=Dense(1, activation=\"sigmoid\")(document_representation)\n",
    "\n",
    "    # Create the Keras Model\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 19: Attention Model Summary\n",
    "We instantiate the attention model and print its summary. Notice the additional `Dense` and `AttentionLayerMasking` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the attention-based model\n",
    "embedding_attention_model=get_embedding_with_attention_masking(embeddings)\n",
    "# Print a summary of the model's architecture\n",
    "print (embedding_attention_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 20: Training the Attention Model\n",
    "We train the attention model, again using `ModelCheckpoint` to save the best performing version on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current model to be the attention model\n",
    "model=embedding_attention_model\n",
    "\n",
    "# Define the filename for saving the best model\n",
    "modelName=\"embedding_attention_model.hdf5\"\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, Y_train, \n",
    "            validation_data=(devX, Y_dev),\n",
    "            epochs=30, batch_size=128,\n",
    "            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore what words in a document a learned attention model is attending to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 22: Loading the Best Model\n",
    "We first load the weights of the best model that were saved by the `ModelCheckpoint` callback during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-instantiate the model structure\n",
    "model=embedding_attention_model\n",
    "# Load the saved weights from the best epoch during training\n",
    "model.load_weights(\"embedding_attention_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 23: Attention Analysis Function\n",
    "This function, `analyze`, visualizes the attention weights. It uses a Keras `function` to create a \"functor\" that can access the outputs of intermediate layers of the model. We use this to get the output of our `AttentionLayerMasking` layer for a given input sentence. It then prints each word with its corresponding weight and plots the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to analyze and visualize attention weights for a given document\n",
    "def analyze(model, doc):\n",
    "    \n",
    "    # Tokenize the input document string\n",
    "    words=doc.split(\" \")\n",
    "    # Convert the tokens into a padded sequence of word IDs\n",
    "    text = get_word_ids([words], vocab, max_length=len(words))\n",
    "   \n",
    "    # Create a Keras Function to get the output of intermediate layers\n",
    "    # `model.input` is the model's input tensor\n",
    "    inp = model.input                                    \n",
    "    # `outputs` is a list of the output tensors of each layer in the model (skipping the input layer itself)\n",
    "    outputs = [layer.output for layer in model.layers[1:]]       \n",
    "    # The functor takes the input and the learning phase (0 for test/inference) and returns the layer outputs\n",
    "    functor = K.function([inp, K.learning_phase()], outputs) \n",
    "\n",
    "    # Prepare the input text for the model\n",
    "    test = text[0]\n",
    "    orig=words\n",
    "    attention_weights=[]\n",
    "    # Reshape the input to have a batch dimension of 1\n",
    "    test=test.reshape((1,len(words)))\n",
    "    # Run the input through the functor to get the outputs of all layers\n",
    "    layer_outs = functor([test, 0.])\n",
    "\n",
    "    # The attention layer is the 3rd layer in our model (0=Input, 1=Embedding, 2=Dense, 3=Attention). \n",
    "    # NOTE: The index might change if the model architecture is modified. Here we access the output of the 'attention' layer.\n",
    "    attention_layer=layer_outs[2]\n",
    "    \n",
    "    # Iterate through the words and their corresponding attention weights\n",
    "    for i in range(len(words)):\n",
    "        # Get the attention weight for the i-th word\n",
    "        val=attention_layer[0,i]\n",
    "        # Append it to our list\n",
    "        attention_weights.append(val)\n",
    "        # Print the weight and the word\n",
    "        print (\"%.3f\\t%s\" % (val, orig[i]))\n",
    "        \n",
    "    # Create a pandas DataFrame for easy plotting\n",
    "    df = pd.DataFrame({'words':orig, 'attention':attention_weights})\n",
    "    # Create a bar plot to visualize the attention weights\n",
    "    ax = df.plot.bar(x='words', y='attention', figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 24: Visualizing Attention on a Positive Example\n",
    "Let's see which words get the most attention in a simple positive sentence. We expect words like \"love\" to have a high weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a positive input sentence\n",
    "text=\"i love this movie !\"\n",
    "# Analyze the attention weights for this sentence\n",
    "analyze(model, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 25: Visualizing Attention on a Negative Example\n",
    "Now let's try a negative sentence. Here, we expect words like \"not\" and \"love\" to be important for the model to make the correct prediction. The attention mechanism should highlight them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a negative input sentence\n",
    "text=\"i do not love this movie !\"\n",
    "# Analyze the attention weights for this sentence\n",
    "analyze(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}