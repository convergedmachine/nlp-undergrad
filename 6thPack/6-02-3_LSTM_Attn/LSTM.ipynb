{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook demonstrates how to build and compare different Long Short-Term Memory (LSTM) network architectures for text classification using Keras. It explores four main approaches to represent a document:\n",
    "* Using the final hidden state of a standard LSTM.\n",
    "* Using the concatenated final hidden states of a Bidirectional LSTM (BiLSTM).\n",
    "* Averaging all the output states from a BiLSTM over the entire sequence.\n",
    "* Taking the maximum value (max-pooling) of all output states from a BiLSTM over the entire sequence.\n",
    "\n",
    "A key focus is on correctly handling padded sequences when performing averaging or max-pooling, which is achieved by implementing custom Keras layers with masking support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 1: Imports**\n",
    "This cell imports all the necessary libraries and modules. We import Keras for building the neural network, NumPy for numerical operations, and scikit-learn for label encoding. Specific layers like `LSTM`, `Dense`, `Embedding`, and `Bidirectional` are imported from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Keras library, the high-level API for TensorFlow\n",
    "import keras\n",
    "# Import NumPy for numerical operations, especially for handling arrays\n",
    "import numpy as np\n",
    "# Import the preprocessing module from scikit-learn for tasks like label encoding\n",
    "from sklearn import preprocessing\n",
    "# Import necessary layers and components from Keras to build the model\n",
    "from keras.layers import Dense, Input, Embedding, GlobalAveragePooling1D, Lambda, Layer, Multiply, GlobalMaxPooling1D, Conv1D, Concatenate, Dropout, LSTM, Bidirectional\n",
    "# Import the Model class to create a neural network model, and Sequential for linear stacks of layers\n",
    "from keras.models import Model, Sequential\n",
    "# Import the Keras backend (K) to access low-level functions, useful for custom layers\n",
    "from keras import backend as K\n",
    "# Import TensorFlow, which Keras uses as its backend engine\n",
    "import tensorflow as tf\n",
    "# Import callbacks for saving the model, stopping training early, and custom actions during training\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 2: `load_embeddings` Function**\n",
    "This function reads pre-trained word embeddings (like GloVe) from a file. It builds a vocabulary dictionary mapping words to integer IDs and an embedding matrix. Special tokens for padding (`_0_`) and unknown words (`_UNK_`) are added at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load pre-trained word embeddings from a file\n",
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    # Initialize a dictionary to store the vocabulary (word -> integer id)\n",
    "    vocab={}\n",
    "    # Initialize a list to store the embedding vectors\n",
    "    embeddings=[]\n",
    "    # Open and read the specified file\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        # Read the first line to get the number of words and embedding dimension\n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        # Add a zero vector for the padding token (ID 0)\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add a zero vector for the Unknown Word token (ID 1)\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add the padding token to the vocabulary with ID 0\n",
    "        vocab[\"_0_\"]=0\n",
    "        # Add the UNK token to the vocabulary with ID 1\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        # Iterate through each line of the embeddings file\n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            # Stop reading if the maximum vocabulary size is reached\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            # Split the line into the word and its vector components\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            # Convert the vector components to a NumPy array of floats\n",
    "            val=np.array(cols[1:])\n",
    "            # Get the word\n",
    "            word=cols[0]\n",
    "            \n",
    "            # Add the embedding vector to our list of embeddings\n",
    "            embeddings.append(val)\n",
    "            # Add the word and its corresponding index (ID) to the vocabulary\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    # Convert the list of embeddings to a NumPy array and return it along with the vocabulary\n",
    "    return np.array(embeddings), vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 3: `read_data` Function**\n",
    "This function reads a tab-separated value (TSV) file containing text data and corresponding labels. It splits each line into a label and a pre-tokenized text, then appends them to separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read text data and labels from a file\n",
    "def read_data(filename, vocab):\n",
    "    # Initialize a list to store the text sequences (documents)\n",
    "    X=[]\n",
    "    # Initialize a list to store the corresponding labels\n",
    "    Y=[]\n",
    "    # Open the file with UTF-8 encoding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Strip whitespace and split the line by the tab character\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label\n",
    "            label=cols[0]\n",
    "            # The second column is the text, which is assumed to be already tokenized (space-separated)\n",
    "            text=cols[1].split(\" \")\n",
    "            # Append the list of tokens to the main text list\n",
    "            X.append(text)\n",
    "            # Append the label to the main labels list\n",
    "            Y.append(label)\n",
    "    # Return the lists of texts and labels\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 4: `get_word_ids` Function**\n",
    "This function converts a list of documents (each a list of tokens) into a matrix of integer IDs. It looks up each token in the vocabulary. If a token is not found, it's assigned the ID for \"Unknown\" (UNK). Each document is then padded with zeros to ensure all sequences have the same length (`max_length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert documents (lists of tokens) into sequences of word IDs\n",
    "def get_word_ids(docs, vocab, max_length=200):\n",
    "    \n",
    "    # Initialize a list to hold the ID sequences for all documents\n",
    "    doc_ids=[]\n",
    "    \n",
    "    # Iterate through each document in the input list\n",
    "    for doc in docs:\n",
    "        # Initialize a list to hold the word IDs for the current document\n",
    "        wids=[]\n",
    "\n",
    "        # Iterate through each token in the document, up to the max_length\n",
    "        for token in doc[:max_length]:\n",
    "            # Look up the token in the vocabulary (converted to lowercase). If not found, use the ID for UNK (1).\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            # Append the ID to the current document's list of IDs\n",
    "            wids.append(val)\n",
    "        \n",
    "        # Pad the sequence with zeros (ID 0) up to the max_length\n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "\n",
    "        # Add the padded sequence of IDs to the main list\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    # Convert the list of ID sequences to a NumPy array and return it\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 5: Loading Embeddings**\n",
    "This cell calls the `load_embeddings` function to load the first 100,000 GloVe word embeddings from a file. The function returns the embedding matrix and the vocabulary dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe embeddings and vocabulary from the specified file\n",
    "# We limit the vocabulary size to the first 100,000 words for efficiency\n",
    "embeddings, vocab=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 6: Data Directory Path**\n",
    "This cell specifies the directory where the training, development (validation), and test datasets are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "# Set the path to the directory containing the text classification data\n",
    "directory=\"../data/text_classification_sample_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 7: Reading Training and Development Data**\n",
    "Here, the `read_data` function is used to load the text and labels for both the training and development sets from their respective `.tsv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data (text and labels) from 'train.tsv'\n",
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "# Read the development (validation) data from 'dev.tsv'\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 8: Converting Text to Word IDs**\n",
    "The `get_word_ids` function is called to convert the raw text of the training and development sets into padded sequences of integer IDs, which can be fed into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training text data into padded sequences of word IDs\n",
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "# Convert the development text data into padded sequences of word IDs\n",
    "devX = get_word_ids(devText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 9: Encoding Labels**\n",
    "The string labels (e.g., \"positive\", \"negative\") are converted into numerical format (0s and 1s) using scikit-learn's `LabelEncoder`. This is necessary for training a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a LabelEncoder to convert string labels to integers\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Fit the encoder on the training labels to learn the label-to-integer mapping\n",
    "le.fit(trainY)\n",
    "# Transform the training labels into their integer representations and convert to a NumPy array\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "# Transform the development labels into their integer representations and convert to a NumPy array\n",
    "Y_dev=np.array(le.transform(devY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 10: `train` Function**\n",
    "This is a helper function that standardizes the training process. It takes a compiled Keras model, prints its summary, and then trains it on the training data (`trainX`, `Y_train`) for 30 epochs, using the development data (`devX`, `Y_dev`) for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to train a given Keras model\n",
    "def train(model):\n",
    "    # Print a summary of the model's architecture (layers, parameters, etc.)\n",
    "    print (model.summary())\n",
    "    # Train the model using the .fit() method\n",
    "    model.fit(trainX, Y_train, \n",
    "                # Provide the development set for validation after each epoch\n",
    "                validation_data=(devX, Y_dev),\n",
    "                # Set the number of training epochs to 30\n",
    "                epochs=30, \n",
    "                # Set the batch size to 32\n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple LSTM\n",
    "First, we'll train a simple LSTM. In this model, the entire document is represented by the final hidden state vector output by the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 11: `get_simple_lstm` Function**\n",
    "This function defines and compiles a simple LSTM model.\n",
    "1.  **Input Layer**: Takes sequences of word IDs.\n",
    "2.  **Embedding Layer**: Converts word IDs into dense vectors using the pre-trained embeddings. `mask_zero=True` tells the model to ignore padded zeros in subsequent layers.\n",
    "3.  **LSTM Layer**: Processes the sequence of embeddings. `return_sequences=False` means it only outputs the final hidden state.\n",
    "4.  **Dense Layer**: A fully connected output layer with a sigmoid activation function for binary classification.\n",
    "5.  **Compilation**: The model is compiled with binary cross-entropy loss and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a simple LSTM model\n",
    "def get_simple_lstm(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    # Get the vocabulary size and embedding dimension from the shape of the embeddings matrix\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    # Define the input layer, which expects sequences of integers of variable length\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Define the embedding layer\n",
    "    word_embedding_layer = Embedding(vocab_size,          # The size of the vocabulary\n",
    "                                    word_embedding_dim,  # The dimension of the embeddings\n",
    "                                    weights=[embeddings],  # Initialize with pre-trained embeddings\n",
    "                                    mask_zero=True,      # Enable masking for padded inputs (value 0)\n",
    "                                    trainable=False)     # Freeze the embedding weights during training\n",
    "\n",
    "    \n",
    "    # Pass the input sequence through the embedding layer\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    # Add an LSTM layer. It will only return the final hidden state because return_sequences=False.\n",
    "    lstm = LSTM(lstm_size, return_sequences=False, activation='tanh', dropout=dropout_rate)(embedded_sequences)\n",
    "  \n",
    "    # Add the final dense output layer with a sigmoid activation for binary classification\n",
    "    predictions=Dense(1, activation=\"sigmoid\")(lstm)\n",
    "\n",
    "    # Create the Keras Model by specifying its inputs and outputs\n",
    "    model = Model(inputs=word_sequence_input, outputs=predictions)\n",
    "\n",
    "    # Compile the model with binary cross-entropy loss, the Adam optimizer, and accuracy metric\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 12: Train the Simple LSTM**\n",
    "This cell calls the `train` function to build and train the simple LSTM model defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create and train the simple LSTM model using the helper function\n",
    "train(get_simple_lstm(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Bidirectional LSTM (BiLSTM)\n",
    "Next, we'll use a Bidirectional LSTM. A BiLSTM consists of two LSTMs: one processing the sequence from start to end (forward), and another from end to start (backward). The document is represented by concatenating the final hidden states of both LSTMs. This captures context from both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 13: `get_simple_bilstm` Function**\n",
    "This function defines a BiLSTM model. It's similar to the simple LSTM, but the `LSTM` layer is wrapped in a `Bidirectional` layer. The `merge_mode='concat'` argument specifies that the final forward and backward hidden states should be concatenated to form the final representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a simple Bidirectional LSTM (BiLSTM) model\n",
    "def get_simple_bilstm(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    # Get the vocabulary size and embedding dimension from the embeddings matrix\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    # Define the input layer for sequences of word IDs\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Define the embedding layer, initialized with pre-trained weights and masking enabled\n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings],\n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    # Pass the input through the embedding layer\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    # Add a Bidirectional LSTM layer. It wraps a standard LSTM.\n",
    "    # merge_mode='concat' concatenates the final forward and backward hidden states.\n",
    "    # return_sequences=False ensures only the final states are output.\n",
    "    bi_lstm = Bidirectional(LSTM(lstm_size, return_sequences=False, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "  \n",
    "    # Add the final dense output layer for classification\n",
    "    predictions=Dense(1, activation=\"sigmoid\")(bi_lstm)\n",
    "\n",
    "    # Create the Keras Model\n",
    "    model = Model(inputs=word_sequence_input, outputs=predictions)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 14: Train the Simple BiLSTM**\n",
    "This cell builds and trains the simple BiLSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and train the simple BiLSTM model\n",
    "train(get_simple_bilstm(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Pooling Strategies with Masking\n",
    "The final hidden state can sometimes be a bottleneck, losing information from the beginning of a long sequence. A better approach is often to aggregate information from the *entire* sequence. This can be done by using the output of the LSTM at every time step (`return_sequences=True`).\n",
    "\n",
    "However, since our sequences are padded with zeros, a simple average or max-pooling would be skewed by these pads. We need to create custom Keras layers that support masking to ensure the pooling operations are only performed over the actual, non-padded parts of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 15: `MaskedAveragePooling1D` Custom Layer**\n",
    "This custom Keras layer performs average pooling over the time-step dimension but correctly handles masking. It multiplies the inputs by the mask to zero out the padded steps, then calculates the sum and divides by the true number of non-masked steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Keras Layer for average pooling that supports masking\n",
    "class MaskedAveragePooling1D(Layer):\n",
    "    # The initialization method for the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        # Indicate that this layer supports masking\n",
    "        self.supports_masking = True\n",
    "        # Call the parent class's constructor\n",
    "        super(MaskedAveragePooling1D, self).__init__(**kwargs)\n",
    "\n",
    "    # Define how the mask is computed for the output of this layer (it doesn't pass on a mask)\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    # This is the main logic of the layer\n",
    "    def call(self, x, mask=None):\n",
    "        # Check if a mask was provided by the previous layer\n",
    "        if mask is not None:\n",
    "            # Cast the boolean mask to float type (e.g., True -> 1.0, False -> 0.0)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            # Repeat the mask to match the shape of the input tensor 'x' along the last dimension\n",
    "            mask = K.repeat(mask, x.shape[-1])\n",
    "            # Transpose the mask to align its dimensions for element-wise multiplication\n",
    "            mask = tf.transpose(mask, [0,2,1])\n",
    "            # Multiply the input 'x' by the mask to zero out the padded (masked) elements\n",
    "            x = x * mask\n",
    "            \n",
    "        # Sum the elements of 'x' along the time step axis (axis=1)\n",
    "        # and divide by the sum of the mask to get the true average over non-padded steps\n",
    "        return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "\n",
    "    # Define the shape of the layer's output\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The output shape is (batch_size, features), removing the time step dimension\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 16: `MaskedMaxPooling1D` Custom Layer**\n",
    "This custom layer performs max pooling while respecting the mask. Instead of zeroing out padded values (which could still be chosen if all other values are negative), it subtracts a very large number from the masked positions. This ensures they will never be selected as the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Keras Layer for max pooling that supports masking\n",
    "class MaskedMaxPooling1D(Layer):\n",
    "    # The initialization method for the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        # Indicate that this layer supports masking\n",
    "        self.supports_masking = True\n",
    "        # Call the parent class's constructor\n",
    "        super(MaskedMaxPooling1D, self).__init__(**kwargs)\n",
    "\n",
    "    # Define how the mask is computed for the output of this layer (it doesn't pass on a mask)\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    # This is the main logic of the layer\n",
    "    def call(self, x, mask=None):\n",
    "        # Check if a mask was provided by the previous layer\n",
    "        if mask is not None:\n",
    "            # Invert the mask (e.g., True -> False, False -> True) because we want to penalize masked steps\n",
    "            mask=tf.logical_not(mask)\n",
    "            # Cast the boolean mask to float type (e.g., True -> 1.0, False -> 0.0)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            # Repeat the mask to match the shape of the input tensor 'x'\n",
    "            mask = K.repeat(mask, x.shape[-1])    \n",
    "            # Transpose the mask to align its dimensions for element-wise operation\n",
    "            mask = tf.transpose(mask, [0,2,1])\n",
    "            \n",
    "            # Multiply the mask by a large number (this will be subtracted)\n",
    "            mask *= 10000\n",
    "            # Subtract the large number from the masked positions in 'x'\n",
    "            x = x - mask\n",
    "        \n",
    "        # Compute the maximum value along the time step axis (axis=1)\n",
    "        return K.max(x, axis=1)\n",
    "    \n",
    "    # Define the shape of the layer's output\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The output shape is (batch_size, features), removing the time step dimension\n",
    "        return (input_shape[0], input_shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: BiLSTM with Masked Average Pooling\n",
    "Now we'll build a model that uses the custom `MaskedAveragePooling1D` layer. The BiLSTM is configured with `return_sequences=True` to output the hidden state at every time step. These outputs are then averaged by our custom layer to create a single vector representation for the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 17: `get_bilstm_with_average_pooling` Function**\n",
    "This function defines the BiLSTM model followed by the custom average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a BiLSTM model with masked average pooling\n",
    "def get_bilstm_with_average_pooling(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    # Get vocab size and embedding dimension\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    # Define the input layer\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Define the embedding layer with masking enabled\n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    # Pass input through the embedding layer\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    # Add a BiLSTM layer. 'return_sequences=True' makes it output the hidden state for each time step.\n",
    "    x = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "    # Apply our custom masked average pooling layer to aggregate the sequences into a single vector\n",
    "    x=MaskedAveragePooling1D()(x)\n",
    "\n",
    "    # Add the final dense output layer\n",
    "    x=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Create the Keras Model\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 18: Train the BiLSTM with Average Pooling**\n",
    "This cell builds and trains the BiLSTM model with masked average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the BiLSTM with masked average pooling\n",
    "train(get_bilstm_with_average_pooling(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: BiLSTM with Masked Max Pooling\n",
    "This final model is similar to the previous one, but instead of averaging the BiLSTM outputs, it uses the custom `MaskedMaxPooling1D` layer to take the maximum value across the time steps. Max pooling is effective at capturing the most important feature or signal in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 19: `get_bilstm_with_max_pooling` Function**\n",
    "This function defines the BiLSTM model followed by the custom max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a BiLSTM model with masked max pooling\n",
    "def get_bilstm_with_max_pooling(embeddings, lstm_size=25, dropout_rate=0.2):\n",
    "\n",
    "    # Get vocab size and embedding dimension\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    # Define the input layer\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Define the embedding layer with masking enabled\n",
    "    word_embedding_layer = Embedding(vocab_size,\n",
    "                                    word_embedding_dim,\n",
    "                                    weights=[embeddings], \n",
    "                                     mask_zero=True,\n",
    "                                    trainable=False)\n",
    "\n",
    "    \n",
    "    # Pass input through the embedding layer\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    \n",
    "    # Add a BiLSTM layer that outputs the hidden state for each time step\n",
    "    x = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "    # Apply our custom masked max pooling layer to aggregate the sequences\n",
    "    x=MaskedMaxPooling1D()(x)\n",
    "\n",
    "    # Add the final dense output layer\n",
    "    x=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Create the Keras Model\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell 20: Train the BiLSTM with Max Pooling**\n",
    "This final cell builds and trains the BiLSTM model with masked max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the BiLSTM with masked max pooling\n",
    "train(get_bilstm_with_max_pooling(embeddings, lstm_size=25, dropout_rate=0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}