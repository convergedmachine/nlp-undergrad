{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of a bidirectional LSTM with attention for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n",
    "This cell imports all the necessary libraries for the project.\n",
    "* **Keras** and **TensorFlow** are used for building and training the neural network.\n",
    "* **NumPy** and **Pandas** are used for numerical operations and data manipulation.\n",
    "* **Scikit-learn** provides utilities like the `LabelEncoder`.\n",
    "* **Scipy** and **math** are used for statistical calculations, specifically for the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main Keras library\n",
    "import keras\n",
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import preprocessing tools from scikit-learn, specifically for encoding labels\n",
    "from sklearn import preprocessing\n",
    "# Import specific layers and components needed from Keras to build the model\n",
    "from keras.layers import Dense, Input, Embedding, Lambda, Layer, Multiply, Dropout, Dot, Bidirectional, LSTM\n",
    "# Import the Model class to create the final model object\n",
    "from keras.models import Model\n",
    "# Import the Keras backend for low-level operations\n",
    "from keras import backend as K\n",
    "# Import TensorFlow, often used with Keras\n",
    "import tensorflow as tf\n",
    "# Import Keras callbacks for saving the best model and stopping training early\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "# Import pandas for data manipulation (though not heavily used in this script)\n",
    "import pandas as pd\n",
    "# Import the normal distribution function from scipy.stats to calculate z-scores\n",
    "from scipy.stats import norm\n",
    "# Import the square root function for calculating standard error\n",
    "from math import sqrt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Word Embeddings\n",
    "This function is designed to load pre-trained word embeddings from a file (in word2vec format). It reads each word and its corresponding vector, stores them in a matrix, and creates a vocabulary that maps each word to an integer index. It also adds special tokens for padding (`_0_`) and unknown words (`_UNK_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load word embeddings from a file\n",
    "def load_embeddings(filename, max_vocab_size):\n",
    "\n",
    "    # Initialize a dictionary to store the vocabulary (word -> index)\n",
    "    vocab={}\n",
    "    # Initialize a list to store the embedding vectors\n",
    "    embeddings=[]\n",
    "    # Open the specified file to read the embeddings\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        # Read the first line, which contains the vocab size and embedding dimension\n",
    "        cols=file.readline().split(\" \")\n",
    "        # Convert the number of words to an integer\n",
    "        num_words=int(cols[0])\n",
    "        # Convert the embedding dimension to an integer\n",
    "        size=int(cols[1])\n",
    "        # Append a vector of zeros for the padding token (at index 0)\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Append a vector of zeros for the unknown word token (at index 1)\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add the padding token to our vocabulary with index 0\n",
    "        vocab[\"_0_\"]=0\n",
    "        # Add the unknown word token to our vocabulary with index 1\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        # Iterate over each line in the embeddings file with an index\n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            # If we have reached our desired maximum vocabulary size, stop reading\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            # Strip whitespace and split the line into word and vector components\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            # Convert the vector components into a NumPy array of floats\n",
    "            val=np.array(cols[1:])\n",
    "            # The first column is the word itself\n",
    "            word=cols[0]\n",
    "            \n",
    "            # Add the word's vector to our list of embeddings\n",
    "            embeddings.append(val)\n",
    "            # Add the word to our vocabulary, mapping it to its new index (idx + 2)\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    # Convert the list of embeddings to a NumPy array and return it along with the vocab and embedding size\n",
    "    return np.array(embeddings), vocab, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read Text Data\n",
    "This function reads a tab-separated (TSV) file. It assumes each line contains a label followed by a tab, followed by pre-tokenized text. It parses these lines and returns two lists: one containing the tokenized texts (`X`) and another containing the corresponding labels (`Y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read the training/dev/test data\n",
    "def read_data(filename, vocab):\n",
    "    # Initialize a list to hold the documents (features)\n",
    "    X=[]\n",
    "    # Initialize a list to hold the labels\n",
    "    Y=[]\n",
    "    # Open the data file, specifying UTF-8 encoding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Strip whitespace and split the line by the tab character\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # The first column is the label\n",
    "            label=cols[0]\n",
    "            # The second column is the text, which is already tokenized; split it by spaces\n",
    "            text=cols[1].split(\" \")\n",
    "            # Add the list of tokens to the features list\n",
    "            X.append(text)\n",
    "            # Add the label to the labels list\n",
    "            Y.append(label)\n",
    "    # Return the lists of texts and labels\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert Words to Integer IDs\n",
    "This function takes the tokenized documents and converts them into a numerical format that the model can understand. It maps each token to its ID from the vocabulary. It also ensures all sequences have the same length by truncating longer ones and padding shorter ones with a '0' ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert documents of words into sequences of integer IDs\n",
    "def get_word_ids(docs, vocab, max_length=200):\n",
    "    \n",
    "    # Initialize a list to store the ID sequences for all documents\n",
    "    doc_ids=[]\n",
    "    \n",
    "    # Iterate through each document (which is a list of tokens)\n",
    "    for doc in docs:\n",
    "        # Initialize a list to store word IDs for the current document\n",
    "        wids=[]\n",
    "        # Iterate through each token in the document, up to the maximum length\n",
    "        for token in doc[:max_length]:\n",
    "            # Look up the token in the vocabulary (after converting to lowercase). If not found, use the ID for UNK (1).\n",
    "            val = vocab[token.lower()] if token.lower() in vocab else 1\n",
    "            # Append the corresponding ID to the list for the current document\n",
    "            wids.append(val)\n",
    "        \n",
    "        # Pad the current document's ID sequence with 0s to make it 'max_length' long\n",
    "        for i in range(len(wids),max_length):\n",
    "            wids.append(0)\n",
    "\n",
    "        # Add the final padded ID sequence for the document to the main list\n",
    "        doc_ids.append(wids)\n",
    "\n",
    "    # Convert the list of lists into a NumPy array and return it\n",
    "    return np.array(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't downloaded the glove vectors, do so first -- the top 50K words in the \"Common Crawl (42B)\"  vectors (300-dimensional) can be found here: [glove.42B.300d.50K.txt](https://drive.google.com/file/d/1n1jt0UIdI3CD26cY1EIeks39XH5S8O8M/view?usp=sharing); download it and place  in your `data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convert GloVe to Word2Vec Format\n",
    "The `load_embeddings` function is written to parse the word2vec file format (which has a header line with vocabulary size and dimension). The original GloVe file does not have this header. This cell uses a utility from the `gensim` library to convert the GloVe file into the word2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the conversion utility from gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Specify the path to the input GloVe file\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "# Specify the path for the output file in word2vec format\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "# Run the conversion function; the return value is not needed, so it's assigned to _\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Execute Embedding Loading\n",
    "This cell calls the `load_embeddings` function to actually load the converted GloVe vectors into memory. This populates the `embeddings` matrix, the `vocab` dictionary, and the `embedding_size` variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to load the embeddings from the converted file, limiting the vocabulary to 50,000 words\n",
    "embeddings, vocab, embedding_size=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Set Data Directory\n",
    "This cell specifies the directory where the training, development, and test data files (`train.tsv`, `dev.tsv`, `test.tsv`) are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "# Set the directory path for the dataset\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Read All Datasets\n",
    "Using the `read_data` function defined earlier, this cell reads the text and labels from the `train.tsv`, `dev.tsv`, and `test.tsv` files into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data from the train.tsv file\n",
    "trainText, trainY=read_data(\"%s/train.tsv\" % directory, vocab)\n",
    "# Read the development (validation) data from the dev.tsv file\n",
    "devText, devY=read_data(\"%s/dev.tsv\" % directory, vocab)\n",
    "# Read the test data from the test.tsv file\n",
    "testText, testY=read_data(\"%s/test.tsv\" % directory, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Vectorize All Text Data\n",
    "This cell converts all the raw text data (for train, dev, and test sets) into padded sequences of integer IDs using the `get_word_ids` function. This is the final preprocessing step for the input features (`X`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training text into padded sequences of word IDs\n",
    "trainX = get_word_ids(trainText, vocab, max_length=200)\n",
    "# Convert the development text into padded sequences of word IDs\n",
    "devX = get_word_ids(devText, vocab, max_length=200)\n",
    "# Convert the test text into padded sequences of word IDs\n",
    "testX = get_word_ids(testText, vocab, max_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Encode Labels\n",
    "The model's output layer requires numerical labels. This cell uses `scikit-learn`'s `LabelEncoder` to convert the string labels (e.g., \"positive\", \"negative\") into integers (e.g., 1, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a LabelEncoder object from scikit-learn\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Fit the encoder on the training labels to learn the classes\n",
    "le.fit(trainY)\n",
    "# Transform the training labels into integers and convert to a NumPy array\n",
    "Y_train=np.array(le.transform(trainY))\n",
    "# Transform the development labels into integers and convert to a NumPy array\n",
    "Y_dev=np.array(le.transform(devY))\n",
    "# Transform the test labels into integers and convert to a NumPy array\n",
    "Y_test=np.array(le.transform(testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Custom Attention Layer\n",
    "This cell defines a custom Keras layer for the attention mechanism. \n",
    "* **build()**: Creates the trainable weight `kernel` used to calculate attention scores.\n",
    "* **call()**: Implements the forward pass. It computes an importance score for each input timestep, applies a softmax to get attention weights, and crucially, uses the `mask` to ignore padded parts of the sequence.\n",
    "* **compute_mask()**: Indicates this layer consumes the mask but doesn't produce a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Keras Layer for attention that handles masking\n",
    "class AttentionLayerMasking(Layer):\n",
    "\n",
    "    # The initializer for the layer\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        # Store the output dimension\n",
    "        self.output_dim = output_dim\n",
    "        # Call the parent class's initializer\n",
    "        super(AttentionLayerMasking, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    # This method creates the layer's trainable weights\n",
    "    def build(self, input_shape):\n",
    "        # Get the dimension of the input embeddings (last dimension of the input shape)\n",
    "        input_embedding_dim=input_shape[-1]\n",
    "        \n",
    "        # Add a trainable weight (kernel) to the layer\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                            # The shape is (input_dim, 1) to compute a dot product\n",
    "                            shape=(input_embedding_dim,1),\n",
    "                            # Initialize weights uniformly\n",
    "                            initializer='uniform',\n",
    "                            # This weight should be trainable\n",
    "                            trainable=True)\n",
    "        # Call the parent class's build method\n",
    "        super(AttentionLayerMasking, self).build(input_shape)\n",
    "\n",
    "    # This method specifies that the layer does not propagate a mask\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # Return None, as the output of this layer is the attention weights, not a sequence\n",
    "        return None\n",
    "\n",
    "    # This method contains the layer's logic (the forward pass)\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        # Calculate the dot product of the input tensor 'x' and the learned kernel\n",
    "        # This computes an unnormalized importance score for each time step\n",
    "        x=K.dot(x, self.kernel)\n",
    "        # Apply an exponentiation, a step in the softmax function\n",
    "        x=K.exp(x)\n",
    "        \n",
    "        # If a mask is provided (from the Embedding layer), apply it\n",
    "        if mask is not None:\n",
    "            # Cast the boolean mask to the backend's float type (e.g., float32)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            # Add a new dimension to the mask to make it compatible for element-wise multiplication\n",
    "            mask = K.expand_dims(mask, axis=-1)\n",
    "            # Multiply the scores by the mask to zero out scores for padded time steps\n",
    "            x = x * mask\n",
    "        \n",
    "        # Normalize the scores by dividing by the sum over the time-step axis (axis 1)\n",
    "        # This completes the softmax operation, yielding attention weights that sum to 1\n",
    "        x /= K.sum(x, axis=1, keepdims=True)\n",
    "        # Remove the last dimension, which is of size 1\n",
    "        x=K.squeeze(x, axis=2)\n",
    "\n",
    "        # Return the computed attention weights\n",
    "        return x\n",
    "\n",
    "    # This method computes the output shape of the layer\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The output shape is (batch_size, num_timesteps)\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Implement a BiLSTM with attention. Feel free to base your code on the models in Attention.ipynb and LSTM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. BiLSTM with Attention Model Architecture\n",
    "This function constructs the Keras model.\n",
    "1.  **Input Layer**: Defines the expected input shape.\n",
    "2.  **Embedding Layer**: Maps integer IDs to GloVe vectors. `mask_zero=True` is critical; it tells subsequent layers to ignore the padded '0's.\n",
    "3.  **Bidirectional LSTM**: Processes the sequence of vectors. `return_sequences=True` is necessary to get the output of every timestep for the attention mechanism.\n",
    "4.  **Dense 'tanh' Layer**: Transforms the BiLSTM outputs into a new representation before calculating attention. This helps the model learn a better context for attention.\n",
    "5.  **Attention Layer**: Uses our custom `AttentionLayerMasking` to calculate attention weights.\n",
    "6.  **Lambda Layer**: Calculates the weighted sum of the BiLSTM outputs using the attention weights, producing a single vector representation for the entire document.\n",
    "7.  **Output Layer**: A final `Dense` layer with a sigmoid activation function for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that builds and returns the BiLSTM with attention model\n",
    "def get_bilstm_with_attention_masking(embeddings, lstm_size=25, dropout_rate=0.25):\n",
    "\n",
    "    # Get the vocabulary size and embedding dimension from the shape of the embeddings matrix\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "    \n",
    "    # Define the input layer for sequences of integers\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Define the embedding layer\n",
    "    word_embedding_layer = Embedding(vocab_size,               # The size of the vocabulary\n",
    "                                    word_embedding_dim,         # The dimension of the embeddings\n",
    "                                    weights=[embeddings],       # Initialize with our pre-trained embeddings\n",
    "                                    mask_zero=True,             # Enable masking for padding (value 0)\n",
    "                                    trainable=False)            # Freeze the embedding weights\n",
    "\n",
    "    \n",
    "    # Pass the input sequence through the embedding layer\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    # Pass the embedded sequences through a Bidirectional LSTM layer\n",
    "    # return_sequences=True is essential for attention, as we need the output of every time step\n",
    "    bilstm_output = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='tanh', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "\n",
    "    # First, transform each BiLSTM hidden state into a new vector for calculating importance\n",
    "    attention_key_dim=300\n",
    "    # A Dense layer with 'tanh' activation is a common way to do this transformation\n",
    "    attention_input=Dense(attention_key_dim, activation='tanh')(bilstm_output)\n",
    "\n",
    "    # Next, pass the transformed inputs through our custom attention layer.\n",
    "    # This returns a normalized attention value a_i for each token i, where sum(a_i) = 1.\n",
    "    attention_output = AttentionLayerMasking(word_embedding_dim, name=\"attention\")(attention_input)\n",
    "    \n",
    "    # Now, multiply the attention weights by the original BiLSTM outputs to get a weighted average.\n",
    "    # The Lambda layer performs a batch dot product between attention weights and BiLSTM outputs.\n",
    "    document_representation = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=1), name='dot')([attention_output,bilstm_output])\n",
    "\n",
    "    # Pass the final document representation through a Dense layer for classification\n",
    "    # A single neuron with a 'sigmoid' activation is used for binary classification\n",
    "    x=Dense(1, activation=\"sigmoid\")(document_representation)\n",
    "\n",
    "    # Create the Keras Model, defining the input and output layers\n",
    "    model = Model(inputs=word_sequence_input, outputs=x)\n",
    "\n",
    "    # Compile the model with loss function, optimizer, and evaluation metrics\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Instantiate and Summarize the Model\n",
    "This cell calls the builder function to create an instance of the model and then prints a summary of its architecture. The summary is useful for verifying the layers, their output shapes, and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using the defined function with an LSTM size of 25 and dropout of 0.25\n",
    "bilstm_attention_model=get_bilstm_with_attention_masking(embeddings, lstm_size=25, dropout_rate=0.25)\n",
    "# Print a summary of the model's architecture\n",
    "print (bilstm_attention_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Train the Model\n",
    "This is where the model training happens. The `fit` method trains the model on the training data (`trainX`, `Y_train`) and validates it on the development set (`devX`, `Y_dev`). The `ModelCheckpoint` callback saves the model weights to a file whenever the validation loss improves, ensuring we keep the best version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the created model to a new variable for clarity\n",
    "model=bilstm_attention_model\n",
    "\n",
    "# Define the filename for saving the best model\n",
    "modelName=\"bilstm_attention_model.hdf5\"\n",
    "# Create a ModelCheckpoint callback to monitor validation loss and save only the best model\n",
    "checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, Y_train, \n",
    "            # Provide the development set for validation after each epoch\n",
    "            validation_data=(devX, Y_dev),\n",
    "            # Set the number of training epochs\n",
    "            epochs=30, \n",
    "            # Set the batch size\n",
    "            batch_size=128,\n",
    "            # Pass in the checkpoint callback\n",
    "            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the accuracy of your model on the test data?  Report the accuracy score with 95% confidence intervals.  Feel free to use the dev data for model selection (e.g., to hyperparameter choices like the size of hidden LSTM state, etc.), but be careful not to use the test data for this.  See keras [model.predict](https://keras.io/models/model/#predict) to generate predictions for a trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Confidence Interval Function\n",
    "This helper function calculates the accuracy of the model's predictions and computes a confidence interval for that accuracy score. This gives a more reliable estimate of the model's performance by providing a range in which the true accuracy likely lies. It uses the normal approximation to the binomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate binomial confidence intervals for accuracy\n",
    "def binomial_confidence_intervals(predictions, truth, confidence_level=0.95):\n",
    "    # Create a list to store whether each prediction was correct (1) or not (0)\n",
    "    correct=[]\n",
    "    # Iterate over the predictions and the true labels simultaneously\n",
    "    for pred, gold in zip(predictions, truth):\n",
    "        # Append 1 if the prediction matches the true label, otherwise append 0\n",
    "        correct.append(int(pred==gold))\n",
    "        \n",
    "    # Calculate the success rate (accuracy) as the mean of the 'correct' list\n",
    "    success_rate=np.mean(correct)\n",
    "\n",
    "    # For a two-tailed test, find the area for each tail\n",
    "    critical_value=(1-confidence_level)/2\n",
    "    # Find the z-score (the number of standard deviations from the mean) for the critical value\n",
    "    # norm.ppf is the inverse of the cumulative distribution function\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    \n",
    "    # Calculate the standard error for a binomial proportion\n",
    "    # The variance of a binomial distribution is p*(1-p)\n",
    "    standard_error=sqrt((success_rate*(1-success_rate))/len(correct))\n",
    "\n",
    "    # Calculate the lower bound of the confidence interval\n",
    "    lower=success_rate-z_alpha*standard_error\n",
    "    # Calculate the upper bound of the confidence interval\n",
    "    upper=success_rate+z_alpha*standard_error\n",
    "    # Print the formatted results\n",
    "    print(\"%.3f, %s%% Confidence interval: [%.3f,%.3f]\" % (success_rate, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Load Best Model Weights\n",
    "Before evaluating on the test set, this cell loads the weights from the file saved by `ModelCheckpoint`. This ensures that we are using the model that performed best on the validation set during training, not necessarily the model from the final epoch, which might be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-assign the model object (this is good practice in notebooks to ensure the right object is used)\n",
    "model=bilstm_attention_model\n",
    "\n",
    "# Load the weights of the best model that were saved during training\n",
    "model.load_weights(\"bilstm_attention_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Make Predictions on Test Data\n",
    "The trained model is now used to make predictions on the unseen test data. The `predict` method outputs raw probabilities (from the sigmoid function), which are then converted into binary class labels (0 or 1) by applying a 0.5 decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to generate predictions on the test set\n",
    "predictions = model.predict(testX, batch_size=128)\n",
    "# Convert the output probabilities to binary predictions (True/False) using a 0.5 threshold\n",
    "binarized_predictions=predictions > .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Calculate and Report Final Accuracy\n",
    "This final code cell calls the `binomial_confidence_intervals` function to compute and print the model's accuracy on the test set, complete with the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the accuracy with a 95% confidence interval on the test set\n",
    "# Note: The original notebook had a typo 'binomial_confidence_interval'. Corrected to 'binomial_confidence_intervals'.\n",
    "binomial_confidence_intervals(binarized_predictions, Y_test, confidence_level=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Take the sentence \"I do not like this movie.\" How is representing this sentence by using attention over the individual word embeddings different from representing it with attention over the output of each time step in an bidirectional LSTM?  What information does the LSTM output encode that individual word embeddings don't have access to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3: Word embeddings encode information about the word *type* but not about its specific use in context; the output of an LSTM at time t encodes information about the context a word *token* was used in -- for a single forward LSTM, the context of the sequence from word 1 through word t; for a BiLSTM, the context of the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}