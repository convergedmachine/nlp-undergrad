{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "This notebook demonstrates how to build and train a Bidirectional LSTM (BiLSTM) model for sequence labeling, specifically for a Named Entity Recognition (NER) task. It uses the Twitter NER dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "# numpy is for numerical operations, especially with arrays.\n",
    "import numpy as np\n",
    "# Import various layers and model-building tools from Keras.\n",
    "from keras.layers import Dense, Input, Embedding, TimeDistributed, Layer, Multiply, Concatenate, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "# K is the Keras backend (in this case, TensorFlow), used for some low-level operations.\n",
    "from keras import backend as K\n",
    "# Import TensorFlow itself.\n",
    "import tensorflow as tf\n",
    "# Import Keras callbacks for saving the model, stopping training early, and custom actions.\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "# Import a utility to convert class vectors to binary class matrices (one-hot encoding).\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Function: Load Word Embeddings\n",
    "This function loads pre-trained word embeddings from a file (like GloVe). It creates an embedding matrix and a vocabulary dictionary that maps words to integer IDs. Two special tokens are reserved: `_0_` for padding (index 0) and `_UNK_` for unknown words (index 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "    \"\"\" Load pre-trained word embeddings, reserving 0 for padding symbol and 1 for UNK \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to store the vocabulary (word -> id).\n",
    "    vocab={}\n",
    "    # Initialize a list to store the embedding vectors.\n",
    "    embeddings=[]\n",
    "    # Open and read the embeddings file.\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        # Read the first line to get the number of words and embedding dimension.\n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        # Append a vector of zeros for the padding token at index 0.\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Append a vector of zeros for the unknown (UNK) token at index 1.\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add the special tokens to our vocabulary.\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        # Iterate over each line in the embeddings file.\n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            # Stop if we have reached the maximum desired vocabulary size.\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            # Split the line into the word and its vector components.\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            # Convert the vector components to a numpy array of floats.\n",
    "            val=np.array(cols[1:])\n",
    "            # The first column is the word.\n",
    "            word=cols[0]\n",
    "            \n",
    "            # Add the word's vector to our embeddings list.\n",
    "            embeddings.append(val)\n",
    "            # Add the word to our vocabulary with its corresponding index (idx + 2 because of padding and UNK).\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    # Convert the list of embeddings to a single numpy matrix and return it with the vocabulary.\n",
    "    return np.array(embeddings), vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Function: Convert Sentences to Word IDs\n",
    "This function takes sentences (lists of word-tag pairs) and converts them into numerical format for the model. It pads all sequences to the same length (the length of the longest sentence in the dataset) so they can be processed in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(sentences, word_vocab, label_vocab):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to convert a list of sentences (where each sentence is a list of (word, tag) tuples)\n",
    "    into:\n",
    "    -- a list of padded sequences of word ids\n",
    "    -- a list of padded sequence of tag ids\n",
    "    -- a list of sequence lengths (the original token count for each sentence)\n",
    "    \n",
    "    Pads each sequence to the maximum sequence length observed in the sentences input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to hold the final processed data.\n",
    "    words_ids=[]\n",
    "    sent_lengths=[]\n",
    "    tags_ids=[]\n",
    "    \n",
    "    # The output dimension for tags is the number of unique tags + 1 (for the padding tag).\n",
    "    output_dim=len(label_vocab)+1\n",
    "    \n",
    "    # Find the length of the longest sentence to use for padding.\n",
    "    max_length=0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > max_length:\n",
    "            max_length=len(sentence)\n",
    "    \n",
    "    # Process each sentence.\n",
    "    for sentence in sentences:\n",
    "        # Initialize lists for the current sentence's word and tag IDs.\n",
    "        wids=[]\n",
    "        tids=[]\n",
    "        \n",
    "        # Iterate over each (word, tag) pair in the sentence.\n",
    "        for word, tag in sentence:\n",
    "            # Get the word's ID from the vocabulary. If not found, use the UNK ID (1).\n",
    "            val = word_vocab[word.lower()] if word.lower() in word_vocab else 1\n",
    "            # Append the word ID to the list.\n",
    "            wids.append(val)\n",
    "            # One-hot encode the tag ID.\n",
    "            y = to_categorical(label_vocab[tag], num_classes=output_dim)\n",
    "            # Append the one-hot encoded tag vector to the list.\n",
    "            tids.append(y)\n",
    "        \n",
    "        \n",
    "        # Pad the sequences with zeros up to the max_length.\n",
    "        for i in range(len(wids),max_length):\n",
    "            # Append the padding ID (0) for the word.\n",
    "            wids.append(0)\n",
    "            # Append a one-hot vector for the padding tag (class 0).\n",
    "            tids.append(to_categorical(0, num_classes=output_dim))\n",
    "            \n",
    "        # Add the padded word ID sequence to the main list.\n",
    "        words_ids.append(wids)\n",
    "        # Add the padded tag ID sequence to the main list.\n",
    "        tags_ids.append(tids)\n",
    "        # Store the original, unpadded length of the sentence.\n",
    "        sent_lengths.append(len(sentence))\n",
    " \n",
    "    # Convert lists to numpy arrays and return.\n",
    "    return np.array(words_ids), np.array(tags_ids), np.array(sent_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Function: Read TSV Data\n",
    "This function reads the dataset from a tab-separated value (TSV) file. The expected format is one token per line (`word\\ttag`), with sentences separated by blank lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(filename):\n",
    "    \n",
    "    \"\"\" Read input in two-column TSV, one line per word, with sentences delimited by a blank line \"\"\"\n",
    "    \n",
    "    # Initialize a list to hold all sentences.\n",
    "    sentences=[]\n",
    "    # Initialize a list to hold the current sentence being built.\n",
    "    sentence=[]\n",
    "    # Open and read the file.\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            # Split the line by tab.\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # If the line is blank (less than 2 columns), it marks the end of a sentence.\n",
    "            if len(cols) < 2:\n",
    "                # If the current sentence has words, add it to the list of all sentences.\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                # Reset the current sentence list for the next one.\n",
    "                sentence=[]\n",
    "                # Skip to the next line.\n",
    "                continue\n",
    "                \n",
    "            # The first column is the word.\n",
    "            word=cols[0]\n",
    "            # The second column is the NER tag.\n",
    "            tag=cols[1]\n",
    "            \n",
    "            # Append the (word, tag) tuple to the current sentence.\n",
    "            sentence.append((word, tag))\n",
    "            \n",
    "        # After the loop, add the last sentence if it exists.\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            \n",
    "    # Return the list of all sentences.\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Function: Create Tag Vocabulary\n",
    "This function builds a vocabulary that maps each unique NER tag found in the dataset to an integer ID. The ID assignment starts at 1, reserving 0 for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_vocab(sentences):\n",
    "    # Initialize a dictionary to store the tag vocabulary.\n",
    "    tags={}\n",
    "    # Start tag IDs from 1, as 0 is reserved for masking/padding.\n",
    "    tid=1\n",
    "    # Iterate through each sentence in the dataset.\n",
    "    for sentence in sentences:\n",
    "        # Iterate through each (word, tag) pair in the sentence.\n",
    "        for word, tag in sentence:\n",
    "            # If the tag is not yet in our vocabulary...\n",
    "            if tag not in tags:\n",
    "                # ...add it and assign it the current tag ID.\n",
    "                tags[tag]=tid\n",
    "                # Increment the ID for the next new tag.\n",
    "                tid+=1\n",
    "    # Return the completed tag vocabulary.\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Loading the Datasets\n",
    "Here, we use the `read_tsv` function to load the training and development (validation) data from their respective files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data from the TSV file.\n",
    "data=read_tsv(\"../data/twitter-ner/ner.train.txt\")\n",
    "# Read the development (validation) data from the TSV file.\n",
    "devData=read_tsv(\"../data/twitter-ner/ner.dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Building the Tag Vocabulary\n",
    "Now we create the tag-to-ID mapping from the training data. We also create a reverse mapping (ID-to-tag), which will be useful later for interpreting the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tag vocabulary from the training data.\n",
    "tag_vocab=get_tag_vocab(data)\n",
    "# Initialize a dictionary for the reverse mapping (ID -> tag).\n",
    "rev_tags={}\n",
    "# Populate the reverse mapping dictionary.\n",
    "for t in tag_vocab:\n",
    "    rev_tags[tag_vocab[t]]=t\n",
    "\n",
    "# Print the tag vocabulary to inspect it.\n",
    "print(tag_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Loading Word Embeddings\n",
    "This cell loads the pre-trained GloVe word embeddings for Twitter data. We limit the vocabulary size to 100,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to load embeddings and the corresponding word vocabulary.\n",
    "embeddings, word_vocab=load_embeddings(\"../data/glove.twitter.27B.100d.50K.txt.w2v\", 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Data Preprocessing\n",
    "This cell converts the raw text data (for both training and validation sets) into numerical format using the `get_word_ids` function defined earlier. The result is padded sequences of word IDs and one-hot encoded tag IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data into numerical format.\n",
    "trainX, trainY, trainS=get_word_ids(data, word_vocab, tag_vocab)\n",
    "# Convert the development (validation) data into numerical format.\n",
    "devX, devY, devS=get_word_ids(devData, word_vocab, tag_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Let's train a bidirectional LSTM for sequence labeling to make predictions about the NER tag for each word in a sentence.  Explore the effect of the lstm size and dropout rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Model Architecture Definition\n",
    "This function defines the Keras model architecture. It consists of:\n",
    "1.  **Input Layer**: Takes sequences of word IDs.\n",
    "2.  **Embedding Layer**: Converts word IDs into dense vectors using the pre-trained embeddings. `mask_zero=True` tells the model to ignore padded inputs.\n",
    "3.  **Bidirectional LSTM Layer**: Processes the sequence of embeddings in both forward and backward directions to capture context from both sides of each word.\n",
    "4.  **TimeDistributed Dense Layer**: Applies a fully connected layer to every timestep of the LSTM's output to produce a probability distribution over the possible NER tags for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bilstm(embeddings, output_dim, lstm_size=25, dropout_rate=0.25):\n",
    "    \n",
    "    # Get the vocabulary size and embedding dimension from the shape of the embeddings matrix.\n",
    "    vocab_size, word_embedding_dim=embeddings.shape\n",
    "\n",
    "    # Define the input layer for word ID sequences. `None` allows for variable sequence lengths.\n",
    "    word_sequence_input = Input(shape=(None,), dtype='int32')\n",
    "    # This input is for the original sentence lengths, but it's not used in this model version.\n",
    "    sentence_lengths = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "    # Define the embedding layer.\n",
    "    word_embedding_layer = Embedding(vocab_size,      # The number of words in our vocabulary.\n",
    "                                    word_embedding_dim, # The dimension of the word vectors.\n",
    "                                    weights=[embeddings], # Initialize with the pre-trained GloVe embeddings.\n",
    "                                    trainable=False, # Freeze the embedding weights; do not update them during training.\n",
    "                                    mask_zero=True) # Ignore padding (inputs of 0) in subsequent layers.\n",
    "\n",
    "    # Pass the input sequences through the embedding layer.\n",
    "    embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    # The Bidirectional LSTM layer. `return_sequences=True` makes it output a sequence, not just the final state.\n",
    "    bi_lstm = Bidirectional(LSTM(lstm_size, return_sequences=True, activation='relu', dropout=dropout_rate), merge_mode='concat')(embedded_sequences)\n",
    "    # The output layer. `TimeDistributed` applies the Dense layer to each time step (each word) of the sequence.\n",
    "    preds = TimeDistributed(Dense(output_dim, activation=\"softmax\"))(bi_lstm)\n",
    "\n",
    "    # Create the Keras Model, defining the inputs and outputs.\n",
    "    model = Model(inputs=[word_sequence_input, sentence_lengths], outputs=preds)\n",
    "\n",
    "    # Compile the model with a loss function, optimizer, and evaluation metric.\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "\n",
    "    # Return the compiled model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Training Function\n",
    "This function handles the model training process. It defines two callbacks:\n",
    "1.  **EarlyStopping**: Stops training if the validation loss doesn't improve for a set number of epochs (`patience=10`), preventing overfitting.\n",
    "2.  **ModelCheckpoint**: Saves the model weights only when the validation loss improves, ensuring we keep the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, modelName):\n",
    "    # Print a summary of the model's architecture and parameters.\n",
    "    print (model.summary())\n",
    "\n",
    "    # Define the EarlyStopping callback.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', # Monitor the validation loss.\n",
    "                                  min_delta=0,        # Minimum change to qualify as an improvement.\n",
    "                                  patience=10,        # Number of epochs with no improvement after which training will be stopped.\n",
    "                                  verbose=0,          # Suppress verbose output.\n",
    "                                  mode='auto')        # Automatically infer the direction of improvement (min for loss).\n",
    "\n",
    "    # Define the ModelCheckpoint callback.\n",
    "    checkpoint = ModelCheckpoint(modelName,             # Filepath to save the model.\n",
    "                               monitor='val_loss',    # Monitor the validation loss.\n",
    "                               verbose=0,             # Suppress verbose output.\n",
    "                               save_best_only=True, # Only save the model if `val_loss` has improved.\n",
    "                               mode='min')           # The monitored quantity should be minimized.\n",
    "    \n",
    "    # Start the training process.\n",
    "    model.fit([trainX, trainS], trainY,              # Training data (inputs and labels).\n",
    "            validation_data=([devX, devS], devY),  # Validation data.\n",
    "            epochs=30, batch_size=32,              # Number of epochs and batch size.\n",
    "            callbacks=[checkpoint, early_stopping])# List of callbacks to use during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Let's train a model on the data and save the one that performs best on the validation data in `bilstm_sequence_labeling.hdf5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Initial Model Training\n",
    "This cell creates an instance of the BiLSTM model and trains it using the `train` function. The best model weights will be saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BiLSTM model with an output dimension equal to the number of tags + 1 (for padding).\n",
    "model=create_bilstm(embeddings, len(tag_vocab)+1)\n",
    "# Train the model and save the best version to 'bilstm_sequence_labeling.hdf5'.\n",
    "train(model, \"bilstm_sequence_labeling.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "We can explore the performance of the model by predicting the NER tags for a new sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Loading the Best Model\n",
    "To make predictions, we first instantiate the model architecture again and then load the best weights that were saved during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the model architecture.\n",
    "model=create_bilstm(embeddings, len(tag_vocab)+1)\n",
    "# Load the saved weights from the file.\n",
    "model.load_weights(\"bilstm_sequence_labeling.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Prediction Function\n",
    "This function takes a raw text sentence, preprocesses it into the numerical format the model expects, runs the prediction, and then decodes the numerical output back into human-readable NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, rev_tags):\n",
    "    # Split the input string into a list of words.\n",
    "    text=text.split(\" \")\n",
    "    # Initialize a list to hold the word IDs.\n",
    "    wids=[]\n",
    "    # Convert each word to its ID.\n",
    "    for t in text:\n",
    "        if t.lower() in word_vocab:\n",
    "            # If the word is in our vocabulary, get its ID.\n",
    "            wids.append(word_vocab[t.lower()])\n",
    "        else:\n",
    "            # Otherwise, use the padding ID (0) which the model will ignore thanks to masking.\n",
    "            wids.append(0)\n",
    "\n",
    "    # Convert the list of IDs to a numpy array.\n",
    "    wids=np.array(wids)\n",
    "    # Get the length of the sequence.\n",
    "    lengths=np.array([len(wids)])\n",
    "\n",
    "    # The model expects batched input, so we wrap the arrays in another list/dimension.\n",
    "    preds=model.predict([[wids], [lengths]])\n",
    "    # For each word, find the tag with the highest probability using argmax.\n",
    "    y_classes = preds.argmax(axis=-1)\n",
    "\n",
    "    # Convert the predicted tag IDs back to tag strings using the reverse vocabulary.\n",
    "    predicted=[rev_tags[t] for t in y_classes[0]]\n",
    "    # Print each word and its predicted tag.\n",
    "    for w, t in zip(text, predicted):\n",
    "        print(\"%s\\t%s\" % (w,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Making a Prediction on New Text\n",
    "Here we test the trained model on a new sentence to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new sentence to test.\n",
    "text=\"Bill Gates is the founder of Microsoft\"\n",
    "# Call the predict function to get the NER tags.\n",
    "predict(text, model, rev_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Q1: You'll notice above that the model gets a token-level validation accuracy around 95 simply due to the high presence of the majority class (\"O\").  That's not a very helpful metric  in this case. Implement F-score for NER.  Remember, the F-score for NER is based on *chunks*; for more, see section 11.3.2 in: of SLP3 [chapter 11](https://web.stanford.edu/~jurafsky/slp3/11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### F1 Score Calculation\n",
    "This function implements precision, recall, and F1-score for the NER task. Unlike simple accuracy, this metric evaluates performance based on correctly identified \"chunks\" of named entities (e.g., correctly identifying \"Bill Gates\" as a single \"PER\" entity). This is a much more meaningful metric for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateF1(gold_sequences, predicted_sequences):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate the precision, recall and F-score over labeled chunks in the gold and predicted\n",
    "    input sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inner helper function to extract entity chunks from a sequence of tags.\n",
    "    def get_entities(sequences):\n",
    "        \n",
    "        # A list to store all found entities as (sentence_id, start_word, end_word, category).\n",
    "        ents=[]\n",
    "\n",
    "        # Iterate over each sentence's tag sequence.\n",
    "        for s_idx in range(len(sequences)):\n",
    "            \n",
    "            sent=sequences[s_idx]\n",
    "            \n",
    "            # Variables to track the start of a potential entity.\n",
    "            start=None\n",
    "            startCat=None\n",
    "\n",
    "            # Iterate over each word's tag in the sentence.\n",
    "            for w_idx in range(len(sent)):\n",
    "                tag=sent[w_idx]\n",
    "                # Split the tag (e.g., \"B-PER\") into its BIO part and category part.\n",
    "                parts=tag.split(\"-\")\n",
    "                BIO=\"O\" # Default to \"O\" (Outside).\n",
    "                if len(parts) == 2:\n",
    "                    BIO=parts[0]\n",
    "                    cat=parts[1]\n",
    "\n",
    "                # If we see a \"B\" tag or an \"O\" tag, the previous entity (if any) has ended.\n",
    "                if BIO == \"B\" or BIO == \"O\":\n",
    "                    if start != None:\n",
    "                        # The entity ended at the previous word.\n",
    "                        end=w_idx-1\n",
    "                        # Add the completed entity to our list.\n",
    "                        ents.append((s_idx, start, end, startCat))\n",
    "                        # Reset the start trackers.\n",
    "                        start=None\n",
    "                        startCat=None\n",
    "                        end=None\n",
    "\n",
    "                # If we see a \"B\" (Begin) tag, a new entity has started.\n",
    "                if BIO == \"B\":\n",
    "                    start=w_idx\n",
    "                    startCat=cat\n",
    "\n",
    "            # After the loop, check if the sentence ended mid-entity.\n",
    "            if start != None:\n",
    "                ents.append((s_idx, start, len(sent)-1, startCat))\n",
    "\n",
    "        # Return the list of all extracted entities.\n",
    "        return ents\n",
    "        \n",
    "    # Extract entities from the ground truth (gold) and predicted sequences.\n",
    "    gold_ents=get_entities(gold_sequences)\n",
    "    pred_ents=get_entities(predicted_sequences)\n",
    "\n",
    "    # Convert the lists of entities to sets for efficient intersection calculation.\n",
    "    g_set=set(gold_ents)\n",
    "    p_set=set(pred_ents)\n",
    "    \n",
    "    # Calculate precision: (number of correctly predicted entities) / (total number of predicted entities).\n",
    "    precision=0\n",
    "    if len(p_set) > 0:\n",
    "        precision=float(len(g_set.intersection(p_set)))/len(p_set)\n",
    "    # Calculate recall: (number of correctly predicted entities) / (total number of actual entities).\n",
    "    recall=0\n",
    "    if len(g_set) > 0:\n",
    "        recall=float(len(g_set.intersection(p_set)))/len(g_set)\n",
    "    \n",
    "    # Calculate F1 score: the harmonic mean of precision and recall.\n",
    "    F1=0\n",
    "    if precision + recall > 0:\n",
    "        F1=2*precision*recall/(precision+recall)\n",
    "\n",
    "    # Return the three metrics.\n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### F1 Score Example\n",
    "This cell runs a small example to test the `calculateF1` function and verify its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from class on 4/4 to test the F1 calculation.\n",
    "# Gold standard has two entities: (B-PER, I-PER) and (B-ORG).\n",
    "gold_sequences=[[\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\"], [\"O\", \"O\", \"O\"]]\n",
    "# Prediction incorrectly splits the first entity and correctly finds the second.\n",
    "predicted_sequences=[[\"B-PER\", \"O\", \"O\", \"O\", \"B-PER\", \"O\", \"B-ORG\"], [\"O\", \"O\", \"O\"]]\n",
    "\n",
    "# Calculate the metrics.\n",
    "precision, recall, F1=calculateF1(gold_sequences, predicted_sequences)\n",
    "# Print the results.\n",
    "print(\"P: %.3f, R: %.3f, F: %.3f\" % (precision, recall, F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Keras by default calculates metrics like accuracy at the batch level (averaging the metric across batches).  F-score, however, is a metric properly calculated over an entire dataset; we can incorporate that into learning by defining a callback function that prints out the validation F-score at the end of each epoch.  Once you've implemented `calculateF1` above, execute the following cells to see the validation F-score while training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Custom Keras Callback for F1 Score\n",
    "This class defines a custom Keras callback. The `on_epoch_end` method is automatically called by Keras at the end of each training epoch. Inside this method, we run predictions on the entire validation set, calculate the chunk-based F1 score, and print it. This gives us a much more accurate view of the model's performance as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F_score(Callback):\n",
    "    \n",
    "    # The constructor saves the reverse tag vocabulary for later use.\n",
    "    def __init__(self, reverse_tag_vocab):\n",
    "        self.reverse_tag_vocab=reverse_tag_vocab\n",
    "        \n",
    "    # This function is executed at the end of each epoch.\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Get the validation data from the model.\n",
    "        valX=self.validation_data[0] # Word ID sequences\n",
    "        valS=self.validation_data[1] # Sentence lengths\n",
    "        valY=self.validation_data[2] # True tag labels (one-hot)\n",
    "        \n",
    "        # Make predictions on the validation data.\n",
    "        predictions=self.model.predict([valX, valS])\n",
    "        # Convert predictions from probabilities to class IDs (the index of the max value).\n",
    "        y_classes = predictions.argmax(axis=-1)\n",
    "        # Convert true labels from one-hot vectors to class IDs.\n",
    "        truth = valY.argmax(axis=-1)\n",
    "\n",
    "        # Initialize lists to hold the decoded tag sequences.\n",
    "        preds=[]\n",
    "        golds=[]\n",
    "\n",
    "        # Get the shape of the predicted classes matrix (num_sentences, max_length).\n",
    "        s,w=y_classes.shape\n",
    "        # Iterate over each sentence in the validation set.\n",
    "        for i in range(s):\n",
    "            # Lists for the current sentence's tags.\n",
    "            sent_preds=[]\n",
    "            sent_golds=[]\n",
    "            # Iterate up to the original length of the sentence to ignore padding.\n",
    "            for j in range(int(valS[i])):\n",
    "                # Decode the true tag ID to its string representation.\n",
    "                sent_golds.append(self.reverse_tag_vocab[truth[i,j]])\n",
    "                # Decode the predicted tag ID to its string representation.\n",
    "                sent_preds.append(self.reverse_tag_vocab[y_classes[i,j]])\n",
    "            # Add the decoded sequences to the main lists.\n",
    "            preds.append(sent_preds)\n",
    "            golds.append(sent_golds)\n",
    "        \n",
    "        # Calculate Precision, Recall, and F1 over the entire validation set.\n",
    "        precision, recall, F1=calculateF1(golds, preds)\n",
    "        # Print the results for this epoch.\n",
    "        print(\"P: %.3f, R: %.3f, F: %.3f\" % (precision, recall, F1))\n",
    "    \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Updated Training Function with F1 Callback\n",
    "This is a revised version of the `train` function. It now creates an instance of our custom `F_score` callback and includes it in the list of callbacks passed to `model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, modelName):\n",
    "    # Print the model summary.\n",
    "    print (model.summary())\n",
    "\n",
    "    # Define the EarlyStopping callback.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                  min_delta=0,\n",
    "                                  patience=10,\n",
    "                                  verbose=0, \n",
    "                                  mode='auto')\n",
    "\n",
    "    # Create an instance of our custom F_score callback.\n",
    "    f_score=F_score(rev_tags)\n",
    "    # Define the ModelCheckpoint callback.\n",
    "    checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    \n",
    "    # Start the training process.\n",
    "    model.fit([trainX, trainS], trainY, \n",
    "            validation_data=([devX, devS], devY),\n",
    "            epochs=30, batch_size=32,\n",
    "            # Pass the list of callbacks, now including our f_score calculator.\n",
    "            callbacks=[f_score, checkpoint, early_stopping])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Training with F1 Score Monitoring\n",
    "Finally, we create a new model and train it using the updated `train` function. Now, at the end of each epoch, we will see the precision, recall, and F1 score on the validation set, providing a much clearer picture of how well the model is learning the NER task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BiLSTM model.\n",
    "model=create_bilstm(embeddings, len(tag_vocab)+1)\n",
    "# Train the model, this time with F1 score reporting at each epoch.\n",
    "train(model, \"bilstm_sequence_labeling.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
