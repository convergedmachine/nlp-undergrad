{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1\n",
    "import spacy, glob, os # Import the spacy library for NLP, glob for finding files, and os for interacting with the operating system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Imports\n",
    "This cell imports the necessary Python libraries for the task.\n",
    "* **spaCy**: A powerful library for Natural Language Processing (NLP).\n",
    "* **glob**: Used to find all pathnames matching a specified pattern.\n",
    "* **os**: Provides a way of using operating system-dependent functionality, like joining file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner','parser']) # Load the small English language model from spaCy, disabling the NER and parser components for efficiency.\n",
    "# The following lines are redundant if 'disable' is used during loading, but explicitly remove the pipes.\n",
    "nlp.remove_pipe('ner') # Remove the 'ner' (Named Entity Recognition) pipeline component.\n",
    "nlp.remove_pipe('parser') # Remove the 'parser' (Dependency Parser) pipeline component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Initializing the spaCy Model\n",
    "This block sets up the spaCy language model. We load the small English model (`en_core_web_sm`). For this specific task of Part-of-Speech (POS) tagging, we don't need the **Named Entity Recognizer (`ner`)** or the **Dependency Parser (`parser`)**. Disabling them makes the model load and run faster because it doesn't have to perform unnecessary analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3\n",
    "def get_spacy_tags(text): # Define a function that takes a text string as input.\n",
    "    doc=nlp(text) # Process the input text with the spaCy nlp object to create a doc object.\n",
    "    for word in doc: # Iterate through each token (word) in the processed doc.\n",
    "        print(word.text, word.tag_) # Print the word itself and its fine-grained POS tag.\n",
    "\n",
    "get_spacy_tags(\"Time flies like an arrow\") # Call the function with an example sentence to see the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Function to Get POS Tags\n",
    "The `get_spacy_tags` function is a simple utility to demonstrate spaCy's POS tagging capability. It takes a sentence, processes it, and then iterates through each word to print the word and its corresponding tag from the Penn Treebank tag set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4\n",
    "def read_docs(inputDir, maxDocs=100): # Define a function to read files from a directory.\n",
    "    \"\"\" Read in movie documents (all ending in .txt) from an input folder\n",
    "    and process with spacy \"\"\"\n",
    "    \n",
    "    docs=[] # Initialize an empty list to store the processed documents.\n",
    "    # Use glob to find all files ending in .txt within the specified directory.\n",
    "    for idx, filename in enumerate(glob.glob(os.path.join(inputDir, '*.txt'))):\n",
    "        with open(filename, encoding='utf-8') as file: # Open each file found (using utf-8 encoding for compatibility).\n",
    "            docs.append((filename, nlp(file.read()))) # Read the file's content, process it with spaCy, and append a (filename, doc) tuple to our list.\n",
    "        if idx >= maxDocs -1: # Check if the maximum number of documents has been reached (adjusting for zero-based index).\n",
    "            break # Exit the loop if the maxDocs limit is met.\n",
    "    return docs # Return the list of processed documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Function to Read and Process Documents\n",
    "This function, `read_docs`, is designed to handle a corpus of text files. It reads all `.txt` files from a given input directory, processes the content of each file with spaCy, and stores the resulting `doc` objects in a list. The `maxDocs` parameter is included to limit the number of files processed, which is useful for quick tests on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5\n",
    "# directory with 2000 movies summaries from Wikipedia\n",
    "inputDir=\"../data/movie_summaries/\" # Define the path to the directory containing the movie summary text files.\n",
    "docs=read_docs(inputDir, maxDocs=100) # Call the read_docs function to load and process 100 movie summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Loading the Movie Summary Corpus\n",
    "This cell defines the location of the dataset and then calls the `read_docs` function to load 100 summaries from that location. The resulting `docs` variable will hold the data that will be analyzed in the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Penn Treebank POS Tags\n",
    "Here are the 45 tags used by the Penn Treebank:\n",
    "\n",
    "|tag|meaning|\n",
    "|---|---|\n",
    "|CC|Coordinating conjunction|\n",
    "|CD|Cardinal number|\n",
    "|DT|Determiner|\n",
    "|EX|Existential there|\n",
    "|FW|Foreign word|\n",
    "|IN|Preposition or subordinating conjunction|\n",
    "|JJ|Adjective|\n",
    "|JJR|Adjective, comparative|\n",
    "|JJS|Adjective, superlative|\n",
    "|LS|List item marker|\n",
    "|MD|Modal|\n",
    "|NN|Noun, singular or mass|\n",
    "|NNS|Noun, plural|\n",
    "|NNP|Proper noun, singular|\n",
    "|NNPS|Proper noun, plural|\n",
    "|PDT|Predeterminer|\n",
    "|POS|Possessive ending|\n",
    "|PRP|Personal pronoun|\n",
    "|PRP\\$|Possessive pronoun|\n",
    "|RB|Adverb|\n",
    "|RBR|Adverb, comparative|\n",
    "|RBS|Adverb, superlative|\n",
    "|RP|Particle|\n",
    "|SYM|Symbol|\n",
    "|TO|to|\n",
    "|UH|Interjection|\n",
    "|VB|Verb, base form|\n",
    "|VBD|Verb, past tense|\n",
    "|VBG|Verb, gerund or present participle|\n",
    "|VBN|Verb, past participle|\n",
    "|VBP|Verb, non-3rd person singular present|\n",
    "|VBZ|Verb, 3rd person singular present|\n",
    "|WDT|Wh-determiner|\n",
    "|WP|Wh-pronoun|\n",
    "|WP\\$|Possessive wh-pronoun|\n",
    "|WRB|Wh-adverb|\n",
    "|.|period|\n",
    "|,|comma|\n",
    "|:|colon|\n",
    "|(|left separator|\n",
    "|)|right separator|\n",
    "|$|dollar sign|\n",
    "|\\`\\`|open double quotes|\n",
    "|''|close double quotes|\n",
    "\n",
    "Explore these tags below by searching for sentences in the (automatically tagged) movie summary corpus that have been tagged for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 7\n",
    "def find_examples(docs, tag, num_examples=10): # Define a function to find examples of a specific tag in the corpus.\n",
    "    window=5 # Set a 'window' of 5 words to show on each side of the target word.\n",
    "    count=0 # Initialize a counter for the number of examples found.\n",
    "    for _, doc in docs: # Iterate through each doc object in our list of documents.\n",
    "        for idx, token in enumerate(doc[5:-5]): # Iterate through each token in the doc, avoiding the first and last 5 tokens to prevent index errors.\n",
    "            if token.tag_ == tag: # Check if the token's tag matches the tag we're looking for.\n",
    "                # Print the context window: 5 words before, the target word highlighted in red, and 5 words after.\n",
    "                print (' '.join([\"%s\" % context.text for context in doc[idx:idx+window]]), \"\\033[91m%s\\033[0m\" % doc[idx+window].text, ' '.join([\"%s\" % context.text for context in doc[idx+window+1:idx+window*2+1] ]))\n",
    "                # for windows users - you may want to use the following print statement\n",
    "                # to highlight the middle token in each sentence using #s\n",
    "                # print (' '.join([\"%s\" % context.text for context in doc[idx-window+5:idx+5 ]]), \"#%s#\" % doc[idx+5].text, ' '.join([\"%s\" % context.text for context in doc[idx+6:idx+window+6] ]))\n",
    "                count+=1 # Increment the counter.\n",
    "                if count >= num_examples: # Check if we have found the desired number of examples.\n",
    "                    return # Exit the function if we have enough examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Function to Find Tag Examples\n",
    "The `find_examples` function is a tool for exploring the corpus. It searches for a specific POS tag and prints any occurrences it finds, along with the surrounding words (the \"context window\"). This is a common technique in corpus linguistics called a **concordance**, and it helps you understand how words and tags are used in natural language. The example uses ANSI escape codes (`\\033[91m`) to print the target word in red in compatible terminals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 8\n",
    "find_examples(docs, \"VB\", num_examples=10) # Use the function to find 10 examples of the 'VB' (Verb, base form) tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Finding Examples of 'VB'\n",
    "This cell calls the `find_examples` function to search for and display 10 examples of the tag `VB` (Verb, base form) from the loaded movie summaries. This demonstrates how the function can be used to investigate the usage of different POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Questions about Tag Differences\n",
    "What's the difference between the following?\n",
    "\n",
    "* PRP and PRP$\n",
    "* NN and NNP\n",
    "* JJ and JJR\n",
    "* VBZ and VB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Manual Tagging Exercise\n",
    "Q2: Use the `find_examples` function to help understand the usage of each part-of-speech tag; work with a partner to manually tag the following four sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 11: Sentence 1\n",
    "1. \"Open the pod bay doors, Hal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: Sentence 2\n",
    "2. \"Frankly, my dear, I don't give a damn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 13: Sentence 3\n",
    "3. \"May the Force be with you\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 14: Sentence 4\n",
    "4. One morning I shot an elephant in my pajamas. How he got in my pajamas, I don't know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 15: Evaluation Question\n",
    "Q3. After tagging the sentences above by hand, run them through the spacy tagger; what's spacy's accuracy on these sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 16\n",
    "# This cell is intentionally left blank for the user to write code to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 16: User Workspace\n",
    "This empty code cell is provided for you to write and run your own code to answer the questions posed in the previous markdown cells. You can use it to run the `get_spacy_tags` function on the sentences and compare the results with your manual tagging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}