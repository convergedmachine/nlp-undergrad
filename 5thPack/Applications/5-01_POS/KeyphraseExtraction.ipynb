{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores part-of-speech tagging through its impact on keyword extraction. Keyphrase extraction is a task designed to select a small number of terms (or phrases) from a document that best represent its content.  Here we'll use a tf-idf metric for ranking terms in a document, and use POS information to filter those terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Importing Necessary Libraries**\n",
    "This cell imports all the libraries required for the notebook.\n",
    "* `spacy`: A powerful library for Natural Language Processing (NLP).\n",
    "* `glob`: Used to find all pathnames matching a specified pattern.\n",
    "* `os`: Provides a way of using operating system-dependent functionality, like path joining.\n",
    "* `operator`: Used here to easily sort items by a specific index.\n",
    "* `math`: Provides access to mathematical functions, specifically `log` for the IDF calculation.\n",
    "* `random`: Used to create a baseline function that selects random keywords.\n",
    "* `collections.Counter`: A specialized dictionary subclass for counting hashable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spacy library for Natural Language Processing\n",
    "import spacy, glob, os, operator, math, random\n",
    "# From the collections module, import the Counter class for counting frequencies\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Initializing the SpaCy Model**\n",
    "Here, we load the English language model from SpaCy. For efficiency, we disable the 'ner' (Named Entity Recognition) and 'parser' components, as we only need the tokenizer and the part-of-speech (POS) tagger for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained English model from spacy, disabling the 'ner' and 'parser' pipes for faster processing\n",
    "nlp = spacy.load('en', disable=['ner,parser'])\n",
    "# Explicitly remove the 'ner' pipe from the processing pipeline\n",
    "nlp.remove_pipe('ner')\n",
    "# Explicitly remove the 'parser' pipe from the processing pipeline\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you get a word and its POS tag from SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Demonstrating SpaCy's POS Tagging**\n",
    "This cell defines and calls a simple function to show how SpaCy processes a text and assigns a part-of-speech (POS) tag to each word (token). The tags provide grammatical information about each word (e.g., `NNP` for proper noun, `VBZ` for verb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get and print spacy tags for a given text\n",
    "def get_spacy_tags(text):\n",
    "    \"\"\" Get spacy tags for an input text \"\"\"\n",
    "    # Process the text with the nlp object to create a Doc object\n",
    "    doc=nlp(text)\n",
    "    # Iterate through each token (word) in the Doc object\n",
    "    for word in doc:\n",
    "        # Print the word's text and its fine-grained POS tag\n",
    "        print(word.text, word.tag_)\n",
    "\n",
    "# Call the function with an example sentence\n",
    "get_spacy_tags(\"Time flies like an arrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Reading and Processing Text Documents**\n",
    "This function, `read_docs`, is designed to read all `.txt` files from a specified directory. For each file, it reads the content and processes it with the SpaCy `nlp` object. It returns a list of tuples, where each tuple contains the filename and the processed SpaCy `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read all .txt documents from an input directory\n",
    "def read_docs(inputDir):\n",
    "    \"\"\" Read in movie documents (all ending in .txt) from an input folder\"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the documents\n",
    "    docs=[]\n",
    "    # Use glob to find all files ending with .txt in the specified directory\n",
    "    for filename in glob.glob(os.path.join(inputDir, '*.txt')):\n",
    "        # Open each file for reading\n",
    "        with open(filename) as file:\n",
    "            # Append a tuple of (filename, processed_spacy_doc) to the list\n",
    "            docs.append((filename, nlp(file.read())))\n",
    "    # Return the list of documents\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Loading the Movie Summaries Dataset**\n",
    "This cell specifies the directory containing the dataset of 2000 movie summaries from Wikipedia and then calls the `read_docs` function to load them into memory. The result is stored in the `original_docs` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path containing the movie summary text files\n",
    "# directory with 2000 movie summaries from Wikipedia\n",
    "inputDir=\"../data/movie_summaries/\"\n",
    "# Call the read_docs function to load and process the documents from the directory\n",
    "original_docs=read_docs(inputDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. We covered tf-idf in lecture 9 (\"lexical semantics\") and in the `7.embeddings/TFIDF.ipynb` notebook. Write a method for extracting the 10 terms with highest tf-idf score for each document in a collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Baseline Function: Extracting Random Keywords**\n",
    "This function provides a simple baseline for keyword extraction. Instead of using a sophisticated metric, it just selects 10 unique words at random from each document. This helps to illustrate the expected output format for the subsequent functions: a dictionary mapping each filename to a list of 10 keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns 10 random words from each document\n",
    "def random_words(docs):\n",
    "    \"\"\" Function to return random 10 terms from doc.\n",
    "    \n",
    "    Input: a list of (filename, [spacy tokens]) documents\n",
    "    Returns: a dict mapping \"filename\" -> [list of 10 keyphrases, ranked from highest tf-idf score to lowest]\n",
    " \n",
    "    Used just to illustrate expected output of functions below \"\"\"\n",
    "    \n",
    "    # Initialize an empty dictionary to store the keyphrases for each file\n",
    "    keyphrases={}\n",
    "    \n",
    "    # Iterate through each document tuple (filename, doc)\n",
    "    for filename, doc in docs:\n",
    "        # Create a list of unique word texts from the document\n",
    "        tokens=list(set([x.text for x in doc]))\n",
    "        # Shuffle the list of unique tokens randomly\n",
    "        random.shuffle(tokens)\n",
    "  \n",
    "        # Assign the first 10 shuffled tokens as the keyphrases for the current filename\n",
    "        keyphrases[filename]=tokens[:10]\n",
    "    \n",
    "    # Return the dictionary of keyphrases\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7. Displaying Random Keywords for Sample Movies**\n",
    "This cell runs the `random_words` function on the dataset and then prints the randomly selected keywords for three specific movies to show the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random keywords for all original documents\n",
    "terms=random_words(original_docs)\n",
    "# Iterate through a list of specific movie filenames\n",
    "for filename in [\"Jaws.txt\", \"Harry_Potter_and_the_Philosophers_Stone.txt\", \"Back_to_the_Future.txt\"]:\n",
    "    # Print the filename\n",
    "    print(\"\\n%s\\n\" % filename)\n",
    "    # Print the randomly selected keywords for that file, each on a new line\n",
    "    print('\\n'.join(terms[os.path.join(inputDir, filename)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8. Answering Q1: Implementing TF-IDF for Keyword Extraction**\n",
    "This is the main function for Question 1. It calculates the TF-IDF (Term Frequency-Inverse Document Frequency) score for every word in every document and returns the top 10 words with the highest scores for each document.\n",
    "\n",
    "* **`get_tf(tokens)`**: A nested helper function that calculates **Term Frequency** (how often a word appears in a single document) using `Counter`.\n",
    "* **`get_idfs(docs)`**: A nested helper function that calculates **Inverse Document Frequency** for every word in the entire collection. IDF measures how important a word is by giving a higher score to words that appear in fewer documents. The formula used is $IDF(t) = \\log(\\frac{\\text{total number of documents}}{\\text{number of documents with term t}})$.\n",
    "* The main body of the function first calculates IDFs for the whole corpus. Then, for each document, it calculates TFs, combines them with the IDFs to get the final TF-IDF score for each term, sorts the terms by this score in descending order, and selects the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to rank terms using TF-IDF\n",
    "def tf_idf_ranking(docs):\n",
    "    \"\"\"\n",
    "    Function to rank terms in document by tf-idf score, and return the top 10 terms\n",
    "    \n",
    "    Input: a list of (filename, [spacy tokens]) documents\n",
    "    Returns: a dict mapping \"filename\" -> [list of 10 keyphrases, ranked from highest tf-idf score to lowest]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a helper function to calculate Term Frequency (TF) for a document\n",
    "    def get_tf(tokens):\n",
    "        # Initialize a Counter object to store word counts\n",
    "        counter=Counter()\n",
    "        # Iterate through each token in the document\n",
    "        for token in tokens:\n",
    "            # Increment the count for the token's text\n",
    "            counter[token.text]+=1\n",
    "        # Return the counter object\n",
    "        return counter\n",
    "    \n",
    "    # Define a helper function to calculate Inverse Document Frequency (IDF) for the entire corpus\n",
    "    def get_idfs(docs):\n",
    "        # Initialize a Counter to store document frequencies for each word\n",
    "        counts=Counter()\n",
    "        # Iterate through each document in the corpus\n",
    "        for _, doc in docs:\n",
    "            # Use a dictionary to store unique words in the current document to count each word only once\n",
    "            doc_types={}\n",
    "            for token in doc:\n",
    "                doc_types[token.text]=1\n",
    "\n",
    "            # For each unique word in the document, increment its document frequency count\n",
    "            for word in doc_types:\n",
    "                counts[word]+=1\n",
    "\n",
    "        # Initialize a dictionary to store the final IDF scores\n",
    "        idfs={}\n",
    "        # Iterate through each term in the document frequency counter\n",
    "        for term in counts:\n",
    "            # Calculate the IDF score using the log formula\n",
    "            idfs[term]=math.log(float(len(docs))/counts[term])\n",
    "\n",
    "        # Return the dictionary of IDF scores\n",
    "        return idfs\n",
    "\n",
    "    # Calculate the IDF scores for the entire collection of documents\n",
    "    idfs=get_idfs(docs)\n",
    "\n",
    "    # Initialize a dictionary to store the final keyphrases for each document\n",
    "    keyphrases={}\n",
    "    \n",
    "    # Iterate through each document (filename, doc)\n",
    "    for filename, doc in docs:\n",
    "        # Calculate the Term Frequency (TF) for the current document\n",
    "        tf=get_tf(doc)\n",
    "        # Initialize a dictionary to store candidate keyphrases and their TF-IDF scores\n",
    "        candidates={}\n",
    "        # For each term in the TF dictionary\n",
    "        for term in tf:\n",
    "            # Calculate the TF-IDF score and store it\n",
    "            candidates[term]=tf[term]*idfs[term]\n",
    "\n",
    "        # Sort the candidate terms by their TF-IDF score in descending order\n",
    "        sorted_x = sorted(candidates.items(), key=operator.itemgetter(1), reverse=True)\n",
    "       \n",
    "        # Store the top 10 terms (the words, not their scores) in the keyphrases dictionary\n",
    "        keyphrases[filename]=[k for k,v in sorted_x[:10]]\n",
    "    \n",
    "    # Return the final dictionary of top 10 keyphrases for each document\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9. Displaying Top 10 TF-IDF Keywords for Sample Movies**\n",
    "This cell runs the `tf_idf_ranking` function and prints the extracted keywords for the same three movies, allowing for a comparison with the random baseline. The results should be much more relevant to the movie plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 TF-IDF ranked keywords for all documents\n",
    "terms=tf_idf_ranking(original_docs)\n",
    "# Iterate through a list of specific movie filenames\n",
    "for filename in [\"Jaws.txt\", \"Harry_Potter_and_the_Philosophers_Stone.txt\", \"Back_to_the_Future.txt\"]:\n",
    "    # Print the filename\n",
    "    print(\"\\n%s\\n\" % filename)\n",
    "    # Print the top 10 keywords for that file, each on a new line\n",
    "    print('\\n'.join(terms[os.path.join(inputDir, filename)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.  Write a method for extracting the 10 terms with highest tf-idf score for each document in a collection that *excludes all proper names*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **10. Answering Q2: TF-IDF Keywords Excluding Proper Nouns**\n",
    "This function addresses Question 2 by first filtering out all proper nouns from the documents and then applying the same `tf_idf_ranking` function from Q1.\n",
    "\n",
    "* **`remove_proper_nouns(docs)`**: This helper function iterates through every token in every document. It creates new, filtered document lists that exclude any token tagged as a singular proper noun (`NNP`) or a plural proper noun (`NNPS`).\n",
    "* The main function simply calls this filtering function and passes the result to `tf_idf_ranking`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get keyphrases while excluding proper nouns\n",
    "def keyphrase_no_proper_nouns(docs):\n",
    "    \"\"\"\n",
    "    Function to rank terms in document by tf-idf score, and return the top 10 terms.  \n",
    "    Constraint: None of the top 10 terms should be proper nouns.\n",
    "    \n",
    "    Input: a list of (filename, [spacy tokens]) documents\n",
    "    Returns: a dict mapping \"filename\" -> [list of 10 keyphrases, ranked from highest tf-idf score to lowest]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a helper function to filter out proper nouns from the documents\n",
    "    def remove_proper_nouns(docs):\n",
    "        # Initialize a list to hold the new, filtered documents\n",
    "        new_docs=[]\n",
    "        # Iterate through each document\n",
    "        for filename, doc in docs:\n",
    "            # Initialize a list to hold the tokens for the new document\n",
    "            new_doc=[]\n",
    "            # Iterate through each token in the original document\n",
    "            for token in doc:\n",
    "                # Check if the token's tag is NOT a singular ('NNP') or plural ('NNPS') proper noun\n",
    "                if token.tag_ != \"NNP\" and token.tag_ != \"NNPS\":\n",
    "                    # If it's not a proper noun, add it to the new document list\n",
    "                    new_doc.append(token)\n",
    "            # Add the filtered document (as a tuple of filename and token list) to the new docs list\n",
    "            new_docs.append((filename, new_doc))\n",
    "       \n",
    "        # Return the collection of documents with proper nouns removed\n",
    "        return new_docs\n",
    "            \n",
    "    # Create the new set of documents by removing proper nouns\n",
    "    new_docs=remove_proper_nouns(docs)\n",
    "    # Run the original tf_idf_ranking function on the filtered documents\n",
    "    terms=tf_idf_ranking(new_docs)\n",
    "    # Return the resulting keywords\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **11. Displaying Keywords (No Proper Nouns) for Sample Movies**\n",
    "This cell executes the function from Q2. The resulting keywords will be high-importance terms but will not include character names, locations, or other proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 keywords, excluding proper nouns\n",
    "terms=keyphrase_no_proper_nouns(original_docs)\n",
    "# Iterate through a list of specific movie filenames\n",
    "for filename in [\"Jaws.txt\", \"Harry_Potter_and_the_Philosophers_Stone.txt\", \"Back_to_the_Future.txt\"]:\n",
    "    # Print the filename\n",
    "    print(\"\\n%s\\n\" % filename)\n",
    "    # Print the top 10 keywords for that file, each on a new line\n",
    "    print('\\n'.join(terms[os.path.join(inputDir, filename)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.  Write a method for extracting the 10 terms with highest tf-idf score for each document in a collection that *includes only common nouns*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **12. Answering Q3: TF-IDF Keywords Including Only Common Nouns**\n",
    "This function addresses Question 3 by filtering the documents to include *only* common nouns before applying the TF-IDF calculation. This is a more restrictive filter than the one in Q2.\n",
    "\n",
    "* **`remove_proper_nouns(docs)`**: Despite its name, this helper function is modified to keep only tokens that are tagged as a singular common noun (`NN`) or a plural common noun (`NNS`).\n",
    "* The main function uses this new filtered set of documents to find the most important nouns in each movie summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get keyphrases that are only common nouns\n",
    "def keyphrase_only_common_nouns(docs):\n",
    "    \"\"\"\n",
    "    Function to rank terms in document by tf-idf score, and return the top 10 terms.  \n",
    "    Constraint: All of the top 10 terms should be common nouns.\n",
    "    \n",
    "    Input: a list of (filename, [spacy tokens]) documents\n",
    "    Returns: a dict mapping \"filename\" -> [list of 10 keyphrases, ranked from highest tf-idf score to lowest]\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    # Define a helper function to filter documents to include ONLY common nouns\n",
    "    def remove_proper_nouns(docs):\n",
    "        # Initialize a list to hold the new, filtered documents\n",
    "        new_docs=[]\n",
    "        # Iterate through each document\n",
    "        for filename, doc in docs:\n",
    "            # Initialize a list to hold the tokens for the new document\n",
    "            new_doc=[]\n",
    "            # Iterate through each token in the original document\n",
    "            for token in doc:\n",
    "                # Check if the token's tag is a singular ('NN') or plural ('NNS') common noun\n",
    "                if token.tag_ == \"NN\" or token.tag_ == \"NNS\":\n",
    "                    # If it is a common noun, add it to the new document list\n",
    "                    new_doc.append(token)\n",
    "            # Add the filtered document (as a tuple of filename and token list) to the new docs list\n",
    "            new_docs.append((filename, new_doc))\n",
    "       \n",
    "        # Return the collection of documents containing only common nouns\n",
    "        return new_docs\n",
    "            \n",
    "    # Create the new set of documents containing only common nouns\n",
    "    new_docs=remove_proper_nouns(docs)\n",
    "    # Run the original tf_idf_ranking function on the filtered documents\n",
    "    terms=tf_idf_ranking(new_docs)\n",
    "    # Return the resulting keywords\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **13. Displaying Keywords (Only Common Nouns) for Sample Movies**\n",
    "This final execution cell runs the function from Q3. The output will consist exclusively of the top 10 most important common nouns for the selected movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 keywords, consisting of only common nouns\n",
    "terms=keyphrase_only_common_nouns(original_docs)\n",
    "# Iterate through a list of specific movie filenames\n",
    "for filename in [\"Jaws.txt\", \"Harry_Potter_and_the_Philosophers_Stone.txt\", \"Back_to_the_Future.txt\"]:\n",
    "    # Print the filename\n",
    "    print(\"\\n%s\\n\" % filename)\n",
    "    # Print the top 10 keywords for that file, each on a new line\n",
    "    print('\\n'.join(terms[os.path.join(inputDir, filename)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}