{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition: Tokenizing OCR'd Text\n",
    "\n",
    "This initial markdown cell sets up the problem. Texts scanned using Optical Character Recognition (OCR) often contain artifacts from the original printed format. A common issue is **line-break hyphenation**, where a word is split with a hyphen at the end of a line (e.g., `interest-` on one line and `ing` on the next). A standard tokenizer would incorrectly treat these as two separate tokens (`interest-` and `ing`) or a single incorrect token (`interest-ing`).\n",
    "\n",
    "The goal is to design a smarter tokenizer that can correctly join words split across lines (like `interesting`) while preserving legitimate hyphenated words (like `newly-formed`).\n",
    "\n",
    "```\n",
    "the inhabitants of the surrounding districts will, also, be thus\n",
    "prevented. Moritz Wagner has lately published an interest-\n",
    "ing essay on this subject, and has shown that the service\n",
    "rendered by isolation in preventing crosses between newly-\n",
    "formed varieties is probably greater even than I supposed.\n",
    "But from reasons already assigned I can by no means agree\n",
    "with this naturalist, that migration and isolation are neces-\n",
    "sary elements for the formation of new species. The im-\n",
    "portance of isolation is likewise great in preventing, after\n",
    "any physical change in the conditions such as of climate ele-\n",
    "vation of the land, &c., the immigration of better adapted or-\n",
    "ganisms; and thus new places in the natural economy of the\n",
    "district will be left open to be filled up by the modification of\n",
    "the old inhabitants. Lastly, isolation will give time for a\n",
    "new variety to be improved at a slow rate ; and this may\n",
    "```\n",
    "\n",
    "Here the printing convention of line-break hyphenization would, under a standard tokenizer, generate incorrect tokens like `interest-ing` (or perhaps `interest-` and `ing`).  Design a better tokenizer (even just using pre- and post-processing) for these texts.  Note here the correct tokenization of `interest-ing` is `interesting` but the correct tokenization for `newly-formed` is still `newly-formed`.\n",
    "\n",
    "For a more thorough library for handling OCR'd book data, see https://github.com/tedunderwood/DataMunging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Importing Libraries\n",
    "\n",
    "This cell imports the necessary Python libraries for the task.\n",
    "* `sys`: Provides access to system-specific parameters and functions.\n",
    "* `nltk`: The Natural Language Toolkit, a popular library for NLP tasks like tokenization.\n",
    "* `re`: The regular expression module, used for pattern matching and string manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys, nltk, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Defining a Text Reading Function\n",
    "\n",
    "This cell defines a helper function, `read_text`, to open and read a file. It iterates through the file line by line, removes any trailing whitespace (like newline characters `\\n`) from each line, and returns the content as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read a text file and return its lines.\n",
    "def read_text(filename):\n",
    "    # Initialize an empty list to store the lines from the file.\n",
    "    lines=[]\n",
    "    # Open the specified file for reading.\n",
    "    with open(filename) as file:\n",
    "        # Loop through each line in the file.\n",
    "        for line in file:\n",
    "            # Remove any trailing whitespace (like the newline character) and append to the list.\n",
    "            lines.append(line.rstrip())\n",
    "    # Return the list of cleaned lines.\n",
    "    return lines        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Specifying the Data File\n",
    "\n",
    "This cell defines a string variable `filename` that holds the path to the text file we want to process. This file contains the full text of Darwin's *Origin of Species*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variable 'filename' to the path of the input text file.\n",
    "filename=\"../data/darwin_origin_ia.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Reading the Text File\n",
    "\n",
    "Here, the `read_text` function defined earlier is called with the `filename` path. The entire content of the Darwin text is loaded into the `lines` variable as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the read_text function to load the content of the file into the 'lines' variable.\n",
    "lines=read_text(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Creating a Sample Text for Testing\n",
    "\n",
    "This cell creates a small, multi-line string variable named `testText`. This string contains the exact passage from Darwin's work shown in the initial problem description. Using this smaller sample makes it easier and faster to develop and test the de-hyphenation logic before running it on the entire book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multi-line string containing the sample text for testing the tokenizer.\n",
    "testText=\"\"\"the inhabitants of the surrounding districts will, also, be thus\n",
    "prevented. Moritz Wagner has lately published an interest-\n",
    "ing essay on this subject, and has shown that the service\n",
    "rendered by isolation in preventing crosses between newly-\n",
    "formed varieties is probably greater even than I supposed.\n",
    "But from reasons already assigned I can by no means agree\n",
    "with this naturalist, that migration and isolation are neces-\n",
    "sary elements for the formation of new species. The im-\n",
    "portance of isolation is likewise great in preventing, after\n",
    "any physical change in the conditions such as of climate ele-\n",
    "vation of the land, &c., the immigration of better adapted or-\n",
    "ganisms; and thus new places in the natural economy of the\n",
    "district will be left open to be filled up by the modification of\n",
    "the old inhabitants. Lastly, isolation will give time for a\n",
    "new variety to be improved at a slow rate ; and this may\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Building a Custom Vocabulary\n",
    "\n",
    "The core strategy for deciding whether to join a hyphenated word is to check if the combined word exists in a dictionary. This cell builds a custom vocabulary (`vocab`) for this purpose.\n",
    "\n",
    "1.  It starts by populating `vocab` with a standard English dictionary from `/usr/share/dict/words`.\n",
    "2.  It then enhances this dictionary by adding all the non-hyphenated words from the actual book text (`lines`). This helps account for specialized terms, names, or archaic words present in Darwin's writing that might not be in a standard modern dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The strategy is to check if a de-hyphenated word exists in a dictionary.\n",
    "# This cell builds that dictionary.\n",
    "\n",
    "# Initialize an empty dictionary to serve as our vocabulary.\n",
    "vocab={}\n",
    "\n",
    "# Open the system's built-in dictionary file.\n",
    "with open(\"/usr/share/dict/words\") as file:\n",
    "    # Iterate through each word (line) in the dictionary file.\n",
    "    for line in file:\n",
    "        # Add the word (in lowercase) to our vocabulary for fast lookups.\n",
    "        vocab[line.rstrip().lower()]=1\n",
    "        \n",
    "# Now, augment the vocabulary with words from the book itself.\n",
    "# This accounts for proper nouns, jargon, etc.\n",
    "for line in lines:\n",
    "    # Tokenize the current line into words using NLTK.\n",
    "    words=nltk.word_tokenize(line, language=\"english\")\n",
    "    # Iterate through each tokenized word.\n",
    "    for word in words:\n",
    "        # Check if the word is NOT a line-break hyphenation fragment.\n",
    "        if not word.endswith(\"-\"):\n",
    "            # Add the word (in lowercase) to our vocabulary.\n",
    "            vocab[word.lower()]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Implementing the De-hyphenation Logic\n",
    "\n",
    "This is the main cell where the custom tokenization logic is implemented. It processes the `testText` line by line, identifies words split by hyphens at line breaks, and intelligently joins them based on the `vocab` created in the previous step.\n",
    "\n",
    "The code iterates through each line, checks if it ends with a hyphenated word, and looks ahead to the next line. If combining the two word fragments creates a valid word found in `vocab`, it merges them. A flag (`previousLineHyphenMatch`) is used to ensure the second part of the merged word is not printed twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:\n",
      "\n",
      "the inhabitants of the surrounding districts will , also , be thus\n",
      "prevented . Moritz Wagner has lately published an interesting\n",
      "essay on this subject , and has shown that the service\n",
      "rendered by isolation in preventing crosses between newly-\n",
      "formed varieties is probably greater even than I supposed .\n",
      "But from reasons already assigned I can by no means agree\n",
      "with this naturalist , that migration and isolation are necessary\n",
      "elements for the formation of new species . The importance\n",
      "of isolation is likewise great in preventing , after\n",
      "any physical change in the conditions such as of climate elevation\n",
      "of the land , & c. , the immigration of better adapted organisms\n",
      "; and thus new places in the natural economy of the\n",
      "district will be left open to be filled up by the modification of\n",
      "the old inhabitants . Lastly , isolation will give time for a\n",
      "new variety to be improved at a slow rate ; and this may\n",
      "\n",
      "Original:\n",
      "\n",
      "the inhabitants of the surrounding districts will, also, be thus\n",
      "prevented. Moritz Wagner has lately published an interest-\n",
      "ing essay on this subject, and has shown that the service\n",
      "rendered by isolation in preventing crosses between newly-\n",
      "formed varieties is probably greater even than I supposed.\n",
      "But from reasons already assigned I can by no means agree\n",
      "with this naturalist, that migration and isolation are neces-\n",
      "sary elements for the formation of new species. The im-\n",
      "portance of isolation is likewise great in preventing, after\n",
      "any physical change in the conditions such as of climate ele-\n",
      "vation of the land, &c., the immigration of better adapted or-\n",
      "ganisms; and thus new places in the natural economy of the\n",
      "district will be left open to be filled up by the modification of\n",
      "the old inhabitants. Lastly, isolation will give time for a\n",
      "new variety to be improved at a slow rate ; and this may\n"
     ]
    }
   ],
   "source": [
    "# --- Tokenization and De-hyphenation ---\n",
    "\n",
    "# Split the test text into a list of individual lines.\n",
    "lines=testText.split(\"\\n\")\n",
    "# Initialize an empty list to store the tokenized version of each line.\n",
    "tokenized_lines=[]\n",
    "# Loop through each line of the raw text.\n",
    "for line in lines:\n",
    "    # Tokenize the line into words using NLTK's tokenizer.\n",
    "    tok_words=nltk.word_tokenize(line, language=\"english\")\n",
    "    # Append the list of tokenized words to our list of tokenized lines.\n",
    "    tokenized_lines.append(tok_words)\n",
    "    \n",
    "# Initialize an empty list to hold the final, corrected tokens.\n",
    "tokens=[]\n",
    "# A boolean flag to track if the previous line ended in a hyphen that was successfully merged.\n",
    "previousLineHyphenMatch=False\n",
    "\n",
    "# Loop through the tokenized lines using an index to allow lookups to the next line.\n",
    "for idx,words in enumerate(tokenized_lines):\n",
    "    # A flag to track if a merge occurs on the *current* line. Reset for each line.\n",
    "    flag=False\n",
    "    \n",
    "    # Check if the line is not empty, ends with a hyphenated word, and is not the last line of the text.\n",
    "    if len(words) > 0 and words[-1].endswith(\"-\") and idx < len(tokenized_lines)-1:\n",
    "        # Get the list of words from the next line.\n",
    "        nextwords=tokenized_lines[idx+1]\n",
    "        # Check if the next line is not empty.\n",
    "        if len(nextwords) > 0:\n",
    "            # Get the first word of the next line.\n",
    "            first=nextwords[0]\n",
    "            # Create a candidate word by removing the hyphen from the last word of the current line\n",
    "            # and concatenating it with the first word of the next line.\n",
    "            candidate=\"%s%s\" % (re.sub(\"-$\", \"\", words[-1]), first)\n",
    "            \n",
    "            # Check if the lowercase version of the candidate word exists in our vocabulary.\n",
    "            if candidate.lower() in vocab:\n",
    "                # If it exists, replace the hyphenated fragment with the complete, merged word.\n",
    "                words[-1]=candidate\n",
    "                \n",
    "                # Set the flag to True, indicating a successful merge occurred.\n",
    "                # This will be used to remove the first word from the *next* line.\n",
    "                flag=True\n",
    "           \n",
    "    # If the previous line was successfully merged, we need to skip the first word of the current line.\n",
    "    if previousLineHyphenMatch:\n",
    "        # Append all words from the current line *except the first one* to our final token list.\n",
    "        tokens.append(words[1:])\n",
    "    else:\n",
    "        # Otherwise, append all words from the current line as they are.\n",
    "        tokens.append(words)\n",
    "    \n",
    "    # Update the master flag for the next iteration. If a merge happened, the next loop needs to know.\n",
    "    previousLineHyphenMatch = True if flag else False\n",
    "\n",
    "# --- Display Results ---\n",
    "    \n",
    "# Print the final, corrected text.\n",
    "print(\"Tokenized:\\n\")\n",
    "for line in tokens:\n",
    "    print(' '.join(line))\n",
    "# Print the original text for comparison.\n",
    "print(\"\\nOriginal:\\n\")\n",
    "print(testText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (anlp)",
   "language": "python",
   "name": "anlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
