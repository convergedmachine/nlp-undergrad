{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook outlines several methods for tokenizing text into words (and sentences), including:\n",
    "\n",
    "* whitespace\n",
    "* nltk (Penn Treebank tokenizer)\n",
    "* nltk (Twitter-aware)\n",
    "* spaCy\n",
    "* custom regular expressions\n",
    "\n",
    "highlighting differences between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Setup: Importing Libraries\n",
    "\n",
    "First, we import the necessary Python libraries.\n",
    "* `nltk`: The Natural Language Toolkit, a popular library for NLP tasks.\n",
    "* `re`: Python's built-in module for regular expressions.\n",
    "* `json`: For parsing the JSON file containing the tweet data.\n",
    "* `spacy`: A modern and powerful NLP library.\n",
    "* `Counter`: A dictionary subclass from the `collections` module for counting hashable objects, which is perfect for tallying token frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk, re, json\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Downloading NLTK Models\n",
    "\n",
    "To perform sentence and word tokenization, NLTK relies on pre-trained models. Here, we download the `punkt` tokenizer models, which are used by NLTK's functions for splitting text into sentences (`sent_tokenize`) and words (`word_tokenize`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't downloaded the sentence segmentation model before, do so here\n",
    "# This command downloads the 'punkt' resource, which includes pre-trained models\n",
    "# for sentence tokenization for multiple languages.\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Downloading spaCy Models\n",
    "\n",
    "Similarly, spaCy uses statistical models to process text. We download `en_core_web_sm`, which is a small, efficient English language model that includes components for tokenization, part-of-speech tagging, named entity recognition, and more. The `!` allows us to run this command directly in the shell from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command executes a shell command to download the small English model for spaCy.\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Loading and Configuring the spaCy Model\n",
    "\n",
    "After downloading the model, we load it into our script using `spacy.load()`. For this notebook's purpose, we only need spaCy's tokenizer. To make the process more efficient, we disable other components of the NLP pipeline like the part-of-speech `tagger`, `ner` (Named Entity Recognizer), and `parser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English spaCy model, disabling unnecessary components for efficiency.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger,ner,parser'])\n",
    "\n",
    "# Explicitly remove the pipeline components to ensure they are not used.\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Data Loading: Reading Tweets from JSON\n",
    "\n",
    "This section defines a helper function to read our data. The function `read_tweets_from_json` opens a specified JSON file, parses its content, and extracts the text from each tweet object, returning a list of tweet strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read tweets from a JSON file.\n",
    "def read_tweets_from_json(filename):\n",
    "    # Initialize an empty list to store the tweet texts.\n",
    "    tweets=[]\n",
    "    # Open the specified file with UTF-8 encoding.\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # Load the entire JSON content from the file.\n",
    "        data=json.load(file)\n",
    "        # Iterate through each tweet object in the loaded data.\n",
    "        for tweet in data:\n",
    "            # Append the value of the \"text\" key to our list.\n",
    "            tweets.append(tweet[\"text\"])\n",
    "    # Return the list of all tweet texts.\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now, let's specify the path to our data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the path to the tweet data file in a variable.\n",
    "filename=\"../data/trump_tweets.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Using the function we just defined, we load the tweet texts from the JSON file into the `tweets` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to read the tweets and store them in the 'tweets' variable.\n",
    "tweets=read_tweets_from_json(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Method 1: Whitespace Tokenization\n",
    "\n",
    "This is the simplest tokenization method. We iterate through each tweet and use Python's built-in `split()` method, which splits a string by any whitespace (spaces, tabs, newlines) by default. The resulting lists of tokens are stored in `whitespace_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the tokenized tweets.\n",
    "whitespace_tokens=[]\n",
    "# Loop through each tweet in the 'tweets' list.\n",
    "for tweet in tweets:\n",
    "    # Split the tweet string by whitespace and append the resulting list of tokens.\n",
    "    whitespace_tokens.append(tweet.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Downloading Additional NLTK Data\n",
    "\n",
    "The `punkt_tab` resource is a version of the Punkt tokenizer data that is specifically trained to handle tab characters within text, although for standard text `punkt` is usually sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an alternative version of the 'punkt' tokenizer data.\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Method 2: NLTK's Penn Treebank Tokenizer\n",
    "\n",
    "Here, we use `nltk.word_tokenize()`. This tokenizer is more sophisticated than simple whitespace splitting. It's based on the Penn Treebank conventions and is better at handling punctuation, separating it from words (e.g., `\"don't\"` becomes `['do', 'n't']`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the tokenized tweets.\n",
    "nltk_tokens=[]\n",
    "# Loop through each tweet in the 'tweets' list.\n",
    "for tweet in tweets:\n",
    "    # Use NLTK's standard word tokenizer and append the result.\n",
    "    nltk_tokens.append(nltk.word_tokenize(tweet, language=\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Method 3: NLTK's Casual (Twitter-aware) Tokenizer\n",
    "\n",
    "The `nltk.casual_tokenize()` function is specifically designed for informal text like tweets. It's better at handling social media conventions like hashtags (`#`), mentions (`@`), and emoticons, often keeping them as single, intact tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the tokenized tweets.\n",
    "nltk_casual_tokens=[]\n",
    "# Loop through each tweet in the 'tweets' list.\n",
    "for tweet in tweets:\n",
    "    # Use NLTK's casual tokenizer designed for social media text.\n",
    "    nltk_casual_tokens.append(nltk.casual_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Method 4: spaCy's Tokenizer\n",
    "\n",
    "spaCy's tokenizer is highly advanced and part of a larger processing pipeline. It's language-specific and considers complex grammatical rules. We process each tweet with our loaded `nlp` object and extract the text of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the tokenized tweets.\n",
    "spacy_tokens=[]\n",
    "# Loop through each tweet in the 'tweets' list.\n",
    "for tweet in tweets:\n",
    "    # Process the tweet with the spaCy nlp object and create a list of the token texts.\n",
    "    spacy_tokens.append([token.text for token in nlp(tweet)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Method 5: Custom Extensible Regex Tokenizer\n",
    "\n",
    "For maximum control, we can define our own tokenizer using regular expressions. This code defines a sequence of regex patterns to capture different types of tokens in a specific order of priority (e.g., mentions first, then hashtags, then words with apostrophes, etc.). The `re.compile()` function creates a reusable regex object for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This regular expression pattern is adapted from Christopher Potts' sentiment tokenizing script.\n",
    "# It defines a tuple of regex patterns. The order is crucial as they are matched sequentially.\n",
    "regexes=(\n",
    "    # Pattern 1: Keep usernames/mentions together (e.g., @user_name).\n",
    "    r\"(?:@[\\w_]+)\",\n",
    "\n",
    "    # Pattern 2: Keep hashtags together (e.g., #topic).\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "    # Pattern 3: Keep words with internal apostrophes, hyphens, or underscores together (e.g., \"word-word\").\n",
    "    r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "    # Pattern 4: Keep all other sequences of word characters (letters, numbers, underscore) together.\n",
    "    r\"(?:[\\w_]+)\",\n",
    "\n",
    "    # Pattern 5: Match any other non-whitespace character as a token (e.g., punctuation).\n",
    "    r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "# Join all the individual regex patterns with the '|' (OR) operator to create one large regex.\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "# Compile the combined regex for faster execution.\n",
    "# re.VERBOSE: Allows for comments and whitespace in the pattern.\n",
    "# re.I: Makes the matching case-insensitive.\n",
    "# re.UNICODE: Makes character classes like \\w work with all Unicode characters.\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# Define a function that takes text and returns all non-overlapping matches found by our regex.\n",
    "def my_extensible_tokenize(text):\n",
    "    return my_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now we apply our custom tokenizer to the tweets, just as we did with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the tokenized tweets.\n",
    "extensible_tokens=[]\n",
    "# Loop through each tweet in the 'tweets' list.\n",
    "for tweet in tweets:\n",
    "    # Use our custom regex-based tokenizer function and append the result.\n",
    "    extensible_tokens.append(my_extensible_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Q1: Write a function to print out the first 5 tokenized tweets in each of the five tokenizers above. Examine those tweets; how would you characterize the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "To answer this, we'll loop through the first five tweets. The `zip()` function is used to iterate over the results from all five tokenization methods simultaneously for each tweet. For each tweet, we print the output from each tokenizer, making it easy to compare them side-by-side.\n",
    "\n",
    "**Observations on Differences:**\n",
    "\n",
    "* **Whitespace:** The most basic. It fails to separate punctuation from words (e.g., `United States.` is one token, `Wall,` is one token). It's generally not ideal for NLP tasks.\n",
    "* **NLTK (Standard):** Better than whitespace. It correctly separates most punctuation (e.g., `.` `!` `,`). However, it splits contractions like `can't` into `ca` and `n't` and handles `’` as a separate token. It also splits mentions like `@newtgingrich` into `@` and `newtgingrich`.\n",
    "* **NLTK (Casual):** Designed for tweets. It correctly keeps mentions (`@newtgingrich`) and hashtags together. It handles HTML entities differently (`&amp;` becomes `&`).\n",
    "* **spaCy:** Very sophisticated. It handles contractions well (`can't` becomes `ca` `n’t`). It keeps mentions together but sometimes attaches preceding punctuation to them (e.g., `.@newtgingrich`).\n",
    "* **Extensible (Regex):** Very effective for this specific text. It correctly keeps mentions and other important structures intact as single tokens because we explicitly defined rules for them. It separates punctuation cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use zip to iterate through the first 5 tokenized tweets from all five lists at once.\n",
    "# 'enumerate' provides an index 'idx' for each set of tweets.\n",
    "for idx, (one, two, three, four, five) in enumerate(zip(nltk_tokens, nltk_casual_tokens, spacy_tokens, whitespace_tokens, extensible_tokens)):\n",
    "    # Stop the loop after processing the first 5 tweets (indices 0 through 4).\n",
    "    if idx >= 5:\n",
    "        break\n",
    "    # Print the output from each tokenizer, joining the token lists back into strings for readability.\n",
    "    print(\"NLTK      :\\t%s\" % ' '.join(one))\n",
    "    print(\"CASUAL    :\\t%s\" % ' '.join(two))\n",
    "    print(\"SPACY     :\\t%s\" % ' '.join(three))\n",
    "    print(\"WHITESPACE:\\t%s\" % ' '.join(four))\n",
    "    print(\"EXTENSIBLE:\\t%s\" % ' '.join(five))\n",
    "\n",
    "    # Print a newline for better separation between tweets.\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Q2: Write a function `compare(tokenization_one, tokenization_two)` that compares two tokenizations of the same text and finds the 20 most frequent tokens that don't appear in the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "This function is designed to highlight the differences between two tokenization methods. It works by:\n",
    "1.  Counting the frequency of every token in both tokenization results using `collections.Counter`.\n",
    "2.  Iterating through the tokens of the first method and checking if they exist in the second. If not, they are added to a \"missing\" list.\n",
    "3.  Doing the same for the second method against the first.\n",
    "4.  Finally, printing the 20 most common tokens that were unique to each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes two lists of tokenized sentences.\n",
    "def compare(one_tokens, two_tokens):\n",
    "    \n",
    "    # Create a Counter object to store token frequencies for the first tokenization.\n",
    "    one_counts=Counter()\n",
    "    # Create a Counter object to store token frequencies for the second tokenization.\n",
    "    two_counts=Counter()\n",
    "\n",
    "    # Iterate through each tokenized sentence in the first list.\n",
    "    for sentence in one_tokens:\n",
    "        # Iterate through each token in the sentence.\n",
    "        for token in sentence:\n",
    "            # Increment the count for that token.\n",
    "            one_counts[token]+=1\n",
    "        \n",
    "    # Iterate through each tokenized sentence in the second list.\n",
    "    for sentence in two_tokens:\n",
    "        # Iterate through each token in the sentence.\n",
    "        for token in sentence:\n",
    "            # Increment the count for that token.\n",
    "            two_counts[token]+=1\n",
    "        \n",
    "    # Create a Counter for tokens present in the second list but missing from the first.\n",
    "    missing_from_one=Counter()\n",
    "    # Create a Counter for tokens present in the first list but missing from the second.\n",
    "    missing_from_two=Counter()\n",
    "    \n",
    "    # Iterate through all unique word types found in the first tokenization.\n",
    "    for word_type in one_counts:\n",
    "        # If a word is not found in the vocabulary of the second tokenization...\n",
    "        if word_type not in two_counts:\n",
    "            # ...add it to the 'missing_from_two' counter with its frequency.\n",
    "            missing_from_two[word_type]=one_counts[word_type]\n",
    "        \n",
    "    # Iterate through all unique word types found in the second tokenization.\n",
    "    for word_type in two_counts:\n",
    "        # If a word is not found in the vocabulary of the first tokenization...\n",
    "        if word_type not in one_counts:\n",
    "            # ...add it to the 'missing_from_one' counter with its frequency.\n",
    "            missing_from_one[word_type]=two_counts[word_type]\n",
    "\n",
    "    # Print a summary of the total number of sentences in each list.\n",
    "    print (\"Token counts -- one: %s, two: %s\" % (len(one_tokens), len(two_tokens)))\n",
    "    # Print the 20 most common tokens that are in the second list but not the first.\n",
    "    print (\"\\nNot in one:\")\n",
    "    print ('\\n'.join(\"%s\\t%d\" % (k,v) for (k,v) in missing_from_one.most_common(20)))\n",
    "    # Print the 20 most common tokens that are in the first list but not the second.\n",
    "    print (\"\\nNot in two:\")\n",
    "    print ('\\n'.join(\"%s\\t%d\" % (k,v) for (k,v) in missing_from_two.most_common(20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now, let's use the `compare` function to see the differences between NLTK's `casual_tokenize` (more modern, tweet-aware) and `word_tokenize` (standard Penn Treebank). The output clearly shows how the casual tokenizer keeps full mentions (`@realDonaldTrump`) and hashtags (`#Trump2016`) together, while the standard one splits them and handles punctuation and contractions differently (e.g., `don't` vs. `n't`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the compare function to analyze the differences between the casual and standard NLTK tokenizers.\n",
    "compare(nltk_casual_tokens, nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Q3: Use one of the NLTK tokenizers; write code to determine how many sentences are in this dataset, and what the average number of words per sentence is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "To solve this, we iterate through each full tweet text. For each tweet, we first use `nltk.sent_tokenize()` to split it into sentences. Then, for each of those sentences, we use `nltk.word_tokenize()` to count the words. We keep a running total of the number of sentences and the total number of words to calculate the average at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a float for the total token/word count.\n",
    "count=0.\n",
    "# Initialize an integer for the total sentence count.\n",
    "num_sents=0\n",
    "# Loop through each raw tweet string.\n",
    "for tweet in tweets:\n",
    "    # For each tweet, loop through the sentences detected by NLTK's sentence tokenizer.\n",
    "    for sent in nltk.sent_tokenize(tweet):\n",
    "        # Add the number of words in the current sentence to the total word count.\n",
    "        count+=len(nltk.word_tokenize(sent))\n",
    "        # Increment the sentence counter.\n",
    "        num_sents+=1\n",
    "# Print the final counts and the calculated average number of tokens per sentence.\n",
    "print(\"Sents: %s, Tokens/sent: %.1f\" % (num_sents, (count/num_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Q4 (check-plus): modify the extensible tokenizer above to keep urls together (e.g., www.google.com or http://www.google.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "To handle URLs, we add new patterns to our tuple of regular expressions. It's important to place them before the more general patterns. We add two new rules:\n",
    "1.  `r\"(?:https?:\\S+)\"`: This captures URLs starting with `http:` or `https:`, followed by any sequence of non-whitespace characters.\n",
    "2.  `r\"(?:www\\.\\S+)\"`: This captures URLs starting with `www.`, also followed by non-whitespace characters.\n",
    "\n",
    "By placing these near the top of the `regexes` tuple, we ensure they are matched before the text can be broken up by more general rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep urls together\n",
    "r\"(?:https?:\\S+)\",\n",
    "r\"(?:www\\.\\S+)\",\n",
    "  \n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_url_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize_with_urls(text):\n",
    "    return my_url_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Let's test our new URL-aware tokenizer on a sample sentence containing a URL. The output shows that the URL is correctly identified and kept as a single token, demonstrating that our modification was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new tokenizer on a sample sentence and print each token on a new line.\n",
    "print ('\\n'.join(my_extensible_tokenize_with_urls(\"The course website is http://people.ischool.berkeley.edu/~dbamman/info256.html\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an empty cell, often left for future code or notes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
