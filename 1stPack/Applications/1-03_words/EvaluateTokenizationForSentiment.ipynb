{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates different methods for tokenization and stemming/lemmatization\n",
    "and assesses the impact on binary sentiment classification, using a train/dev dataset of sample of 1000 reviews from the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/).  Each tokenization method is evaluated on the same learning algorithm ($\\ell_2$-regularized logistic regression); the only difference is the tokenization process. For more, see: http://sentiment.christopherpotts.net/tokenizing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Importing Libraries\n",
    "This cell imports all the necessary Python libraries and modules for the notebook's tasks. It includes libraries for natural language processing (`nltk`, `spacy`), a custom class for testing (`TokenizationTest`), and a specialized tokenizer (`happyfuntokenizing`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Natural Language Toolkit library, a popular library for NLP tasks.\n",
    "import nltk\n",
    "# Import the spaCy library, another powerful library for advanced NLP.\n",
    "import spacy\n",
    "# From NLTK's stemming module, import the PorterStemmer algorithm.\n",
    "from nltk.stem.porter import *\n",
    "# Import the custom class that will be used to run the evaluation tests.\n",
    "from TokenizationTest import TokenizationTest\n",
    "# Import a tokenizer designed for social media and sentiment analysis from a library by Christopher Potts.\n",
    "from happyfuntokenizing import Tokenizer as potts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Downloading the spaCy Model\n",
    "This cell attempts to download a small English language model (`en_core_web_sm`) for spaCy. This model is pre-trained and required for tasks like tokenization, part-of-speech tagging, and lemmatization. **Note:** The output shows an error (`No module named spacy`), indicating that the spaCy library was not installed in the environment where this command was run. This would cause subsequent cells that rely on spaCy to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a shell command executed from the notebook to download the specified spaCy model.\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Initializing Models and Tools\n",
    "Here, the various tools needed for tokenization and stemming are initialized. This includes loading the spaCy language model and creating instances of the NLTK Porter Stemmer and the Potts tokenizer. Assuming spaCy was installed correctly, this cell sets up the objects that will process the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English spaCy model. Disable the Named Entity Recognition (ner) and parser components to speed up processing,\n",
    "# as only the tagger (for lemmatization) is needed.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
    "# Explicitly remove the 'ner' component from the processing pipeline.\n",
    "nlp.remove_pipe('ner')\n",
    "# Explicitly remove the 'parser' component from the processing pipeline.\n",
    "nlp.remove_pipe('parser')\n",
    "\n",
    "# Create an instance of the NLTK PorterStemmer.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Create an instance of the Potts sentiment-aware tokenizer.\n",
    "potts_tokenizer=potts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Defining spaCy Wrapper Functions\n",
    "This cell defines two functions that act as wrappers for spaCy's functionality. One function is for tokenization (splitting text into words), and the other is for lemmatization (reducing words to their base or dictionary form). These functions make it easy to pass spaCy's methods to the evaluation tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes raw text data as input for spaCy tokenization.\n",
    "def spacy_tokenizer(data):\n",
    "    # Process the data with the spaCy nlp object.\n",
    "    spacy_tokens=nlp(data)\n",
    "    # Return a list of the token texts (the actual words).\n",
    "    return [token.text for token in spacy_tokens]\n",
    "\n",
    "# Define a function that takes raw text data as input for spaCy lemmatization.\n",
    "def spacy_lemmatizer(data):\n",
    "    # Process the data with the spaCy nlp object.\n",
    "    spacy_tokens=nlp(data)\n",
    "    # Return a list of the lemmas for each token.\n",
    "    return [token.lemma_ for token in spacy_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Setting Up the Evaluation Tester\n",
    "This cell initializes the `TokenizationTest` class with the paths to the training and development (validation) datasets. This `tester` object will handle the process of training the logistic regression model and evaluating its accuracy for each tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the TokenizationTest class.\n",
    "# The constructor takes the file paths for the training data and the development (test) data.\n",
    "tester=TokenizationTest(\"../data/sentiment.1000.train.txt\", \"../data/sentiment.1000.dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Evaluation 1: Basic String Splitting\n",
    "The first evaluation uses Python's built-in `str.split` method, which tokenizes text by splitting it at whitespace characters. This is a simple, baseline approach. The output shows an accuracy of **85.6%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate method of the tester object, passing the built-in string split method as the tokenizer.\n",
    "tester.evaluate(str.split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Evaluation 2: Porter Stemming\n",
    "This evaluation uses the NLTK Porter Stemmer. Stemming reduces words to their root form (e.g., \"running\" becomes \"run\"), which can help the model generalize. However, it can sometimes be too aggressive. The accuracy here is **82.5%**, lower than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Porter Stemmer's stemming method.\n",
    "tester.evaluate(stemmer.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Evaluation 3: NLTK Word Tokenizer\n",
    "This test uses `nltk.word_tokenize`, a more sophisticated tokenizer than `str.split`. It handles punctuation and contractions more effectively (e.g., \"don't\" becomes \"do\" and \"n't\"). This improved tokenization results in a higher accuracy of **87.5%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the NLTK library's standard word tokenizer.\n",
    "tester.evaluate(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Evaluation 4: spaCy Tokenizer\n",
    "Here, the `spacy_tokenizer` function defined earlier is evaluated. spaCy's tokenizer is highly advanced and context-aware. It achieves an accuracy of **87.1%**, which is very good but slightly below the NLTK tokenizer in this specific test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the custom spaCy tokenizer function.\n",
    "tester.evaluate(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 11: Evaluation 5: spaCy Lemmatizer\n",
    "This test evaluates the `spacy_lemmatizer` function. Lemmatization is similar to stemming but more linguistically informed, always reducing words to their valid dictionary form (e.g., \"better\" becomes \"good\"). This results in an accuracy of **87.3%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the custom spaCy lemmatizer function.\n",
    "tester.evaluate(spacy_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: Evaluation 6: Potts Sentiment Tokenizer\n",
    "The final evaluation uses the tokenizer from the `happyfuntokenizing` library, which is specifically designed for sentiment analysis and social media text. It handles features like emoticons, slang, and repeated punctuation well. It achieves the highest accuracy of all methods tested: **88.6%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tokenize method from the Potts tokenizer instance.\n",
    "tester.evaluate(potts_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (anlp)",
   "language": "python",
   "name": "anlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
