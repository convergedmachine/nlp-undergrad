{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores methods for comparing two different textual datasets to identify the terms that are distinct to each one:\n",
    "\n",
    "* Difference of proportions (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) section 3.2.2)\n",
    "* Mann-Whitney rank-sums test (described in [Kilgarriff 2001, Comparing Corpora](https://www.sketchengine.eu/wp-content/uploads/comparing_corpora_2001.pdf), section 2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup: Importing Necessary Libraries\n",
    "\n",
    "First, we'll import the libraries needed for our analysis. \n",
    "- `sys` and `operator` are standard Python libraries for system functions and efficient operations, respectively. `operator.itemgetter` is particularly useful for sorting dictionaries.\n",
    "- `Counter` from the `collections` module provides a specialized dictionary subclass for counting hashable objects, which is perfect for tallying word frequencies.\n",
    "- `mannwhitneyu` from `scipy.stats` is the specific implementation of the Mann-Whitney U rank test we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries for system operations and efficient functions.\n",
    "import sys, operator\n",
    "# Import the Counter class for easy frequency counting.\n",
    "from collections import Counter\n",
    "# Import the Mann-Whitney U test function from the SciPy statistics library.\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Data\n",
    "\n",
    "Next, we load our two text corpora. The data consists of tokenized text from political speeches, separated into files for Republicans (`repub.convote.txt`) and Democrats (`dem.convote.txt`). We open each file, read its entire content into a single string, and then split that string by spaces to create a list of tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The convote data is already tokenized so just split on whitespace\n",
    "# Open, read, and split the Republican speeches file into a list of tokens.\n",
    "repub_tokens=open(\"../data/repub.convote.txt\", encoding=\"utf-8\").read().split(\" \")\n",
    "# Open, read, and split the Democrat speeches file into a list of tokens.\n",
    "dem_tokens=open(\"../data/dem.convote.txt\", encoding=\"utf-8\").read().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Difference of Proportions\n",
    "\n",
    "Our first approach is a straightforward comparison of word frequencies. For each word in the combined vocabulary, we calculate its relative frequency (proportion) in each corpus and then find the difference between these two proportions. A large positive difference means the word is more characteristic of the first corpus, while a large negative difference means it's more characteristic of the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: First, calculate the simple \"difference of proportions\" measure from Monroe et al.'s \"Fighting Words\", section 3.2.2.  What are the top ten terms in this measurement that are most republican and most democrat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the difference in relative frequency for every word in two lists of tokens.\n",
    "def count_differences(one_tokens, two_tokens):\n",
    "    # Get the total number of tokens in the first list.\n",
    "    one_N=len(one_tokens)\n",
    "    # Get the total number of tokens in the second list.\n",
    "    two_N=len(two_tokens)\n",
    "    \n",
    "    # Create a Counter object to store word frequencies for the first corpus.\n",
    "    one_counts=Counter()\n",
    "    # Create a Counter object to store word frequencies for the second corpus.\n",
    "    two_counts=Counter()\n",
    "    \n",
    "    # Create a dictionary to hold the combined vocabulary from both corpora.\n",
    "    vocab={}\n",
    "    # Iterate through each token in the first list.\n",
    "    for token in one_tokens:\n",
    "        # Increment the count for the current token in the first corpus.\n",
    "        one_counts[token]+=1\n",
    "        # Add the token to our vocabulary set (value is arbitrary, just tracking keys).\n",
    "        vocab[token]=1\n",
    "        \n",
    "    # Iterate through each token in the second list.\n",
    "    for token in two_tokens:\n",
    "        # Increment the count for the current token in the second corpus.\n",
    "        two_counts[token]+=1    \n",
    "        # Add the token to our vocabulary set.\n",
    "        vocab[token]=1\n",
    "        \n",
    "    # Create a dictionary to store the calculated frequency differences.\n",
    "    differences={}\n",
    "    # Iterate through every unique word in the combined vocabulary.\n",
    "    for word in vocab:\n",
    "        # Calculate the relative frequency of the word in the first corpus.\n",
    "        freq1=one_counts[word]/one_N\n",
    "        # Calculate the relative frequency of the word in the second corpus.\n",
    "        freq2=two_counts[word]/two_N\n",
    "        \n",
    "        # Compute the difference between the two frequencies.\n",
    "        diff=freq1-freq2\n",
    "        # Store the difference in our dictionary.\n",
    "        differences[word]=diff\n",
    "        \n",
    "    # Return the dictionary of word-frequency differences.\n",
    "    return differences\n",
    "\n",
    "# This function uses the count_differences result to find and print the top 10 distinctive terms for each corpus.\n",
    "def difference_of_proportions(one_tokens, two_tokens):\n",
    "\n",
    "    # Call the previously defined function to get the frequency differences.\n",
    "    differences=count_differences(one_tokens, two_tokens)\n",
    "    \n",
    "    # Sort the dictionary items by their value (the difference) in ascending order.\n",
    "    # Words with the most negative scores will appear first.\n",
    "    sorted_differences = sorted(differences.items(), key=operator.itemgetter(1))\n",
    "    # Since we passed (dem_tokens, repub_tokens), negative scores mean the word is more frequent in the Republican corpus.\n",
    "    print (\"More Republican:\")\n",
    "    # Loop through the first 10 items in the sorted list (most negative scores).\n",
    "    for k,v in sorted_differences[:10]:\n",
    "        # Print the word and its score.\n",
    "        print (\"%s\\t%s\" % (k,v))\n",
    "    # Print a newline for better formatting.\n",
    "    print(\"\\nMore Democrat:\")\n",
    "    # Loop through the last 10 items of the sorted list in reverse order (most positive scores).\n",
    "    for k,v in reversed(sorted_differences[-10:]):\n",
    "        # Print the word and its score.\n",
    "        print (\"%s\\t%s\" % (k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we execute the `difference_of_proportions` function. We pass `dem_tokens` as the first argument and `repub_tokens` as the second. Therefore, words with a high positive score are more distinctive to Democrats, and words with a high negative score are more distinctive to Republicans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis with Democrat tokens as the first corpus and Republican tokens as the second.\n",
    "difference_of_proportions(dem_tokens, repub_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Mann-Whitney U Test\n",
    "\n",
    "As we can see from the results above, the difference of proportions method tends to highlight very common words (like 'i', 'we', 'and', 'of'). This is because even a small percentage difference for a high-frequency word results in a large absolute difference. This method also doesn't tell us if the difference is statistically significant. \n",
    "\n",
    "To address this, we turn to a more robust statistical test: the **Mann-Whitney U test**. This test helps us avoid the pitfalls of word \"burstiness\"â€”where a word might appear many times in one document but not be representative of the entire corpus. Instead of comparing total counts, we divide each corpus into smaller, equal-sized chunks. We then compare the *distribution* of word counts across these chunks. The test determines if the word counts in the chunks from one corpus are systematically higher or lower than the counts in the chunks from the other corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply analyzing the difference in relative frequencies has a number of downsides: 1.) As Monroe et al (2009) points out (and we can see here as well), it tends to emphasize high-frequency words (be sure you understand why).  2.) We're not measuring whether a difference is statistically meaningful or just due to chance; the $\\chi^2$ test is one method (described in Kilgarriff 2001 and in the context of collocations in Manning and Schuetze [here](https://nlp.stanford.edu/fsnlp/promo/colloc.pdf)) that addresses the desideratum of finding statistically significant terms, but it too has another downside: 3.) Simply counting up the total number of mentions of a term doesn't account for the \"burstiness\" of language -- if we see the word \"Dracula\" in a text, we're probably going to see it again in that same text.  The occurrence of words are not independent random events; they are tightly coupled with each other. If we're trying to understanding the robust differences between two corpora, we might prefer to prioritize words that show up more frequently *everywhere* in corpus A (but not in corpus B) over those that show up only very frequently within a narrow slice of A (such as one text in a genre, one chapter in a book, or one speaker when measuring the differences between policital parties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 (check-plus): One measure that does account for this burstiness is the adaptation by corpus linguistics of the non-parametric Mann-Whitney rank-sum test. The specific adaptation of this test for text is described in Kilgarriff 2001, section 2.3.  Implement this test using a fixed chunk size of 500 and the [scikit-learn mannwhitneyu function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html); what are the top ten terms in this measurement that are most republican and most democrat? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a flat list of tokens into a list of frequency counters, one for each chunk.\n",
    "def get_chunk_counts(tokens, chunkLength):\n",
    "    # Initialize an empty list to store the chunk counts.\n",
    "    chunks=[]\n",
    "    # Iterate through the tokens, stepping by the chunkLength.\n",
    "    for i in range(0, len(tokens), chunkLength):\n",
    "            # Create a new Counter for the current chunk.\n",
    "            counts=Counter()\n",
    "            # Iterate from 0 to chunkLength to populate the chunk's counter.\n",
    "            for j in range(chunkLength):\n",
    "                # Make sure we don't go beyond the end of the token list.\n",
    "                if i+j < len(tokens):\n",
    "                    # Increment the count for the token at the current position.\n",
    "                    counts[tokens[i+j]]+=1\n",
    "            # Add the populated counter for the current chunk to our list of chunks.\n",
    "            chunks.append(counts)\n",
    "    # Return the list of chunk-based frequency counters.\n",
    "    return chunks\n",
    "\n",
    "# This function calculates the Mann-Whitney U p-value for each word in the vocabulary.\n",
    "def mann_whitney(one_tokens, two_tokens):\n",
    "\n",
    "    # Define the size of each text chunk.\n",
    "    chunkLength=500\n",
    "    # Get the chunked counts for the first corpus.\n",
    "    one_chunks=get_chunk_counts(one_tokens, chunkLength)\n",
    "    # Get the chunked counts for the second corpus.\n",
    "    two_chunks=get_chunk_counts(two_tokens, chunkLength)\n",
    "    \n",
    "    # Create a dictionary to hold the combined vocabulary from both sets of chunks.\n",
    "    vocab={}\n",
    "    \n",
    "    # Populate the vocabulary from the first corpus.\n",
    "    for chunk in one_chunks:\n",
    "        for word in chunk:\n",
    "            vocab[word]=1\n",
    "    # Populate the vocabulary from the second corpus.\n",
    "    for chunk in two_chunks:\n",
    "        for word in chunk:\n",
    "            vocab[word]=1\n",
    "    \n",
    "    # Initialize a dictionary to store the p-value for each word.\n",
    "    pvals={}\n",
    "    \n",
    "    # Iterate through every word in the combined vocabulary.\n",
    "    for word in vocab:\n",
    "        \n",
    "        # Initialize a list 'a' to hold the counts of the current word in each chunk of the first corpus.\n",
    "        a=[]\n",
    "        # Initialize a list 'b' to hold the counts of the current word in each chunk of the second corpus.\n",
    "        b=[]\n",
    "        \n",
    "        # The Mann-Whitney U test can handle samples of different sizes (i.e., a different number of chunks).\n",
    "        \n",
    "        # Populate list 'a' with the frequency of the word in each chunk of the first corpus.\n",
    "        for chunk in one_chunks:\n",
    "            a.append(chunk[word])\n",
    "        # Populate list 'b' with the frequency of the word in each chunk of the second corpus.\n",
    "        for chunk in two_chunks:\n",
    "            b.append(chunk[word])\n",
    "\n",
    "        # Perform the Mann-Whitney U test on the two lists of counts.\n",
    "        # The 'alternative=\"two-sided\"' checks if the distributions are different, regardless of direction.\n",
    "        statistic,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "        \n",
    "        # The p-value indicates the statistical significance of the difference.\n",
    "        # A smaller p-value means the observed difference is less likely to be due to random chance.\n",
    "        pvals[word]=pval\n",
    "\n",
    "    # Return the dictionary of words and their corresponding p-values.\n",
    "    return pvals\n",
    "    \n",
    "# This function orchestrates the Mann-Whitney analysis and presents the results.\n",
    "def mann_whitney_analysis(one_tokens, two_tokens):\n",
    "    \n",
    "    # Calculate the p-values for all words using the mann_whitney function.\n",
    "    pvals=mann_whitney(one_tokens, two_tokens)\n",
    "    \n",
    "    # The p-value tells us *if* there's a significant difference, but not the *direction* of that difference.\n",
    "    # We reuse our simple 'count_differences' function to determine which corpus uses the word more overall.\n",
    "    differences=count_differences(one_tokens, two_tokens)\n",
    "    \n",
    "    # Create a dictionary for words used more by the second corpus (Republicans in our case).\n",
    "    # A non-positive difference (<= 0) means the word is more or equally frequent in corpus two.\n",
    "    one_terms={k : pvals[k] for k in pvals if differences[k] <= 0}\n",
    "    # Create a dictionary for words used more by the first corpus (Democrats in our case).\n",
    "    # A positive difference (> 0) means the word is more frequent in corpus one.\n",
    "    two_terms={k : pvals[k] for k in pvals if differences[k] > 0}\n",
    "    \n",
    "    # Sort the Republican-leaning terms by their p-value in ascending order (most significant first).\n",
    "    sorted_pvals = sorted(one_terms.items(), key=operator.itemgetter(1))\n",
    "    print(\"More Republican:\\n\")\n",
    "    # Print the top 10 most statistically significant Republican terms.\n",
    "    for k,v in sorted_pvals[:10]:\n",
    "        print(\"%s\\t%.15f\" % (k,v))\n",
    "\n",
    "    print(\"\\nMore Democrat:\\n\")\n",
    "    # Sort the Democrat-leaning terms by their p-value in ascending order.\n",
    "    sorted_pvals = sorted(two_terms.items(), key=operator.itemgetter(1))\n",
    "    # Print the top 10 most statistically significant Democrat terms.\n",
    "    for k,v in sorted_pvals[:10]:\n",
    "        print(\"%s\\t%.15f\" % (k,v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the complete Mann-Whitney U test analysis. The output will show the 10 words that are most statistically significant for each party, sorted by their p-value (a lower p-value indicates higher significance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Mann-Whitney analysis function with our token lists.\n",
    "mann_whitney_analysis(dem_tokens, repub_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Comparing the two methods, we see a marked difference in the results. The Mann-Whitney U test produces lists that seem more topically relevant (e.g., 'growth', 'economy' for Republicans; 'cuts', 'billion' for Democrats) and less dominated by common function words. This demonstrates how choosing a more sophisticated statistical method that accounts for the distributional properties of language can yield more insightful and meaningful results when comparing text corpora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
